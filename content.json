[{"title":"统一认证中心 Oauth2 认证坑","date":"2021-11-11T07:11:21.000Z","path":"2021/11/11/oauth2/","text":"在前面文章 Springcloud Oauth2 HA篇 中，实现了基于 Oauth2 的统一认证的认证与授权。在配置中，我们可以看到： 1234567891011121314cas-server-url: http:&#x2F;&#x2F;cas-server-service #这里配置成HA地址security: oauth2: #与cas-server对应的配置 client: client-id: admin-web client-secret: admin-web-123 user-authorization-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;authorize #是授权码认证方式需要的 access-token-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;token #是密码模式需要用到的获取 token 的接口 resource: loadBalanced: true id: admin-web user-info-uri: $&#123;cas-server-url&#125;&#x2F;api&#x2F;user #指定user info的URI prefer-token-info: false 这里的 url 配置是基于k8s的 Service，实现负载均衡，从而实现高可用。但我们接下来分析 user-info-uri。user-info-uri 的原理是在授权服务器认证后将认证信息 Principal 通过形参绑定的方法通过URL的方式获取用户信息。当然它也有配套的 UserInfoTokenService 等。 但这个在客户端获取用户权限时候，是存在一定问题的。譬如 Web端请求消费端的某个接口： 123456789101112131415161718192021222324252627&#x2F;** * 返回发现的所有服务 * @author Damon * @date 2021年11月2日 下午8:18:44 * @return * *&#x2F;@PreAuthorize(&quot;hasRole(&#39;admin&#39;)&quot;) @GetMapping(value &#x3D; &quot;&#x2F;getService&quot;) public String getService()&#123; HttpHeaders headers &#x3D; new HttpHeaders(); MediaType type &#x3D; MediaType.parseMediaType(&quot;application&#x2F;json; charset&#x3D;UTF-8&quot;); headers.setContentType(type); headers.add(&quot;Accept&quot;, MediaType.APPLICATION_JSON.toString()); HttpEntity&lt;String&gt; formEntity &#x3D; new HttpEntity&lt;String&gt;(null, headers); String body &#x3D; &quot;&quot;; try &#123; ResponseEntity&lt;String&gt; responseEntity &#x3D; restTemplate.exchange(&quot;http:&#x2F;&#x2F;cas-server&#x2F;api&#x2F;v1&#x2F;user&quot;, HttpMethod.GET, formEntity, String.class); if (responseEntity.getStatusCodeValue() &#x3D;&#x3D; 200) &#123; return &quot;ok&quot;; &#125; &#125; catch (Exception e) &#123; System.out.println(e.getMessage()); &#125; return body; &#125; 在这个接口中，我们通过添加@PreAuthorize(&quot;hasRole(&#39;admin&#39;)&quot;)来控制权限，只要是admin的用户才能访问改接口。 我们先来请求认证中心登录接口，获取token： 在拿到token之后，我们请求这个接口，我们会发现： 说明未认证，我们再看看：发现原来当请求这个接口时，消费端后去请求认证中心的接口： 12345672021-11-03 15:59:09.385 DEBUG 127896 --- [io2-2001-exec-4] org.springframework.web.HttpLogging : HTTP GET http:&#x2F;&#x2F;cas-server&#x2F;auth&#x2F;user2021-11-03 15:59:09.389 DEBUG 127896 --- [io2-2001-exec-4] org.springframework.web.HttpLogging : Accept&#x3D;[application&#x2F;json, application&#x2F;*+json]2021-11-03 15:59:09.427 DEBUG 127896 --- [io2-2001-exec-4] org.springframework.web.HttpLogging : Response 404 NOT_FOUND2021-11-03 15:59:09.446 DEBUG 127896 --- [io2-2001-exec-4] o.s.w.c.HttpMessageConverterExtractor : Reading to [org.springframework.security.oauth2.common.exceptions.OAuth2Exception]2021-11-03 15:59:09.456 WARN 127896 --- [io2-2001-exec-4] o.s.b.a.s.o.r.UserInfoTokenServices : Could not fetch user details: class org.springframework.web.client.HttpClientErrorException$NotFound, 404 : [&#123;&quot;timestamp&quot;:&quot;2021-11-03T07:59:09.423+00:00&quot;,&quot;status&quot;:404,&quot;error&quot;:&quot;Not Found&quot;,&quot;message&quot;:&quot;&quot;,&quot;path&quot;:&quot;&#x2F;auth&#x2F;user&quot;&#125;]2021-11-03 15:59:09.457 ERROR 127896 --- [io2-2001-exec-4] c.l.h.CustomAuthenticationEntryPoint : 无效的token，请重新认证访问&#123;&quot;data&quot;:&quot;b34841b4-61fa-4dbb-9e2b-76496deb27b4&quot;,&quot;result&quot;:&#123;&quot;code&quot;:20202,&quot;msg&quot;:&quot;未认证&quot;,&quot;status&quot;:401&#125;&#125; 但认证中心给返回的404状态码，此时会走统一异常EntryPoint提示报错：无效的token，请重新认证访问。从而返回信息体：{&quot;data&quot;:&quot;b34841b4-61fa-4dbb-9e2b-76496deb27b4&quot;,&quot;result&quot;:{&quot;code&quot;:20202,&quot;msg&quot;:&quot;未认证&quot;,&quot;status&quot;:401}}。 接下来分析：为什么认证中心会返回404呢？看认证中心日志： 1234567892021-11-03 15:59:09.407 DEBUG 54492 --- [o2-2000-exec-15] o.s.web.servlet.DispatcherServlet : GET &quot;&#x2F;auth&#x2F;user&quot;, parameters&#x3D;&#123;&#125;2021-11-03 15:59:09.409 DEBUG 54492 --- [o2-2000-exec-15] o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped to ResourceHttpRequestHandler [&quot;classpath:&#x2F;META-INF&#x2F;resources&#x2F;&quot;, &quot;classpath:&#x2F;resources&#x2F;&quot;, &quot;classpath:&#x2F;static&#x2F;&quot;, &quot;classpath:&#x2F;public&#x2F;&quot;]2021-11-03 15:59:09.413 DEBUG 54492 --- [o2-2000-exec-15] o.s.w.s.r.ResourceHttpRequestHandler : Resource not found2021-11-03 15:59:09.414 DEBUG 54492 --- [o2-2000-exec-15] o.s.web.servlet.DispatcherServlet : Completed 404 NOT_FOUND2021-11-03 15:59:09.422 DEBUG 54492 --- [o2-2000-exec-15] o.s.web.servlet.DispatcherServlet : &quot;ERROR&quot; dispatch for GET &quot;&#x2F;error&quot;, parameters&#x3D;&#123;&#125;2021-11-03 15:59:09.423 DEBUG 54492 --- [o2-2000-exec-15] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController#error(HttpServletRequest)2021-11-03 15:59:09.424 DEBUG 54492 --- [o2-2000-exec-15] o.s.w.s.m.m.a.HttpEntityMethodProcessor : Using &#39;application&#x2F;json&#39;, given [application&#x2F;json] and supported [application&#x2F;json, application&#x2F;*+json, application&#x2F;json, application&#x2F;*+json, application&#x2F;json, application&#x2F;*+json]2021-11-03 15:59:09.424 DEBUG 54492 --- [o2-2000-exec-15] o.s.w.s.m.m.a.HttpEntityMethodProcessor : Writing [&#123;timestamp&#x3D;Wed Nov 03 15:59:09 CST 2021, status&#x3D;404, error&#x3D;Not Found, message&#x3D;, path&#x3D;&#x2F;auth&#x2F;user&#125;]2021-11-03 15:59:09.426 DEBUG 54492 --- [o2-2000-exec-15] o.s.web.servlet.DispatcherServlet : Exiting from &quot;ERROR&quot; dispatch, status 404 发现原来Oauth2没有此类接口:/auth/user。最后决定自写一个接口来替换原生: 123456@GetMapping(&quot;&#x2F;api&#x2F;v1&#x2F;user&quot;) public Authentication user(Map map, Principal user, Authentication auth) &#123; &#x2F;&#x2F;获取当前用户信息 logger.info(&quot;cas-server provide user: &quot; + JSON.toJSONString(auth)); return auth; &#125; 在封装、覆盖后，在消费端直接配置相关配置： 123456789101112131415161718cas-server-url: http:&#x2F;&#x2F;cas-serversecurity: path: ignores: &#x2F;,&#x2F;index,&#x2F;static&#x2F;**,&#x2F;css&#x2F;**, &#x2F;image&#x2F;**, &#x2F;favicon.ico, &#x2F;js&#x2F;**,&#x2F;plugin&#x2F;**,&#x2F;avue.min.js,&#x2F;img&#x2F;**,&#x2F;fonts&#x2F;** oauth2: client: client-id: rest-service client-secret: rest-service-123 user-authorization-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;authorize access-token-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;token resource: loadBalanced: true id: rest-service prefer-token-info: false user-info-uri: $&#123;cas-server-url&#125;&#x2F;api&#x2F;v1&#x2F;user authorization: check-token-access: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;check_token 同时启动认证中心、消费端，继续获取token后，请求接口： 此时，发现是403，没有权限了，这下我们可以对用户添加这种权限即可： 1&quot;authorities&quot;: [ &#123; &quot;authority&quot;: &quot;ROLE_admin&quot; &#125;, &#123; &quot;authority&quot;: &quot;admin&quot; &#125; 添加完之后，我们发现可以请求接口成功： 12&#123; &quot;authorities&quot;: [ &#123; &quot;authority&quot;: &quot;ROLE_admin&quot; &#125;, &#123; &quot;authority&quot;: &quot;admin&quot; &#125; ], &quot;details&quot;: &#123; &quot;remoteAddress&quot;: &quot;0:0:0:0:0:0:0:1&quot;, &quot;sessionId&quot;: null, &quot;tokenValue&quot;: &quot;b34841b4-61fa-4dbb-9e2b-76496deb27b4&quot;, &quot;tokenType&quot;: &quot;bearer&quot;, &quot;decodedDetails&quot;: null &#125;, &quot;authenticated&quot;: true, &quot;userAuthentication&quot;: &#123; &quot;authorities&quot;: [ &#123; &quot;authority&quot;: &quot;ROLE_admin&quot; &#125;, &#123; &quot;authority&quot;: &quot;admin&quot; &#125; ], &quot;details&quot;: &#123; &quot;authorities&quot;: [ &#123; &quot;authority&quot;: &quot;ROLE_admin&quot; &#125;, &#123; &quot;authority&quot;: &quot;admin&quot; &#125; ], &quot;details&quot;: &#123; &quot;remoteAddress&quot;: &quot;169.254.200.12&quot;, &quot;sessionId&quot;: null, &quot;tokenValue&quot;: &quot;b34841b4-61fa-4dbb-9e2b-76496deb27b4&quot;, &quot;tokenType&quot;: &quot;Bearer&quot;, &quot;decodedDetails&quot;: null &#125;, &quot;authenticated&quot;: true, &quot;userAuthentication&quot;: &#123; &quot;authorities&quot;: [ &#123; &quot;authority&quot;: &quot;ROLE_admin&quot; &#125;, &#123; &quot;authority&quot;: &quot;admin&quot; &#125; ],... 这里简单测试，直接写的返回当前用户权限的接口，发现权限就是”ROLE_admin、”admin”。 总结有时候官网的源码解析很少，我们必须看源码，结合实际行动才能准确的分析其用意。所以当其不存在、或者不满足我们的需求时，可以选择覆盖其源码逻辑，实现自定义模式，这样会避免很多不必要的麻烦。因为源码解析毕竟不同版本，对应的源码也是不同的。","tags":[]},{"title":"统一认证中心 Oauth2 高可用坑","date":"2021-11-11T07:10:32.000Z","path":"2021/11/11/oauth2-ha/","text":"前面 (统一认证中心 Oauth2 认证坑) 我们利用user-info-uri来实现消费端的认证信息以及授权获取判断，接下来我们借助 token-info-uri 来实现认证以及授权破。具体配置见： 123456789101112131415161718cas-server-url: http:&#x2F;&#x2F;cas-serversecurity: path: ignores: &#x2F;,&#x2F;index,&#x2F;static&#x2F;**,&#x2F;css&#x2F;**, &#x2F;image&#x2F;**, &#x2F;favicon.ico, &#x2F;js&#x2F;**,&#x2F;plugin&#x2F;**,&#x2F;avue.min.js,&#x2F;img&#x2F;**,&#x2F;fonts&#x2F;** oauth2: client: client-id: rest-service client-secret: rest-service-123 user-authorization-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;authorize access-token-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;token resource: loadBalanced: true id: rest-service prefer-token-info: true token-info-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;check_token authorization: check-token-access: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;check_token 这里的/oauth/check_token是 Oauth2 原生自带的，这里不需要封装。接下来，我们启动服务，在拿到 token 后，通过 token 请求消费端： 123456789102021-11-03 16:40:09.057 DEBUG 24652 --- [io2-2001-exec-4] o.s.web.client.RestTemplate : HTTP POST http:&#x2F;&#x2F;cas-server&#x2F;oauth&#x2F;check_token2021-11-03 16:40:09.060 DEBUG 24652 --- [io2-2001-exec-4] o.s.web.client.RestTemplate : Accept&#x3D;[application&#x2F;json, application&#x2F;*+json]2021-11-03 16:40:09.062 DEBUG 24652 --- [io2-2001-exec-4] o.s.web.client.RestTemplate : Writing [&#123;token&#x3D;[b34841b4-61fa-4dbb-9e2b-76496deb27b4]&#125;] as &quot;application&#x2F;x-www-form-urlencoded&quot;2021-11-03 16:40:11.332 ERROR 24652 --- [io2-2001-exec-4] o.a.c.c.C.[.[.[&#x2F;].[dispatcherServlet] : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exceptionorg.springframework.web.client.ResourceAccessException: I&#x2F;O error on POST request for &quot;http:&#x2F;&#x2F;cas-server&#x2F;oauth&#x2F;check_token&quot;: cas-server; nested exception is java.net.UnknownHostException: cas-server at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:746) at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:672) at org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:581) at org.springframework.security.oauth2.provider.token.RemoteTokenServices.postForMap(RemoteTokenServices.java:149) 我们从上面的日志中，可以发现系统抛出 UnknownHostException 这种异常，无法找到cas-server，但我要说的是：我们这里用到的是Nacos注册中心来实现服务的注册与发现： 那说明注册的服务可以被发现，接下来我们看支持 LB 的几种服务消费方式： RestTemplate、WebClient、Feign。我们这里基于 Ribbon，RestTemplate，因为在Oauth2原生中，就是基于RestTemplate来调用远程服务： 1234567891011private Map&lt;String, Object&gt; postForMap(String path, MultiValueMap&lt;String, String&gt; formData, HttpHeaders headers) &#123; if (headers.getContentType() &#x3D;&#x3D; null) &#123; headers.setContentType(MediaType.APPLICATION_FORM_URLENCODED); &#125; @SuppressWarnings(&quot;rawtypes&quot;) Map map &#x3D; restTemplate.exchange(path, HttpMethod.POST, new HttpEntity&lt;MultiValueMap&lt;String, String&gt;&gt;(formData, headers), Map.class).getBody(); @SuppressWarnings(&quot;unchecked&quot;) Map&lt;String, Object&gt; result &#x3D; map; return result; &#125; 大家都知道默认的原生Ribbon，是基于 RestTemplate 的负载均衡，所以这里配置如下： 123456789@LoadBalanced@Beanpublic RestTemplate restTemplate() &#123; SimpleClientHttpRequestFactory requestFactory &#x3D; new SimpleClientHttpRequestFactory(); requestFactory.setReadTimeout(env.getProperty(&quot;client.http.request.readTimeout&quot;, Integer.class, 15000)); requestFactory.setConnectTimeout(env.getProperty(&quot;client.http.request.connectTimeout&quot;, Integer.class, 3000)); RestTemplate rt &#x3D; new RestTemplate(requestFactory); return rt;&#125; 可以看到，在定义 RestTemplate 的时候，增加了@LoadBalanced注解，但其实在真正调用服务接口的时候，原来host部分是通过手工拼接ip和端口的，直接采用服务名的时候来写请求路径即可。在真正调用的时候，Spring Cloud会将请求拦截下来，然后通过负载均衡器选出节点，并替换服务名为具体的ip和端口，从而实现基于服务名的负载均衡调用。 接下来，我们再看看负载均衡的策略是否有问题，Ribbon默认的负载均衡策略是轮询，内置了多种负载均衡策略，内置的负载均衡的顶级接口为com.netflix.loadbalancer.IRule。具体的策略有：AvailabilityFilteringRule、RoundRobinRule、RetryRule、RandomRule、WeightedResponseTimeRule、BestAvailableRule等。这里直接使用默认的轮询: 12345678910@Beanpublic IRule ribbonRule(IClientConfig config)&#123; &#x2F;&#x2F;return new AvailabilityFilteringRule(); return new RoundRobinRule();&#x2F;&#x2F;轮询 &#x2F;&#x2F;return new RetryRule();&#x2F;&#x2F;重试 &#x2F;&#x2F;return new RandomRule();&#x2F;&#x2F;这里配置策略，和配置文件对应 &#x2F;&#x2F;return new WeightedResponseTimeRule();&#x2F;&#x2F;这里配置策略，和配置文件对应 &#x2F;&#x2F;return new BestAvailableRule();&#x2F;&#x2F;选择一个最小的并发请求的server &#x2F;&#x2F;return new MyProbabilityRandomRule();&#x2F;&#x2F;自定义&#125; 到这里，目前都还未发现问题，那么既然实现了基于RestTemplate的负载均衡，为什么还是报错呢？ 找了半天，最后发现在Oauth2源码中，注入的是这么个玩意： 这时候才发现多么的坑，于是乎，一顿猛操作：在资源检验时调用的覆盖其注入： 12345@Autowired(required &#x3D; true)private RemoteTokenServices remoteTokenServices;@AutowiredRestTemplate restTemplate; 其二，直接set RestTemplate： 1234567891011121314151617181920212223242526@Override public void configure(ResourceServerSecurityConfigurer resource) throws Exception &#123; super.configure(resource); restTemplate.setErrorHandler(new DefaultResponseErrorHandler() &#123; @Override &#x2F;&#x2F; Ignore 400 public void handleError(ClientHttpResponse response) throws IOException &#123; if (response.getRawStatusCode() !&#x3D; 400) &#123; super.handleError(response); &#125; &#125; &#125;); if (Objects.nonNull(remoteTokenServices)) &#123; remoteTokenServices.setRestTemplate(restTemplate); resource.tokenServices(remoteTokenServices); &#125; resource &#x2F;&#x2F;.tokenStore(tokenStore) &#x2F;&#x2F;.tokenServices(tokenServices) .authenticationEntryPoint(customAuthenticationEntryPoint) .accessDeniedHandler(customAccessDeniedHandler) &#x2F;&#x2F;.tokenExtractor(new BearerTokenExtractor()) ; &#125; 接下来，我们重启消费端，看看效果，根据之前请求的token，直接访问消费端接口： 123452021-11-03 16:57:50.476 INFO 81424 --- [io2-2001-exec-3] o.s.web.servlet.DispatcherServlet : Completed initialization in 12 ms2021-11-03 16:57:50.522 DEBUG 81424 --- [io2-2001-exec-3] o.s.web.client.RestTemplate : HTTP POST http:&#x2F;&#x2F;cas-server&#x2F;oauth&#x2F;check_token2021-11-03 16:57:50.526 DEBUG 81424 --- [io2-2001-exec-3] o.s.web.client.RestTemplate : Accept&#x3D;[application&#x2F;json, application&#x2F;*+json]2021-11-03 16:57:50.528 DEBUG 81424 --- [io2-2001-exec-3] o.s.web.client.RestTemplate : Writing [&#123;token&#x3D;[b34841b4-61fa-4dbb-9e2b-76496deb27b4]&#125;] as &quot;application&#x2F;x-www-form-urlencoded&quot;2021-11-03 16:57:50.635 DEBUG 81424 --- [io2-2001-exec-3] o.s.web.client.RestTemplate : Response 200 OK 发现ok了，返回成功200，并且有权限访问该接口： 总结 有时候自己的代码写的已经很好了，但发现还是无法实现自己想要的：于是乎，可以大胆设想是不是官网源码出了幺蛾子，就像本文一样，如果不一步步检查，怎么也不会发现原来是源码留下如此大的坑，在前面的文章中，其实发现很多源码的不合理之处之后，都在修改，并且生成一套自己的规范返回，这样对于代码本身来说，我们会更加深刻体会、理解。Oauth2源码本身可以只是一个带头的基础功能，后面基于大项目，需要自己对于一些系统的设计进行改造，例如：高可用、高并发鉴权方案、统一认证SSO等等。","tags":[]},{"title":"基于 spring-cloud-k8s 跨NS坑续集","date":"2021-11-11T07:07:51.000Z","path":"2021/11/11/spring-cloud-k8s-note2/","text":"在前面文章 (spring-cloud-k8s 跨 NS 的坑) 中，讲述了 spring-cloud-k8s 中，如何利用 k8s 基于 Ribbon 等负载均衡利器来实现 LB，但存在跨命名空间的问题。 今天主要分享的是，基于 K8s 本身的 LB 利器，如何实现跨命名空间的应用服务互相访问，而且不是通过 K8s 原生的负载均衡 url 方式。还是基于 ServiceName。 直击源码首先，我们新建一个服务提供者:diff-ns-service，该服务提供了一个接口： 123456789&#x2F;** * 返回远程调用的结果 * @return *&#x2F;@RequestMapping(&quot;&#x2F;getservicedetail&quot;)public String getservicedetail( @RequestParam(value &#x3D; &quot;servicename&quot;, defaultValue &#x3D; &quot;&quot;) String servicename) &#123; return JSON.toJSONString(discoveryClient.getInstances(servicename));&#125; 该接口的功能是返回指定 service 的相关信息。比如：这个 Service 对应的有几个 pod，每个 pod 的节点信息，host 等。 如果想结合 K8s 来实现这个服务的发现，可以基于这个配置： 123456789101112131415161718192021management: endpoint: restart: enabled: true health: enabled: true info: enabled: truespring: application: name: diff-ns-service cloud: loadbalancer: ribbon: enabled: false kubernetes: ribbon: mode: SERVICE discovery: all-namespaces: true 另外，如果想利用 k8s configMap 的配置来实现动态刷新应用服务的环境配置，可以这样配置： 12345678910111213141516spring: application: name: diff-ns-service cloud: kubernetes: reload: enabled: true strategy: refresh mode: event config: name: $&#123;spring.application.name&#125; namespace: default sources: - name: $&#123;spring.application.name&#125; namespace: ns-app 这里的动态刷新的模式有两个：[polling、event。一个是主动拉取，一个是当 configmap 发生改变时，这种事件会被监听到，会主动刷新。 另外，这个刷新的策略也有几种： refresh，直接刷新 restart_context，整个 Spring Context 会优雅重启，里面的所有配置都会重新加载 shutdown，重启容器 这样，我们再来配置一下 Service： 12345678910111213apiVersion: v1kind: Servicemetadata: name: diff-ns-service-service namespace: ns-appspec: type: NodePort ports: - name: diff-ns-svc port: 2008 targetPort: 2001 selector: app: diff-ns-service 这里我们设置了 Service 的 port,并且这个 Service 以 NodePort 类型创建。在(spring-cloud-k8s 跨 NS 的坑)一文中，我们使用的是默认的类型：ClusterIp。 这样，一个简单的服务提供者就创建成功了。接下来，我们看看服务消费者。 同样，我们先来创建一个服务 rest-service，创建接口： 1234567891011121314151617181920@GetMapping(&quot;&#x2F;getClientRes&quot;)public Response&lt;Object&gt; getClientRes() throws Exception &#123; HttpHeaders headers &#x3D; new HttpHeaders(); MediaType type &#x3D; MediaType.parseMediaType(&quot;application&#x2F;json; charset&#x3D;UTF-8&quot;); headers.setContentType(type); headers.add(&quot;Accept&quot;, MediaType.APPLICATION_JSON.toString()); HttpEntity&lt;String&gt; formEntity &#x3D; new HttpEntity&lt;String&gt;(null, headers); String body &#x3D; &quot;&quot;; try &#123; ResponseEntity&lt;String&gt; responseEntity &#x3D; restTemplate.exchange(&quot;http:&#x2F;&#x2F;diff-ns-service-service&#x2F;getservicedetail?servicename&#x3D;cas-server-service&quot;, HttpMethod.GET, formEntity, String.class); System.out.println(JSON.toJSONString(responseEntity)); if (responseEntity.getStatusCodeValue() &#x3D;&#x3D; 200) &#123; return Response.ok(responseEntity.getBody()); &#125; &#125; catch (Exception e) &#123; System.out.println(e.getMessage()); &#125; return Response.error(&quot;failed&quot;);&#125; 同理地，结合 K8s 来实现这个服务的发现，可以基于这个配置： 123456789101112131415161718192021management: endpoint: restart: enabled: true health: enabled: true info: enabled: truespring: application: name: rest-service cloud: loadbalancer: ribbon: enabled: false kubernetes: ribbon: mode: SERVICE discovery: all-namespaces: true 这里，我们不使用 RibbonLoadBalancerClient。 另外，如果想利用 k8s configMap 的配置来实现动态刷新应用服务的环境配置，可以这样配置： 1234567891011121314spring: cloud: kubernetes: reload: enabled: true strategy: refresh mode: event config: name: $&#123;spring.application.name&#125; namespace: default sources: - name: $&#123;spring.application.name&#125; namespace: system-server 对于这些，在前面的文章说过，我们需要依赖配置： 123456789101112131415161718192021222324252627282930313233343536373839&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-webflux&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes-config&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-kubernetes-discovery&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes-loadbalancer&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.squareup.okhttp3&lt;&#x2F;groupId&gt; &lt;artifactId&gt;okhttp&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt; 这里没有用 Ribbon 的，直接使用 spring-cloud-starter-kubernetes-loadbalancer，但我们还是利用 RestTemplate： 123456789@LoadBalanced@Beanpublic RestTemplate restTemplate() &#123; SimpleClientHttpRequestFactory requestFactory &#x3D; new SimpleClientHttpRequestFactory(); requestFactory.setReadTimeout(env.getProperty(&quot;client.http.request.readTimeout&quot;, Integer.class, 15000)); requestFactory.setConnectTimeout(env.getProperty(&quot;client.http.request.connectTimeout&quot;, Integer.class, 3000)); RestTemplate rt &#x3D; new RestTemplate(requestFactory); return rt;&#125; 接下来，我们开始部署这两个应用服务了，同时，我们采用服务扩容方式实现多 pod： 1kubectl scale --replicas&#x3D;2 deployment diff-ns-service-deployment 我们苦役看看服务节点信息： 查看 Service 信息： 接下来，我们访问服务rest-service：http://192.168.8.107:5556/rest-service/getClientRes， 这里我们可以看到日志： 同时，去哦们可以看到返回结果：这里，我们请求的是获取cas-server 这个服务的 pod 的分布信息。 同样地，我们通过 Service 的 Ip 和端口也可以直接访问：http://192.168.8.107:30916/getservicedetail?servicename=cas-server-service PS:如果需要实现负载均衡，还是需要注入：@LoadBalanced，如果我们把这个注解去掉会发生什么呢？ 我们发现去掉后，竟然不能访问了。 我们再做一组测试，如果我们利用 spring.cloud.kubernetes.ribbon.mode=POD，我们来看看会有啥结果不？修改配置后，重新编译、部署，我们继续请求 urlhttp://192.168.8.107:5556/rest-service/getClientRes: 新发现如果我们引入的是基于 Spring cloud 本身的spring-cloud-starter-kubernetes-loadbalancer，同时，我们没有去掉基于 Ribbon 的 LB 的能力，如：spring.cloud.loadbalancer.ribbon.enabled=false，是有可能会报错的： 总结 Spring cloud 本身：如果是基于 Spring cloud 本身的 LB，需要隐藏 Ribbon 的能力，同时基于RestTemplate 需要注解@LoadBalanced。 k8s 本身：如果采用 Spring cloud 的负载，再结合 K8s，可以实现应用服务的 LB。如果设置spring.cloud.kubernetes.ribbon.mode=POD，其禁用了 Ribbon 的 LB 能力，此时不会生效，走的还是 Spring cloud LoadBalancer。另外对于 Service，这里都设置为 NodePort 类型，如果是默认类型是否可以实现 LB，需要待确认，因为目前来看，没有实现，可能是网络问题，并不是说默认类型的 Service 不可实现 LB。 Ribbon，基于上面，下次可以尝试基于 NodePort 类型的 Service 来实现 Ribbon 的 LB，看是否是因为 Service 的网络导致的。 实践验证在前面我们已经针对默认类型的Service进行Ribbon负载均衡测试过，发现无法对跨 NS 进行LB。接下来，我们测试下基于 NodePort 类型的Service，打开spring.cloud.loadbalancer.ribbon.enabled=true，引入依赖： 12345&lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes-ribbon&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt; 按照以上配置，我们部署服务diff-ns-service，我们发现服务启动后日志： 请求后返回日志： 结论不管怎样，Ribbon 无法解决跨 NS 的应用服务之间的相互访问。但对于 Service 类型来说，可能是网络设置问题，跟其类型无关。","tags":[]},{"title":"spring-cloud-k8s 跨 NS 的坑","date":"2021-11-11T07:06:15.000Z","path":"2021/11/11/spring-cloud-k8s-note1/","text":"回顾前面文章 (Spring Cloud Kubernetes 之实战服务注册与发现) 中，讲述了 spring-cloud-k8s 在微服务实践中，带来了多大的优势。介绍了 k8s 中资源 Service，其如何来实现服务的注册与发现。 其实在 k8s 中，Service 资源的类型比较多，有四种： ExternalName：创建一个 DNS 别名指向 service name，这样可以防止 service name 发生变化，但需要配合 DNS 插件使用。 ClusterIP：默认的类型，用于为集群内 Pod 访问时，提供的固定访问地址,默认是自动分配地址,可使用 ClusterIP 关键字指定固定 IP。 NodePort：基于 ClusterIp，用于为集群外部访问 Service 后面 Pod 提供访问接入端口。 LoadBalancer：它是基于 NodePort。 我们一般会默认使用的类型：ClusterIP，但此时会出现一种问题，那就是此类型的 Service 被用来访问非同一 NS 下的 pods，即&lt;servicename&gt;.&lt;namespace&gt;.svc.cluster.local形式访问 pod，只能通过 servicename 直接访问同一 namespace 下的 pod。 案例下面，我们来看案例：假设我这里有三个服务：cas-server、rest-service、diff-ns-service 等，我通过 deployment 来部署这些服务的 pod。 可以看到这些 pod 处于 不同的 namespace 下，同样的对应的 service 也是处于对应的 namespace 下： 123ns-app diff-ns-service-service ClusterIP 10.16.178.187 &lt;none&gt; 2008&#x2F;TCP 6h39m app&#x3D;diff-ns-servicesystem-server cas-server-service ClusterIP 10.16.134.168 &lt;none&gt; 2000&#x2F;TCP 16d app&#x3D;cas-serversystem-server rest-service-service ClusterIP 10.16.128.58 &lt;none&gt; 2001&#x2F;TCP 16d app&#x3D;rest-service 这里的 Service 类型都是 ClusterIp,在前面，我们验证过基于这样的服务，我们可以利用 springcloud-k8s 来实现同一 namespace 下服务之间的注册与发现，实现负载均衡。但如果不在同一 namespace 下呢？比如这里的diff-ns-service，它与另外两个服务不在同一 namespace。此时我们通过基于 Ribbon 的负载均衡策略。这是因为我们默认了 KubernetesRibbonMode 的模式：POD，就是获取服务提供者的 pod 的 ip 和 port，该 ip 是 kubernetes 集群的内部 ip，只要服务消费者是部署在同一个 kubernetes 集群内就能通过 pod 的 ip 和服务提供者暴露的端口访问。当我们使用当mode为SERVICE时，就是获取服务提供者在 k8s 中的 service 的名称和端口，使用这种模式会导致 Ribbon 的负载均衡失效，转而使用 k8s 的负载均衡。 所以，如果不使用默认的 Ribbon 来实现负载均衡，可以配置： 12345spring: cloud: kubernetes: ribbon: mode: SERVICE 这个前提其实还是在同一 namespace 下，但如果不在同一 NS 呢？还是设置为SERVICE模式，但里面还是用 k8s 原生的调用方式：&lt;servicename&gt;.&lt;namespace&gt;.svc.cluster.local***，假设这里需要调用 diff-ns-service,则： 12ResponseEntity&lt;String&gt; responseEntity &#x3D; new RestTemplate().exchange(&quot;http:&#x2F;&#x2F;diff-ns-service-service&#x2F;getservicedetail?servicename&#x3D;cas-server-service&quot;, HttpMethod.GET, formEntity, String.class); 访问请求该服务时，发现并未请求到： 12342021-11-04 09:23:16.830:147 [http-nio2-2001-exec-2] DEBUG org.springframework.web.client.RestTemplate -HTTP GET http:&#x2F;&#x2F;diff-ns-service-service&#x2F;getservicedetail?servicename&#x3D;cas-server-service2021-11-04 09:23:16.834:147 [http-nio2-2001-exec-2] DEBUG org.springframework.web.client.RestTemplate -Accept&#x3D;[text&#x2F;plain, application&#x2F;json, application&#x2F;*+json, *&#x2F;*]2021-11-04 09:23:16.859:255 [http-nio2-2001-exec-2] DEBUG org.springframework.web.servlet.mvc.method.annotation.RequestResponseBodyMethodProcessor -Using &#39;text&#x2F;html&#39;, given [text&#x2F;html, application&#x2F;xhtml+xml, image&#x2F;avif, image&#x2F;webp, image&#x2F;apng, application&#x2F;xml;q&#x3D;0.9, application&#x2F;signed-exchange;v&#x3D;b3;q&#x3D;0.9, *&#x2F;*;q&#x3D;0.8] and supported [text&#x2F;plain, *&#x2F;*, text&#x2F;plain, *&#x2F;*, application&#x2F;json, application&#x2F;*+json, application&#x2F;json, application&#x2F;*+json]2021-11-04 09:23:16.860:91 [http-nio2-2001-exec-2] DEBUG org.springframework.web.servlet.mvc.method.annotation.RequestResponseBodyMethodProcessor -Writing [&quot;&quot;] 如果走 mode 为POD的 Ribbon 的负载均衡： 123456789102021-11-04 09:34:32.188:147 [http-nio2-2001-exec-2] DEBUG org.springframework.web.client.RestTemplate -HTTP GET http:&#x2F;&#x2F;diff-ns-service-service&#x2F;getservicedetail?servicename&#x3D;cas-server-service2021-11-04 09:34:32.193:147 [http-nio2-2001-exec-2] DEBUG org.springframework.web.client.RestTemplate -Accept&#x3D;[text&#x2F;plain, application&#x2F;json, application&#x2F;*+json, *&#x2F;*]2021-11-04 09:34:32.261:115 [http-nio2-2001-exec-2] INFO com.netflix.config.ChainedDynamicProperty -Flipping property: diff-ns-service-service.ribbon.ActiveConnectionsLimit to use NEXT property: niws.loadbalancer.availabilityFilteringRule.activeConnectionsLimit &#x3D; 21474836472021-11-04 09:34:32.271:197 [http-nio2-2001-exec-2] INFO com.netflix.loadbalancer.BaseLoadBalancer -Client: diff-ns-service-service instantiated a LoadBalancer: DynamicServerListLoadBalancer:&#123;NFLoadBalancer:name&#x3D;diff-ns-service-service,current list of Servers&#x3D;[],Load balancer stats&#x3D;Zone stats: &#123;&#125;,Server stats: []&#125;ServerList:null2021-11-04 09:34:32.278:222 [http-nio2-2001-exec-2] INFO com.netflix.loadbalancer.DynamicServerListLoadBalancer -Using serverListUpdater PollingServerListUpdater2021-11-04 09:34:32.281:88 [http-nio2-2001-exec-2] WARN org.springframework.cloud.kubernetes.ribbon.KubernetesEndpointsServerList -Did not find any endpoints in ribbon in namespace [system-server] for name [diff-ns-service-service] and portName [null]2021-11-04 09:34:32.282:150 [http-nio2-2001-exec-2] INFO com.netflix.loadbalancer.DynamicServerListLoadBalancer -DynamicServerListLoadBalancer for client diff-ns-service-service initialized: DynamicServerListLoadBalancer:&#123;NFLoadBalancer:name&#x3D;diff-ns-service-service,current list of Servers&#x3D;[],Load balancer stats&#x3D;Zone stats: &#123;&#125;,Server stats: []&#125;ServerList:org.springframework.cloud.kubernetes.ribbon.KubernetesEndpointsServerList@d48bf012021-11-04 09:34:32.308:255 [http-nio2-2001-exec-2] DEBUG org.springframework.web.servlet.mvc.method.annotation.RequestResponseBodyMethodProcessor -Using &#39;text&#x2F;html&#39;, given [text&#x2F;html, application&#x2F;xhtml+xml, image&#x2F;avif, image&#x2F;webp, image&#x2F;apng, application&#x2F;xml;q&#x3D;0.9, application&#x2F;signed-exchange;v&#x3D;b3;q&#x3D;0.9, *&#x2F;*;q&#x3D;0.8] and supported [text&#x2F;plain, *&#x2F;*, text&#x2F;plain, *&#x2F;*, application&#x2F;json, application&#x2F;*+json, application&#x2F;json, application&#x2F;*+json]2021-11-04 09:34:32.309:91 [http-nio2-2001-exec-2] DEBUG org.springframework.web.servlet.mvc.method.annotation.RequestResponseBodyMethodProcessor -Writing [&quot;&quot;]2021-11-04 09:34:32.316:1131 [http-nio2-2001-exec-2] DEBUG org.springframework.web.servlet.DispatcherServlet -Completed 200 OK 此时，给我的感觉就是： 12345678spring: cloud: kubernetes: ribbon: #直接走k8s的LB mode: SERVICE #POD走ribbon的LB discovery: all-namespaces: true 此类配置是无法进行 Service 到应用服务的访问，只能访问到 Service。同时我们看到日志： 12021-11-04 09:34:32.281:88 [http-nio2-2001-exec-2] WARN org.springframework.cloud.kubernetes.ribbon.KubernetesEndpointsServerList -Did not find any endpoints in ribbon in namespace [system-server] for name [diff-ns-service-service] and portName [null] 上面给到的是 mode 为 POD 时，走的 Ribbon 的负载均衡后，无法找到当前 pod 对应的 NS 下的 Servcie 为 diff-ns-service-service 的服务。 12021-11-04 09:20:27.109:89 [PollingServerListUpdater-1] WARN org.springframework.cloud.kubernetes.ribbon.KubernetesServicesServerList -Did not find any service in ribbon in namespace [system-server] for name [diff-ns-service-service] and portName [null] 同样地，当 mode 为 SERVICE 时，依然无法找到当前 pod 的对应的 NS 的 Servcie 为 diff-ns-service-service 的服务。 同样会拿不到请求返回信息，这里说明：在不同NS下，Service为ClusterIP，不管如何负载均衡，都无法访问。 解决方案一、通过 Springcloud k8s 社区来实现跨 NS 下的服务的相互访问的简单策略二、走 K8s 的原生的负载均衡策略从前面的分析可以看到：虽然 spring-cloud-k8s 帮我们发现了 Service，但在底层策略时，不同的NS还是做了隔离，只能通过 k8s 原生的方式来进行服务的发现：&lt;servicename&gt;.&lt;namespace&gt;.svc.cluster.local PS：同时，我们需要注意的是，此时基于 k8s 负载均衡，我们不能再基于 Ribbon 或其他来进行负载均衡机制了，直接通过 Http 协议来请求 k8s 的 service，实现跨 NS 的 pod 之间的互通。","tags":[]},{"title":"Springcloud Oauth2 HA篇","date":"2021-10-11T07:17:58.000Z","path":"2021/10/11/oauth-ha/","text":"在 浅谈微服务安全架构设计 一文中，介绍了基于Springcloud 结合了Oauth2分析了其各种模式下的鉴权认证，今天主要分析如何结合k8s作实现鉴权的高可用。 假设我们的项目中有几个模块： 鉴权中心：Oauth2服务 订单系统：客户端A 用户管理系统：客户端B在上面的系统中，每个服务之间的耦合性很低，但是又有着很频繁的调用，这就涉及到UI与其之间的频繁流量交互。如何做到其HA，这里引入k8s的Service方法： 在 Spring Cloud Kubernetes之实战服务注册与发现一文中，就讲解了k8s的Service方式创建服务，然后可以部署多个pod，同时结合 Spring Cloud Kubernetes之实战网关Gateway 来实现LB，类似通过域名来解析其服务，并根据所定义的规则进行LB。同样，本文则是Oauth2的基础上，结合这些来实现微服务的LB。同时此处利用了k8s来作主要处理，如果是其他语言(Python、Go、Rust等)的客户端服务，则自身可以通过逻辑来控制其鉴权以及获取流量的。 注意点：由于各微服务与鉴权中心有交互，故鉴权中心需要提供HA服务，即先在启动类加入@EnableDiscoveryClient ，后续在注入bean时，@LoadBalanced来实现LB鉴权中心。 12345678910111213@EnableOAuth2Sso@Configuration@EnableAutoConfiguration@ComponentScan(basePackages &#x3D; &#123;&quot;com.damon&quot;&#125;)@EnableConfigurationProperties(EnvConfig.class)@EnableDiscoveryClient #为LB多节点鉴权中心准备public class AdminApp &#123; public static void main(String[] args) &#123; SpringApplication.run(AdminApp.class, args); &#125;&#125; 在客户端项目模块中，调用鉴权中心时，需要实现LB： 1234567891011121314151617@Configurationpublic class BeansConfig &#123; @Resource private Environment env; @LoadBalanced @Bean public RestTemplate restTemplate() &#123; SimpleClientHttpRequestFactory requestFactory &#x3D; new SimpleClientHttpRequestFactory(); requestFactory.setReadTimeout(env.getProperty(&quot;client.http.request.readTimeout&quot;, Integer.class, 15000)); requestFactory.setConnectTimeout(env.getProperty(&quot;client.http.request.connectTimeout&quot;, Integer.class, 3000)); RestTemplate rt &#x3D; new RestTemplate(requestFactory); return rt; &#125;&#125; 另外本身在配置交互的时候，需要加上域名等形式来实现LB，这里利用了k8s的Service来实现。 1234567891011121314cas-server-url: http:&#x2F;&#x2F;cas-server-service #这里配置成HA地址security: oauth2: #与cas-server对应的配置 client: client-id: admin-web client-secret: admin-web-123 user-authorization-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;authorize #是授权码认证方式需要的 access-token-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;token #是密码模式需要用到的获取 token 的接口 resource: loadBalanced: true id: admin-web user-info-uri: $&#123;cas-server-url&#125;&#x2F;api&#x2F;user #指定user info的URI prefer-token-info: false 这样，一个客户端关于鉴权的核心就是如此了，同样需要把消费客户端以service形式提供给UI，此时需要借助 Spring Cloud Kubernetes之实战网关Gateway 和nginx代理服务，我们来测试下：curl -X POST -d &quot;username=admin&amp;password=123456&amp;grant_type=password&amp;client_id=admin-web&amp;client_secret=admin-web-123&quot; http://192.168.8.10:5556/cas-server/oauth/token 看到结果： 1&#123;&quot;access_token&quot;:&quot;5a7892b0-7483-4f60-89fd-44255a429ff6&quot;,&quot;token_type&quot;:&quot;bearer&quot;,&quot;refresh_token&quot;:&quot;23f2e8ea-f091-4ab0-822c-f28bebc4ec08&quot;,&quot;expires_in&quot;:3599,&quot;scope&quot;:&quot;all&quot;&#125; 通过获取到的access_token来访问对应的客户端：curl -H &quot;Accept: application/json&quot; -H &quot;Authorization:bearer 5a7892b0-7483-4f60-89fd-44255a429ff6&quot; -X GET http://192.168.8.10:5556/admin-web/api/user/getCurrentUser 输出结果： 1&#123;&quot;authorities&quot;:[&#123;&quot;authority&quot;:&quot;admin&quot;&#125;],&quot;details&quot;:&#123;&quot;remoteAddress&quot;:&quot;10.244.0.196&quot;,&quot;sessionId&quot;:null,&quot;tokenValue&quot;:&quot;5a7892b0-7483-4f60-89fd-44255a429ff6&quot;,&quot;tokenType&quot;:&quot;bearer&quot;,&quot;decodedDetails&quot;:null&#125;,&quot;authenticated&quot;:true,&quot;userAuthentication&quot;:&#123;&quot;authorities&quot;:[&#123;&quot;authority&quot;:&quot;admin&quot;&#125;],&quot;details&quot;:&#123;&quot;authorities&quot;:[&#123;&quot;authority&quot;:&quot;admin&quot;&#125;],&quot;details&quot;:&#123;&quot;remoteAddress&quot;:&quot;10.244.0.201&quot;,&quot;sessionId&quot;:null,&quot;tokenValue&quot;:&quot;5a7892b0-7483-4f60-89fd-44255a429ff6&quot;,&quot;tokenType&quot;:&quot;Bearer&quot;,&quot;decodedDetails&quot;:null&#125;,&quot;authenticated&quot;:true,&quot;userAuthentication&quot;:&#123;&quot;authorities&quot;:[&#123;&quot;authority&quot;:&quot;admin&quot;&#125;],&quot;details&quot;:&#123;&quot;client_secret&quot;:&quot;admin-web-123&quot;,&quot;grant_type&quot;:&quot;password&quot;,&quot;client_id&quot;:&quot;admin-web&quot;,&quot;username&quot;:&quot;admin&quot;&#125;,&quot;authenticated&quot;:true,&quot;principal&quot;:&#123;&quot;password&quot;:null,&quot;username&quot;:&quot;admin&quot;,&quot;authorities&quot;:[&#123;&quot;authority&quot;:&quot;admin&quot;&#125;],&quot;accountNonExpired&quot;:true,&quot;accountNonLocked&quot;:true,&quot;credentialsNonExpired&quot;:true,&quot;enabled&quot;:true&#125;,&quot;credentials&quot;:null,&quot;name&quot;:&quot;admin&quot;&#125;,&quot;oauth2Request&quot;:&#123;&quot;clientId&quot;:&quot;admin-web&quot;,&quot;scope&quot;:[&quot;all&quot;],&quot;requestParameters&quot;:&#123;&quot;grant_type&quot;:&quot;password&quot;,&quot;client_id&quot;:&quot;admin-web&quot;,&quot;username&quot;:&quot;admin&quot;&#125;,&quot;resourceIds&quot;:[],&quot;authorities&quot;:[],&quot;approved&quot;:true,&quot;refresh&quot;:false,&quot;redirectUri&quot;:null,&quot;responseTypes&quot;:[],&quot;extensions&quot;:&#123;&#125;,&quot;grantType&quot;:&quot;password&quot;,&quot;refreshTokenRequest&quot;:null&#125;……… 最后，这里鉴权的高可用通过k8s的service，进行默认的轮询方式的访问鉴权中心，鉴权中心如果鉴权时不管使用redis还是jwt，来管理token，都是可以的。","tags":[]},{"title":"Jupyter Notebook 的使用","date":"2021-07-12T09:45:47.000Z","path":"2021/07/12/jupyter/","text":"深度学习编程常用工具我们先来看 4 个常用的编程工具：Sublime Text、Vim、Jupyter。虽然我介绍的是 Jupyter，但并不是要求你必须使用它，你也可以根据自己的喜好自由选择。 Sublime Text第一个是 Sublime Text，它是一个非常轻量且强大的文本编辑工具，内置了很多快捷的功能，同时还支持很丰富的插件功能，对我们来说非常方便。 如上图所示，它可以自动为项目中的类、方法和函数生成索引，我们让我们可以跟踪代码。可以通过它的 goto anything 功能，根据一些关键字查找到项目中的对应的代码行。 Vim第二个是 Vim，它是 Linux 系统中的文本编辑工具，方便快捷且强大，我们在项目中经常会使用到。 在我们的项目中，经常需要登录到服务器上进行开发，而服务器一般都是 Linux 系统，不会有 Sublime Text 与 Pycharm，所以我们可以直接用 Vim 打开代码进行编辑。对于没有接触过 Linux 或者是一直使用 IDE 进行编程开发的同学，一开始可能觉得不是很方便，但 Vim 的快捷键十分丰富，对于 Shell 与 Python 的开发来说非常便捷。 Vim 的缺点正如刚才所说，有一点点门槛，需要你去学习它的使用方法。只要你学会了，我保证你将对它爱不释手。 Jupyter Notebook &amp; Lab最后一个是今天要介绍的 Jupyter Notebook 了，它是一个开源的 Web 应用，能够让你创建、分享包含可执行代码、可视化结构和文字说明的文档。 Jupyter Notebook 的应用非常广泛，它可以用在数据清理与转换、数字模拟、统计模型、数据可视化、机器学习等方面。 Jupyter Notebooks 非常活跃于深度学习领域。在项目的实验测试阶段，它相比于用 py 文件来直接编程还是方便一些。在项目结束之后如果要写项目报告，用 Jupyter 也比较合适。 简单介绍之后，我们接下来就从 Jupyter 的功能、Jupyter 的安装与启动与 Jupyter Lab 的操作这 3 个方面学习 Jupyter。 Jupyter Notebook &amp; Lab 的功能Jupyter 主要有以下 3 点的作用：执行代码、数据可视化以及使用 Markdown 功能写报告。 执行代码。一般是 Python 程序，也可以添加新的编程语言。 数据可视化。设想一下，我们经常在 Linux 环境编程开发，如果需要对数据可视化该怎么办呢？是不是只能把图片保存下来，然后下载到本地进行查看？使用 Jupyter Notebook 就不用多此一举，我们可以直接在页面中查看。如下图所示： 使用 Markdown 功能写文档，或者制作 PPT。这些文档中还包含代码以及代码执行后的结果，非常有助于你书写项目报告。 Jupyter Notebook &amp; Lab 的安装与启动了解了 Jupyter 的功能之后，我们来看看具体要如何进行安装与启动。这一节我介绍了 3 种安装和启动的方式，分别是 Anaconda、Docker 和 pip。 使用 Anaconda 安装与启动我们先来看如何使用 Anaconda 来安装与启动。 安装最简单的方法是通过安装 Anaconda 来使用 Jupyter Notebook &amp; Lab。Anaconda 已自动安装了 Jupter Notebook 及其他工具，还有 Python 中超过 180 个科学包及其依赖项。你可以通过 Anaconda 的官方网站得到 Anaconda 的下载工具。 启动这里我会分 MacOS 系统和 Win 环境来讲解。 （1）MacOS 系统 安装完 Anaconda 之后，打开终端后系统会默认进入 base 环境。 在命令行最前面有个(base)的标志则表示代码进入 base 环境了，如果没有就需要通过下面的命令激活 base 环境： 1conda activate base 在 base 环境下执行下面的命令，会自动进入 Jupyte Notebook 的开发环境。 1jupyter notebook 执行下面的命令，则会自动进入到 Jupyter Lab 的开发环境。 1jupyter lab （2）Win 环境 Windows 环境中的启动方式与 MacOS 基本一样。 当你想通过命令 Jupyter Notebook 或 Jupyter Lab 启动时，你需要在 Anaconda Prompt 中执行。 通过 Anaconda Navigator 启动的方式与 MacOS 一样。 使用 Docker通过 Docker 使用 Jupyter 也非常简单，连安装都不需要，但前提是你要有 Docker 相关的知识。 使用 pip 安装与启动了解完 Anaconda 和 Docker 的安装与启动方式后，我们最后来看 pip 是如何安装和启动的。 安装通过 pip 安装 Jupyter Notebook： 1pip install Jupyter 通过 pip 安装 Jupyter Lab： 1pip install Jupyterlab 启动安装完成后，直接在终端执行 Jupyter Notebok 或 Jupyter Lab 命令启动。 不管在 MacOS 系统还是在 Windows 系统，通过以上任意一种方式成功启动后，浏览器都会自动打开 Jupyter Notebook 或 Jupyter Lab 的开发环境: Jupyter Lab 的操作Jupyter Lab 是 Jupyter Notebook 的下一代产品，在使用方式上更为灵活、便捷。 我们在命令行或者 Anaconda Navigator 中启动 Jupyter Lab 之后，浏览器会自动打开如下所示的 Jupyter Lab 界面： 最左侧显示的是你启动时所在的目录，右侧是你可以使用的一些开发工具。 Notebook点击 Notebook 下面的“Python 3”的图标之后，就会自动新建一个 Notebook。 Jypter Lab 与 Jupyter Notebook 中都会用到这个叫作 Notebook 的编辑工具。 Jupyter Lab 与 Jupyter Notebook 不同的地方是 IDE 的界面以及操作方式，这里讲解用的是 Jupyter Lab 的操作。 一个 Notebook 的编辑界面主要由 4 个部分组成：菜单栏、工具栏、单元格（Cell）以及内核。如下图所示： 菜单栏与工具栏这里就不详细介绍了。我们先来看单元格（Cell），然后再介绍内核。 单元格（Cell）单元格是我们 Notebook 的主要内容，这里我会介绍两种单元格。 Code 单元格：包含可以在内核运行的代码，并且在单元格下方输出运行结果。 Markdown 单元格：包含运用 Markdown 的文档，常用于文档的说明，也是可以运行的单元格。 从 Code 单元格切换到 Markdown 单元格的切换的快捷键是 m；从 Markdown 单元格切换到 Code 单元格的切换的快捷键是 y。 切换之前需要先按 Esc，从单元格的编辑状态中退出。 在工具栏中也可以切换，但是还是快捷键方便些。工具栏的位置在下图中红框的位置： 我们看一个例子。我编辑了下面的 Notebook。第一行是 1 个 Markdown 单元格，是 1 个一级标题，第二行是 1 个 Python 的代码。两行代码都是未运行状态。 你注意到左边那个蓝色的竖条了吗？它代表我们所在的单元格。 我们在编辑这个单元格的时候，左边是绿色的竖条。如果我们按 Esc 退出单元格，它就会变为蓝色。 退出单元格后，我们可以通过上下键移动选中的单元格。我们移动到第一行，然后开始运行这两个单元格。 运行单独一个单元格的快捷键 Ctrl+Enter，运行选中单元格并切换到下一个单元格的快捷键是 Shift + Enter。运行结果如下图所示： Markdown 没有左边的“[]”标签，通过这一点你可以区分 Code 单元格与 Markdown 单元格。 “[]”中的数字代表单元格被执行的顺序，例子中“[1]”代表第一个被执行的单元格。 以上就是单元格的内容了。我们接下来看看，单元格中的一些快捷键的使用。 （1）快捷键 如果你是用 Jupyter 进行开发，掌握单元格的快捷键能让你的开发速度变得更快，下面我列举了几个常用的快捷键： 执行单元格 Ctrl+Enter 或 Shift+Enter； a 在单元格上方插入新的单元格； b 在单元格下方插入新的单元格； x 删除单元格； z 撤销删除的单元格。 （2）Magic 命令 Jupyter Notebook 的前身是 IPython Notebook，所以 Jupyter 也支持 IPython 的 Magic 命令。IPython 是一个比 Python 自带的 Shell 更加灵活方便的 Shell，它主要活跃于数据科学领域。 Magic 命令分两种： Line Magics 命令：在命令前面加%，表示只在本行有效 Cell Magics 命令：在命令前面加%%，表示在整个 Cell 单元有效。 下面我介绍几个常用的 Magic 命令。 %lsmagic：用来查看可以使用的 Magic 命令。 %matplotlib inline：可以在单元格下面直接打印出 matplotlib 的图标，通常要在 matplotlib 模块引入之前使用；使用这个 Magic 命令之后，可以不用 plt.show()。 %pwd：查看当前的文件路径。 %%writefile：写文件，%%writefile 后面紧跟着文件名，然后下面写文件的内容。 %run： 运行一个文件，%run 后面跟着要运行的文件。 %load：加载文件。使用%load + 文件名可以把指定的文件加载到单元格内。请看下面的例子，我们要把 temp.py 加载到单元格里，首先是执行前， （3）Markdown 命令 了解了 Magic 命令后，我们再来看 Markdown 命令。Markdown 是一种在 Markdown 单元中用于格式化文本的语言，常用于 Notebook 的文档说明，我们列举了几个常用的命令。 标题：通过井号的数目可以决定标题的大小。 123456789# 一级标题：## 二级标题：### 三级标题：#### 四级标题：##### 五级标题： 列表：分为无序列表与有序列表。 1234567891011## 无序列表- 项目 1- 项目 2## 有序列表1. 项目 1 (1. 与项目 1 之间有一个空格)2. 项目 2 字体：可以通过”*”或者_的数目控制强调的内容，即斜体、加粗以及粗斜体。具体的请看下面的例子。12345678910111213*斜体***加粗*****粗斜体***或者_斜体___加粗_____粗斜体___ （4）调用系统命令 最后，在 Notebook 中还可以调用所在操作系统的命令，只需要在命令前加一个“!”就可以了。例如，在 Linux 系统中查看当前路径： 1!pwd","tags":[]},{"title":"云原生微服务架构实践","date":"2021-05-06T12:58:51.000Z","path":"2021/05/06/micro-cloud-native/","text":"前言写作本书的目的&emsp;&emsp;微服务架构已经火了很多年了，如：Dubbo、Spring Cloud，再到后来的 Spring Cloud Alibaba，但都是仅限于 Java 语言的瓶颈，如何让各种语言之间的微服务更加有效、快速的通讯，这是当前很多企业需要面临的问题，因为一个企业中，不只是基于单纯的某一种语言开发，这就涉及到多语言服务之间的访问。本书的创作重点，则是在于讲述在巨多语言的情况下，该如何设计微服务架构，以及云原生时代的微服务的高可用、自动化等等。 如何阅读本书&emsp;&emsp;由于本书介绍的一些知识都是比较流行的，最近几年很火的，而且本身其涉及的面还是很广的：K8s，大家可以通过其官网系统的学习其相关技术与实践，以便可以更好的将其发挥于微服务架构设计中。另外，本书中介绍的知识，大家都可以一边实践、一边阅读，以便更深刻的理解其原理。 扫码关注作者 作者毕业于三流院校，笔名：Damon，拥有七年工作经验，技术爱好者，长期从事 Java 开发、Spring Cloud、Golang 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号：程序猿Damon，个站：www.damon8.cn。 欢迎扫码回复「入群」加入技术交流群 目录第一部分 基础知识第一章 什么是微服务架构1.1 微服务到底是啥1.2 微服务的发展史1.3 微服务的革命性与重要性第二章 微服务的拆分2.1 微服务的设计原则2.2 微服务划分的粒度2.3 不同场景的微服务第三章 容器化技术3.1 什么是容器3.2 容器的发展进程3.3 Docker 与 Kubernetes3.4 K8s 容器化应用第四章 为何借助容器助力微服务4.1 微服务的多语言性4.2 微服务的高可用4.3 微服务的复杂性第二部分 原理与应用第五章 Kubernetes 介绍5.1 Kubernetes 的基本概念与特性5.2 部署 Kubernetes 集群5.3 Kubernetes 的组件及及负载均衡第六章 为什么选择 Kubernetes6.1 Kubernetes 与微服务的天生绝配6.2 基于 Kubernetes 集群的服务治理6.3 基于 Kubernetes 的服务无缝迁移第七章 第一个基于 K8s 的多语言微服务架构7.1 基于 K8s 的 Java 微服务7.2 第一个Golang微服务7.3 部署微服务应用 正文第一部分 基础知识第一章 什么是微服务架构1.1 微服务的发展史&emsp;&emsp;在微服务到来之前，单体应用程序所暴露的缺点主要有： 复杂性高 团队协作开发成本高 扩展性差 部署效率低下 系统很差的高可用性 &emsp;&emsp;复杂性，体现在：随着业务的不断迭代，项目的代码量急剧的增多，项目模块也会随着而增加，整个项目就会变成的非常复杂。 &emsp;&emsp;开发成本高，体现在：团队开发几十个人在修改代码，然后一起合并到同一地址分支，打包部署，测试阶段只要有一小块功能有问题，就得重新编译打包部署，重新测试，所有相关的开发人员都得参与其中，效率低下，开发成本极高。 &emsp;&emsp;扩展性差，体现在：在新增功能业务的时候，代码层面会考虑在不影响现有的业务基础上编写代码，提高了代码的复杂性。 &emsp;&emsp;部署效率低，体现在：当单体应用的代码越来越多，依赖的资源越来越多时，应用编译打包、部署测试一次，需要花费的时间越来越多，导致部署效率低下。 &emsp;&emsp;高可用差，体现在：由于所有的业务功能最后都部署到同一个文件，一旦某一功能涉及的代码或者资源有问题，那就会影响到整个文件包部署的功能。举个特别鲜明的示例：上世纪八、九十年代，很多的黄页以及延伸到后来的网站中，很多的展示页面与获取数据的后端都是在一个服务模块中。这就造成一个很不好的影响：如果只是修改极小部分的页面展示或图片展示，则需要把整个服务模块进行打包部署，这样会导致时间的严重浪费以及成本的增加。更加糟糕的是，给用户带来非常不好的体验，用户无法理解的是：只是换个网站的某块微小的展示区，导致了整个网站在那一时刻无法正常的访问。当然，也许，对于那个时候互联网的不发达，人们对于这样的体验，已经算是一种幸福的享受了。 &emsp;&emsp;由于单体应用具有以上的种种缺点，导致了一个新名词、新概念的诞生：微服务。 1.2 微服务到底是啥&emsp;&emsp;其实，从早年间的单体应用，到 2014 年起，得益于以 Docker 为代表的容器化技术的成熟以及 DevOps 文化的兴起，服务化的思想进一步演化，演变为今天我们所熟知的微服务。那么，微服务到底是啥？ &emsp;&emsp;微服务，英文名：microservice，百度百科上将其定义为：SOA 架构的一种变体。微服务（或微服务架构）是一种将应用程序构造为一组低耦合的服务。 &emsp;&emsp;微服务有着一些鲜明的特点： 功能单一 服务粒度小 服务间独立性强 服务间依赖性弱 服务独立维护 服务独立部署 &emsp;&emsp;对于每一个微服务来说，其提供的功能应该是单一的；其粒度很小的；它只会提供某一业务功能涉及到的相关接口。如：电商系统中的订单系统、支付系统、产品系统等，每一个系统服务都只是做该系统独立的功能，不会涉及到不属于它的功能逻辑。 &emsp;&emsp;微服务之间的依赖性应该是尽量弱的，这样带来的好处是：不会因为单一系统服务的宕机，而导致其它系统无法正常运行，从而影响用户的体验。同样以电商系统为例：用户将商品加入购物车后，提交订单，这时候去支付，发现无法支付，此时，可以将订单进入待支付状态，从而防止订单的丢失和用户体验的不友好。如果订单系统与支付系统的强依赖性，会导致订单系统一直在等待支付系统的回应，这样会导致用户的界面始终处于加载状态，从而导致用户无法进行任何操作。 &emsp;&emsp;当出现某个微服务的功能需要升级，或某个功能需要修复 bug 时，只需要把当前的服务进行编译、部署即可，不需要一个个打包整个产品业务功能的巨多服务，独立维护、独立部署。 &emsp;&emsp;上面描述的微服务，其实突出其鲜明特性：高内聚、低耦合，问题来了。什么是高内聚，什么是低耦合呢？所谓高内聚：就是说每个服务处于同一个网络或网域下，而且相对于外部，整个的是一个封闭的、安全的盒子。盒子对外的接口是不变的，盒子内部各模块之间的接口也是不变的，但是各模块内部的内容可以更改。模块只对外暴露最小限度的接口，避免强依赖关系。增删一个模块，应该只会影响有依赖关系的相关模块，无关的不应该受影响。 &emsp;&emsp;所谓低耦合：从小的角度来看，就是要每个 Java 类之间的耦合性降低，多用接口，利用 Java 面向对象编程思想的封装、继承、多态，隐藏实现细节。从模块之间来讲，就是要每个模块之间的关系降低，减少冗余、重复、交叉的复杂度，模块功能划分尽可能单一。 1.3 微服务的革命性与重要性&emsp;&emsp;上一小节讲述了什么是微服务，微服务的鲜明特性。其实，从单体应用看微服务，就能看出微服务的重要性，它是彻底改革了应用程序的惯性，它的设计理念的出现：让开发人员减少大量的开发成本以及修复成本；让产品的使用者拥有一种舒适的体验感。它解决了单体应用程序的很多难以解决的问题，更具有创新性。它让我们的系统尽可能快地响应变化。 &emsp;&emsp;微服务将原来耦合在一起的复杂业务拆分为单个服务，规避了原本复杂度无止境的积累，每一个微服务专注于单一功能，并通过定义良好的接口清晰表述服务边界。 &emsp;&emsp;由于微服务具备独立的运行进程，所以每个微服务可以独立部署。当业务迭代时只需要发布相关服务的迭代即可，降低了测试的工作量同时也降低了服务发布的风险。 &emsp;&emsp;在微服务架构下，当某一组件发生故障时，故障会被隔离在单个服务中。如通过限流、熔断等方式降低错误导致的危害，保障核心业务的正常运行。 第二章 微服务的拆分2.1 微服务的设计原则 高内聚、低耦合 &emsp;&emsp;紧密关联的事应该放在一起，每个服务是针对一个单一职责的业务能力的封装，需要专注于做好一件事情。这样避免内容耦合，降低代码的冗余，提高代码的可复用性。 &emsp;&emsp;避免服务之间的数据库的共享。这样既可以减少数据库的并发操作，又可以避免死锁的出现。通常微服务不会直接共用一个数据库，可以通过主、从表的方式来进行，结合读、写分离，实现数据的共享。 &emsp;&emsp;微服务之间应该是轻量级的通信方式，主要是为了解耦，降低业务的复杂性，以及服务的负载。一个服务调用另一个服务应该是不受到后者的牵制，如果后者发生宕机，前者应该照样继续运行下去，这才是微服务设计时的合理安排。为了降低前者的不受牵制，这就需要轻量级的通信，比如：异步调用、重试策略等。 以业务为中心 &emsp;&emsp;每个服务代表了特定的业务逻辑，不应该掺着其他业务的逻辑，或微不足道的、公共的逻辑。 &emsp;&emsp;围绕业务开展，主要就当前业务进行扩展。 &emsp;&emsp;能快速的响应业务的变化，需要做到接口的兼容性，兼容业务场景的变化，这样减少变动太大带来的风险。 &emsp;&emsp;隔离实现细节，让业务领域可以被重用，需要以封装接口形式来实现业务逻辑，这就涉及到代码的复用性。引用 Java 的原理：封装、继承、多态。 弹性容错设计 &emsp;&emsp;设计可容错的系统：拥抱失败，为已知的错误而设计，这主要是为了增强交互。 &emsp;&emsp;可防御的系统：服务降级、服务隔离、请求限制、防止级联错误等，这也是为了增强交互的友好。 自治和高可用 &emsp;&emsp;独立开发和业务扩展，这就涉及到服务随着业务的发展而进行兼容、合理的扩展后的高度自治。 &emsp;&emsp;独立部署、运行和高可用，避免单点的孤注一掷。 日志与监控 &emsp;&emsp;聚合系统日志、数据，从而当遇到问题时，可以深入分析原因。 &emsp;&emsp;当需要重现问题时，可以根据日志以及监控来复盘。 &emsp;&emsp;监控主要包括服务状态、请求流量、调用链、API 错误计数，结构化的日志、服务依赖关系可视化等内容，以便发现问题及时修复，实时调整系统负载，必要时进行服务降级，过载保护等等，从而让系统和环境提供高效高质量的服务。 自动化 &emsp;&emsp;降低部署和发布的难度，如：在持续集成和持续交付中，自动化编译，测试，安全扫描，打包，集成测试，部署。随着服务越来越多，在发布过程中，需要进一步自动化金丝雀部署。 &emsp;&emsp;利用 K8s 等进行服务自动弹性伸缩等。 2.2 微服务划分的粒度&emsp;&emsp;服务的划分，可以从水平的功能划分，也可从垂直的业务划分，粒度的大小，可以根据当前的产品需求来定位，最关键的是要做到：高内聚、低耦合。 &emsp;&emsp;如电商系统为例，如下图： &emsp;&emsp;电商中涉及到业务很可能是最多的，商品、库存、订单、促销、支付、会员、购物车、发票、店铺等等，这个是根据业务的不同来进行模块的划分。微服务划分的粒度一定是要有明确性的，不能因为含糊而新增一个服务模块，这样会导致功能接口的可复用性差。一个好的架构设计，肯定是可复用性很强的结构模式。我喜欢这样的一句话：微服务的边界 (粒度) 是 “决策”, 而不是个 “标准答案”。即应该将各微服务划分的方式，深度思考，周全的考量各方面的因素下，所作出的一个”最适合”的架构决策，而不是一个人芸亦芸的”标准答案“。 2.3 不同场景的微服务&emsp;&emsp;微服务的应用场景也是很多的，电商场景是比较常见的，比如阿里的体系：淘宝、支付宝、钉钉、饿了么、咸鱼、口碑等。电商场景的微服务实相对比较复杂的，所以需要更好的做好微服务的拆分以及扩展。 &emsp;&emsp;金融系统中也存在微服务的场景，比如：银行系统、证券系统、金融公司、机构。其需要考虑的重点是微服务的安全性、可靠性。 第三章 容器化技术3.1 什么是容器&emsp;&emsp;什么是容器呢？自然界的解释：容器是指用以容纳物料并以壳体为主的基本装置。但今天讲的容器也是一个容纳物质的载体。那计算机所指的容器(Container)到底是什么呢？容器是镜像（Image）的运行时实例。正如从虚拟机模板上启动 VM 一样，用户也同样可以从单个镜像上启动一个或多个容器。虚拟机和容器最大的区别是容器更快并且更轻量级，与虚拟机运行在完整的操作系统之上相比，容器会共享其所在主机的操作系统/内核。 &emsp;&emsp;为什么要用容器呢？假设你在使用一台电脑开发一个应用，而且开发环境具有特定的配置。其他开发人员身处的环境配置可能稍有不同。你正在开发的应用不止依赖于您当前的配置，还需要某些特定的库、依赖项和文件。与此同时，你的企业还拥有标准化的开发和生产环境，有着自己的配置和一系列支持文件。你希望尽可能多在本地模拟这些环境，而不产生重新创建服务器环境的开销。这时候，就会需要容器来模拟这些环境。 &emsp;&emsp;我们常见的容器启动方式是 Docker，Docker 是一个开源的应用容器引擎，基于 Go 语言 并遵从 Apache2.0 协议开源。Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何 Linux 机器上，也可以实现虚拟化。 3.2 容器的发展进程&emsp;&emsp;2010 年，几个年轻小伙在旧金山成立了一家做 PaaS 平台的公司，起名为”dotCloud”，该公司主要是基于 PaaS 平台为开发者或开发商提供技术服务。他们提供了对多种运行环境支持。但随着市场接受度、规模、加上科技巨头等影响，有一天 dotCloud 的创始人 Solomon Hykes 就召集了公司核心开发人员，商量准备开源 Docker 技术。因此，在 2013 年 3 月，Docker 正式以开源软件形式在 pycon 网站(见下图)首次发布了。正式由于这次开源，让容器领域焕发了第二春。后来在美国，几乎所有的云计算厂商都在拥抱 Docker 这个生态圈。很快 Docker 技术风靡全球，于是，dotCloud 决定改名为 Docker Inc(下面简称”Docker”)，全身心投入到 Docker 的开发中。更名后的 Docker 并于 2014 年 8 月，Docker 宣布把平台即服务的业务 dotCloud 出售给位于德国柏林的平台即服务提供商 cloudControl，自此 dotCloud 和 Docker 分道扬镳。 3.3 Docker 与 Kubernetes&emsp;&emsp;Google 多年来一直使用容器作为交付应用程序的一种重要方式，且运行有一款名为 Borg 的编排工具。Google、RedHat 等公司为了对抗以 Docker 公司为核心的容器商业生态，他们一起成立了 CNCF(Cloud Native Computing Foundation)。当谷歌于 2014 年 3 月开始开发 Kubernetes 时，很明智的选择当时最流行的容器，没错，就是 Docker。Kubernetes 对 Docker 容器运行时的支持，迎来了大量的使用用户。Kubernetes 于 2014 年 6 月 6 日首次发布。这便有了容器编排工具 Kubernetes 的诞生。另外，CNCF 的目的是以开源的 K8S 为基础，使得 K8S 能够在容器编排方面能够覆盖更多的场景，提供更强的能力。K8S 必须面临 Swarm 和 Mesos 的挑战。Swarm 的强项是和 Docker 生态的天然无缝集成，Mesos 的强项是大规模集群的管理和调度。K8S 是 Google 基于公司已经使用了十多年的 Borg 项目进行了沉淀和升华才提出的一套框架。它的优点就是有一套完整的全新的设计理念，同时有 Google 的背书，而且在设计上有很强的扩展性，所以，最终 K8S 赢得了胜利，成为了容器生态的行业标准。 3.4 K8s 容器化应用&emsp;&emsp;前面说了，K8s 是一种编排容器管理容器工具，那么如何通过 K8s 来将服务容器化呢？ &emsp;&emsp;首先，我们来看看 K8s 如何使用？第一条就是编写配置文件，因为配置文件可以是 YAML 或者 JSON 格式的。为方便阅读与理解，在后面的讲解中，我会统一使用 YAML 文件来指代它们。 Kubernetes 跟 Docker 等很多项目最大的不同，就在于它不推荐你使用命令行的方式直接运行容器（虽然 Kubernetes 项目也支持这种方式，比如：kubectl run），而是希望你用 YAML 文件的方式，即：把容器的定义、参数、配置，统统记录在一个 YAML 文件中，然后用这样一句指令把它运行起来： 1kubectl create -f xxx.yaml &emsp;&emsp;这样做最直接的一个好处是：你会有一个文件能记录下 K8s 到底 run 了哪些东西。比如下面这个例子： 12345678910111213141516171819apiVersion: apps&#x2F;v1kind: Deploymentmetadata: name: tomcat-deploymentspec: selector: matchLabels: app: tomcat replicas: 2 template: metadata: labels: app: tomcat spec: containers: - name: tomcat image: tomcat:10.0.5 ports: - containerPort: 80 &emsp;&emsp;像这样的一个 YAML 文件，对应到 kubernetes 中，就是一个 API Object（API 对象）。当你为这个对象的各个字段填好值并提交给 Kubernetes 之后，Kubernetes 就会负责创建出这些对象所定义的容器或者其他类型的 API 资源。可以看到，这个 YAML 文件中的 Kind 字段，指定了这个 API 对象的类型（Type），是一个 Deployment。Deployment 是一个定义多副本应用（即多个副本 Pod）的对象。此外，Deployment 还负责在 Pod 定义发生变化时，对每个副本进行滚动更新（Rolling+Update）。 &emsp;&emsp;在上面这个 Yaml 文件中，我给它定义的 Pod 副本个数 (spec.replicas)是：2。但，这些 Pod 副本长啥样子呢？为此，我们定义了一个 Pod 模版（spec.template），这个模版描述了我想要创建的 Pod 的细节。在上面的例子里，这个 Pod 里只有一个容器，这个容器的镜像（spec.containers.image）是 tomcat=10.0.5，这个容器监听端口（containerPort）是 80。 &emsp;&emsp;需要注意的是，像这种，使用一种 API 对象（Deployment）管理另一种 API 对象（Pod）的方法，在 Kubernetes 中，叫作“控制器”模式（controller pattern）。在我们的这个 demo 中，Deployment 扮演的正是 Pod 的控制器的角色。而 Pod 是 Kubernetes 世界里的应用；而一个应用，可以由多个容器（container）组成。为了让我们这个 tomcat 服务容器化运行起来，我们只需要执行： 12tom@PK001:~&#x2F;damon$ kubectl create -f tomcat-deployment.yamldeployment.apps&#x2F;tomcat-deployment created &emsp;&emsp;执行完上面的命令后，你就可以看容器运行情况，此时，只需要执行： 1234tom@PK001:~&#x2F;damon$ kubectl get pod -l app&#x3D;tomcatNAME READY STATUS RESTARTS AGEtomcat-deployment-799f46f546-7nxrj 1&#x2F;1 Running 0 77stomcat-deployment-799f46f546-hp874 0&#x2F;1 Running 0 77s &emsp;&emsp;’kubectl get’ 指令的作用，就是从 Kubernetes 里面获取（GET）指定的 API 对象。可以看到，在这里我还加上了一个 -l 参数，即获取所有匹配 app=nginx 标签的 Pod。需要注意的是，在命令行中，所有 key-value 格式的参数，都使用“=”而非“：”表示。 从这条指令返回的结果中，我们可以看到现在有两个 Pod 处于 Running 状态，也就意味着我们这个 Deployment 所管理的 Pod 都处于预期的状态。 &emsp;&emsp;此外， 你还可以使用 kubectl describe 命令，查看一个 API 对象的细节，比如： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849tom@PK001:~&#x2F;damon$ kubectl describe pod tomcat-deployment-799f46f546-7nxrjName: tomcat-deployment-799f46f546-7nxrjNamespace: defaultPriority: 0Node: ca005&#x2F;10.10.2.5Start Time: Thu, 08 Apr 2021 10:41:08 +0800Labels: app&#x3D;tomcat pod-template-hash&#x3D;799f46f546Annotations: cni.projectcalico.org&#x2F;podIP: 20.162.35.234&#x2F;32Status: RunningIP: 20.162.35.234Controlled By: ReplicaSet&#x2F;tomcat-deployment-799f46f546Containers: tomcat: Container ID: docker:&#x2F;&#x2F;5a734248525617e950b7ce03ad7a19acd4ffbd71c67aacd9e3ec829d051b46d3 Image: tomcat:10.0.5 Image ID: docker-pullable:&#x2F;&#x2F;tomcat@sha256:2637c2c75e488fb3480492ff9b3d1948415151ea9c503a496c243ceb1800cbe4 Port: 80&#x2F;TCP Host Port: 0&#x2F;TCP State: Running Started: Thu, 08 Apr 2021 10:41:58 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount from default-token-2ww52 (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled TrueVolumes: default-token-2ww52: Type: Secret (a volume populated by a Secret) SecretName: default-token-2ww52 Optional: falseQoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: node.kubernetes.io&#x2F;not-ready:NoExecute for 300s node.kubernetes.io&#x2F;unreachable:NoExecute for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 4m17s default-scheduler Successfully assigned default&#x2F;tomcat-deployment-799f46f546-7nxrj to ca005 Normal Pulling 4m16s kubelet, ca005 Pulling image &quot;tomcat:10.0.5&quot; Normal Pulled 3m27s kubelet, ca005 Successfully pulled image &quot;tomcat:10.0.5&quot; Normal Created 3m27s kubelet, ca005 Created container tomcat Normal Started 3m27s kubelet, ca005 Started container tomcat &emsp;&emsp;在 kubectl describe 命令返回的结果中，可以的清楚地看到这个 Pod 的详细信息，比如它的 IP 地址等等。其中，有一个部分值得你特别关注，它就是 Events（事件）。 &emsp;&emsp;在 Kubernetes 执行的过程中，对 API 对象的所有重要操作，都会被记录在这个对象的 Events 里，并且显示在 kubectl describe 指令返回的结果中。这些 Events 中的信息很重要，可以排查容器是否运行、正常运行的原因。 &emsp;&emsp;如果你希望升级 tomcat 的版本，那可以直接修改 Yaml 文件： 123456spec: containers: - name: tomcat image: tomcat:latest ports: - containerPort: 80 &emsp;&emsp;修改完 Yaml 文件后，执行： 1kubectl apply -f tomcat-deployment.yaml &emsp;&emsp;这样的操作方法，是 Kubernetes“声明式 API”所推荐的使用方法。也就是说，作为用户，你不必关心当前的操作是创建，还是更新，你执行的命令始终是 kubectl apply，而 Kubernetes 则会根据 YAML 文件的内容变化，自动进行具体的处理。 &emsp;&emsp;同时，可以查看容器内的服务的日志情况： 123456789101112131415161718192021222324252627282930313233343536tom@PK001:~&#x2F;damon$ kubectl logs -f tomcat-deployment-799f46f546-7nxrjNOTE: Picked up JDK_JAVA_OPTIONS: --add-opens&#x3D;java.base&#x2F;java.lang&#x3D;ALL-UNNAMED --add-opens&#x3D;java.base&#x2F;java.io&#x3D;ALL-UNNAMED --add-opens&#x3D;java.base&#x2F;java.util&#x3D;ALL-UNNAMED --add-opens&#x3D;java.base&#x2F;java.util.concurrent&#x3D;ALL-UNNAMED --add-opens&#x3D;java.rmi&#x2F;sun.rmi.transport&#x3D;ALL-UNNAMED08-Apr-2021 02:41:59.037 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version name: Apache Tomcat&#x2F;10.0.508-Apr-2021 02:41:59.040 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server built: Mar 30 2021 08:19:50 UTC08-Apr-2021 02:41:59.040 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version number: 10.0.5.008-Apr-2021 02:41:59.040 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log OS Name: Linux08-Apr-2021 02:41:59.040 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log OS Version: 4.4.0-116-generic08-Apr-2021 02:41:59.040 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Architecture: amd6408-Apr-2021 02:41:59.040 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Java Home: &#x2F;usr&#x2F;local&#x2F;openjdk-1108-Apr-2021 02:41:59.040 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log JVM Version: 11.0.10+908-Apr-2021 02:41:59.040 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log JVM Vendor: Oracle Corporation08-Apr-2021 02:41:59.040 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log CATALINA_BASE: &#x2F;usr&#x2F;local&#x2F;tomcat08-Apr-2021 02:41:59.041 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log CATALINA_HOME: &#x2F;usr&#x2F;local&#x2F;tomcat08-Apr-2021 02:41:59.051 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: --add-opens&#x3D;java.base&#x2F;java.lang&#x3D;ALL-UNNAMED08-Apr-2021 02:41:59.051 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: --add-opens&#x3D;java.base&#x2F;java.io&#x3D;ALL-UNNAMED08-Apr-2021 02:41:59.051 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: --add-opens&#x3D;java.base&#x2F;java.util&#x3D;ALL-UNNAMED08-Apr-2021 02:41:59.051 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: --add-opens&#x3D;java.base&#x2F;java.util.concurrent&#x3D;ALL-UNNAMED08-Apr-2021 02:41:59.052 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: --add-opens&#x3D;java.rmi&#x2F;sun.rmi.transport&#x3D;ALL-UNNAMED08-Apr-2021 02:41:59.052 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.util.logging.config.file&#x3D;&#x2F;usr&#x2F;local&#x2F;tomcat&#x2F;conf&#x2F;logging.properties08-Apr-2021 02:41:59.052 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.util.logging.manager&#x3D;org.apache.juli.ClassLoaderLogManager08-Apr-2021 02:41:59.052 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djdk.tls.ephemeralDHKeySize&#x3D;204808-Apr-2021 02:41:59.052 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.protocol.handler.pkgs&#x3D;org.apache.catalina.webresources08-Apr-2021 02:41:59.052 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Dorg.apache.catalina.security.SecurityListener.UMASK&#x3D;002708-Apr-2021 02:41:59.052 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Dignore.endorsed.dirs&#x3D;08-Apr-2021 02:41:59.052 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Dcatalina.base&#x3D;&#x2F;usr&#x2F;local&#x2F;tomcat08-Apr-2021 02:41:59.052 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Dcatalina.home&#x3D;&#x2F;usr&#x2F;local&#x2F;tomcat08-Apr-2021 02:41:59.052 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.io.tmpdir&#x3D;&#x2F;usr&#x2F;local&#x2F;tomcat&#x2F;temp08-Apr-2021 02:41:59.056 INFO [main] org.apache.catalina.core.AprLifecycleListener.lifecycleEvent Loaded Apache Tomcat Native library [1.2.27] using APR version [1.6.5].08-Apr-2021 02:41:59.056 INFO [main] org.apache.catalina.core.AprLifecycleListener.lifecycleEvent APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true], UDS [true].08-Apr-2021 02:41:59.059 INFO [main] org.apache.catalina.core.AprLifecycleListener.initializeSSL OpenSSL successfully initialized [OpenSSL 1.1.1d 10 Sep 2019]08-Apr-2021 02:41:59.312 INFO [main] org.apache.coyote.AbstractProtocol.init Initializing ProtocolHandler [&quot;http-nio-8080&quot;]08-Apr-2021 02:41:59.331 INFO [main] org.apache.catalina.startup.Catalina.load Server initialization in [441] milliseconds08-Apr-2021 02:41:59.369 INFO [main] org.apache.catalina.core.StandardService.startInternal Starting service [Catalina]08-Apr-2021 02:41:59.370 INFO [main] org.apache.catalina.core.StandardEngine.startInternal Starting Servlet engine: [Apache Tomcat&#x2F;10.0.5]08-Apr-2021 02:41:59.377 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler [&quot;http-nio-8080&quot;]08-Apr-2021 02:41:59.392 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in [61] milliseconds 第四章 为何借助容器助力微服务4.1 微服务的多语言性&emsp;&emsp;微服务可以说是越来越火了：那么对于语言场景，从 Java 中的 Spring MVC，到后面的 SOA、Dubbo、Spring Cloud、Spring cloud Alibaba 等。这是单纯 Java 语言的微服务的发展史，那么对于 Python、Go 呢？其实也是有其微服务设计理念的，比如 Golang 的 beego、gin 等。 &emsp;&emsp;这么多的微服务框架，都是独立、限制于自己的体系，撇开其它技术不说，Java 可以自成生态体系，Golang 亦是。其实微服务不应该受某种语言的限制，各种语言的服务之间应该可以互通：这在云原生时代设计中，它们都被称为 Service。后面会介绍对于不同语言的服务（Service）如何在云原生中互通。 4.2 微服务的高可用&emsp;&emsp;微服务出现后，同样面临着一个重要的话题：高可用。所谓高可用：英文缩写 HA(High Availability)，是指当某个服务或服务所在节点出现故障时，其对外的功能可以转移到该服务其他的副本或该服务在其他节点的副本，从而在减少停工时间的前提下，满足业务的持续性，这两个或多个服务构成了服务高可用。同时，这种高可用需要考虑到服务的性能压力，即服务的负载均衡。 &emsp;&emsp;我们知道对于服务的高可用，或者说服务的负载来说，有很多方式来解决这些问题。比如： 主从方式，其工作原理是：主机处于工作状态，备机处于监控、准备状态，当主机出现宕机的情况下，备机可以接管主机的一切工作，等到主机恢复正常后，将会手动或自动的方式将服务切换到主机上运行，数据的一致性通过共享存储来实现。 集群方式，其工作原理是：多台机器同时运行一个或几个服务，当其中的某个节点出现宕机时，这时该节点的服务将无法提供业务功能，可以选择根据一定的机制，将服务请求转移到该服务所在的其他节点上，这样可以让逻辑持续的执行下去，即消除软件单点故障。这其实就涉及到负载均衡策略。 &emsp;&emsp;对于微服务的高可用，涉及到的其中一个就是其服务的负载均衡。在微服务中，负载均衡的前提是，同一个服务需要被发现多个，或者说多个副本，这样才能实现负载均衡以及服务的高可用。那么，怎么让服务被发现呢？我们来看一张图： &emsp;&emsp;在上图中，我们可以看到： 1.各微服务先往注册中心 Eureka 注册服务。 2.各微服务保持与注册中心心跳。 3.注册中心发现各微服务。 4.注册中心根据配置规则定期获取心跳，超时即认为节点无效。 5.根据规则来定期清理无效节点。 &emsp;&emsp;这是基于注册中心 Eureka 的服务注册与发现，同样，基于其他的注册中心实现原理基本类似。如：Eureka、Zookeeper、Consul、etcd、nacos 等。其实，在云原生 K8s 中，也存在着服务的注册与发现，这在后面章节中会讲解。 &emsp;&emsp;服务发现后，其实面临的是一个主要的问题就是应该访问哪一个？因为发现了某个服务的多个实例，最终只会访问其中某一个，这就涉及到服务的负载均衡了。 &emsp;&emsp;负载均衡在微服务中是一个很常见的话题，实现负载均衡的插件也越来越多。netflix 开源的 Zuul、Gateway 等等，其实 K8s 中也存在着负载均衡器：kube-dns、kube-proxy。 &emsp;&emsp;在实现服务注册与发现、负载均衡后，其实高可用还涉及很多：高并发、缓存等。先讲讲高并发： 幂等性 接口代码的规范性 操作 DB 的性能 读写分离操作 服务的横向扩展 服务的健壮性（缓存、限流、熔灾） &emsp;&emsp;其中，幂等性：就是说一次和多次请求某一个资源时对于资源本身应该具有同样的结果（网络超时等问题除外）。也就是说，其任意次执行所产生的效果和返回的结果都是一样的。这种场景是一个很有效的实现高并发的情景，设想，用户充值某个会员，在并发情况下，用户由于误操作，或者由于网络、时间等问题导致重试机制的发生时，可能会触发触发多次交易的扣费，这样给用户一个很不好的体验。此时，就需要接口幂等性来解决这类问题。 &emsp;&emsp;幂等性解决方案有以下几种： token 机制 接口逻辑实现幂等性 数据库层处理实现幂等性 &emsp;&emsp;token 机制：数据提交时携带 token，token 放到 redis，token 有效时间，提交后台后校验 token，同时删除 token，生成新的 token 并返回。 &emsp;&emsp;接口的幂等性：常见的接口幂等性，是定义接口时，加上参数序列号、来源等，序列号与请求来源联合唯一索引，这样可以有效判断本次请求方与请求的序列号，防止重复的请求。 &emsp;&emsp;数据库处理：DB 层处理有多种方式：1. 悲观锁，2. 乐观锁，3. 唯一索引、组合唯一索引，4. 分布式锁。 &emsp;&emsp;悲观锁：所谓悲观锁，是指存在危机意识，事先(查询时)加锁处理，防止事情发生。如： 1select * from xxx where id&#x3D; 1 for update &emsp;&emsp;乐观锁：是指存在乐观心理，只在更新时加锁，乐观锁通常用 version 版本号来控制如： 1update xxx set name&#x3D;#name#,version&#x3D;version+1 where version&#x3D;#version# &emsp;&emsp;也可以通过条件限制，这里就使用了组合唯一索引来处理，如： 1update xxx set name&#x3D;#name#,version&#x3D;version+1 where id&#x3D;#id# and version&#x3D;#version# &emsp;&emsp;分布式锁：通过 redis、zookeeper 来设置分布式锁，当插入或更新数据时，获取分布式锁，然后做操作，之后释放锁。 &emsp;&emsp;接口的规范性：接口的性能如何，最终还是跟接口的实现逻辑有关，比如代码规范，逻辑实现等，尤其是业务逻辑复杂的情况下，这点需要注意的。 &emsp;&emsp;操作 DB：对于业务的持久层，用的比较多的就是 mybatis、hibernate，还有可能是 JPA，无论是哪个，最终都是通过工厂类注入 bean，最后执行 SQL 来操作 DB。所以这里尤为重要的是 SQL 的写法，SQL 的优化决定着操作 DB 的时间以及效果，如果写得不好的话，则会导致死循环，或死锁，或内存溢出。另外测试时，使用真实、规范的数据进行测试，并在测试时不要局限于相同的数据，最后就是并发压测了。 &emsp;&emsp;读写分离：当服务足够多，数据足够多时，有可能读与写的占比为：10:1，此时读写应该分离，这样可以有效减少因为读的频繁操作导致的写的性能下降。常见的读写分离的方法有：采用 mycat 中间件方式、amoeba 直接实现读写分离、手动修改 mysql 操作类直接实现读写分离和随机实现的负载均衡，权限独立分配、mysql-proxy（还是测试版本，时间消耗有点高）。 &emsp;&emsp;服务的横向扩展：对于服务的请求越来越多时，此时需要对服务进行多节点部署，这样减少单机带来的服务负载压力。 &emsp;&emsp;服务的健壮性：服务的健壮性包括缓存、限流、熔灾。 &emsp;&emsp;对于缓存，大家都知道，有常见的许多中间件如：redis、kafka、RabbitMq、zookeeper。对于一些 session 等常用 redis 来缓存、共享。对于一些大一点的数据如果嫌弃加载慢，也可以采用缓存机制来解决。 &emsp;&emsp;什么叫限流呢？很好理解，就是限制节点的流量，限制服务的请求数。那么如何做到限流呢？常用的限流算法比如有计数器算法、令牌桶算法、漏桶算法。有几种方式：利用 springcloud 组件 zuul 来对请求进行限流，主要是通过谷歌提供的 RateLimiter 结合一些限流算法来限流比较常用。利用 redis 同样可以做限流算法的，甚至可以利用 nginx 直接作计数限流，可以对请求速率进行限制、对每个 ip 连接数量进行限制、对每个服务的连接数量进行限制。如： 1234567891011121314#对请求速率进行限制limit_req_zone $binary_remote_addr zone&#x3D;req_one:20m rate&#x3D;12r&#x2F;s;limit_conn_zone $binary_remote_addr zone&#x3D;addr:10m;#对每个ip连接数量进行限制limit_conn_zone $server_name zone&#x3D;perserver:20m;#对每个服务的连接数量进行限制server&#123; listen 80; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;ip:port; limit_req zone&#x3D;req_one burst&#x3D; 80 nodelay; limit_conn addr 20; &#125;&#125; &emsp;&emsp;其实在 Springboot2.x 中，推出自己的 Spring-Cloud-Gateway 来作网关，同时 Spring-Cloud-Gateway 中提供了基于 Redis 的实现来达到限流的目的。对于其它框架，都有加入限流的插件功能。 &emsp;&emsp;对于熔灾，或者说熔断，这个在实际的业务当中是很有必要的。比如：用户在某一商城秒杀某一件物品，或在某米商城上抢购某一部手机，在准点抢购时，发现人很多，请求很多，这时，主要是需要有限流机制，同时也需要有熔灾（熔断），给用户留下一个很好的体验的感觉。当用户在点击抢购按钮后，如果当前的请求数很多，需要用户等待，这是需要给一个友好的界面让用户去等待，而不是直接给用户提示请求失败，或者报异常，这样的红色抛出是一个非常不好的事情，用户可能会骂街的，下次也不会逛了。 &emsp;&emsp;Spring-Cloud-Gateway 作网关时，过滤器时使用 HystrixGatewayFilterFactory 来创建一个 Filter 实现基于 Route 级别的熔断功能。 &emsp;&emsp;对于缓存问题，随着系统用户的越来愈多，所有的服务压力也会指数型递增，这时候缓存是一个很好的减轻服务压力的方式。这样可以有效缓冲请求对服务的负载压力。常见的缓存可能是 Redis、MQ(RabbitMQ、RocketMQ)、Kafka、ZooKeeper 等。 &emsp;&emsp;Redis 一般主要做 session 或用户信息的缓存，实现多机中 session 的共享。也会用来作分布式锁，在分布式高并发下实现锁的功能，例如实现秒杀、抢单等功能。还会被用作一些订单信息的缓存，防止大量的订单信息被积压而导致服务器的负载很高。总之，Redis 常被用来作为一种缓冲剂使用。 &emsp;&emsp;MQ 常见的有 RabbitMQ、RocketMQ、ActiveMQ、Kafka 等，以下是各种之间的对比： 特性ActiveMQRabbitMQRocketMQKafka单机吞吐量万级，吞吐量比RocketMQ和Kafka要低了一个数量级万级，吞吐量比RocketMQ和Kafka要低了一个数量级10万级，RocketMQ也是可以支撑高吞吐的一种MQ10万级别，这是kafka最大的优点，就是吞吐量高。一般配合大数据类的系统来进行实时数据计算、日志采集等场景时效性ms级微秒级ms级延迟在ms级以内可用性高，基于主从架构实现高可用性高，基于主从架构实现高可用性非常高，分布式架构非常高，kafka是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用消息可靠性有较低的概率丢失数据经过参数优化配置，可以做到0丢失经过参数优化配置，消息可以做到0丢失功能支持MQ领域的功能极其完备基于erlang开发，所以并发能力很强，性能极其好，延时很低MQ功能较为完善，还是分布式的，扩展性好功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用，是事实上的标准 &emsp;&emsp;ZooKeeper 也是经常会存储海量数据，例如 Hadoop 中，在使用 YARN 作资源调度时，采用 ZooKeeper 来存储海量的状态机状态以及任务的信息（包括历史信息）。 4.3 微服务的复杂性&emsp;&emsp;说起微服务，使用 DDD 划分微服务的好处的时候，经常会说 DDD 能够让相关的业务逻辑更加内聚，并且降低服务之间的耦合性，从而最终实现达到降解系统的复杂性。但是在这里，不论是高内聚，低耦合，甚至我们经常说的系统复杂性，我们有没有一个客观的可以量化的指标来衡量这些概念。由于无法量化，所以就没法度量，这样当团队在讨论一个系统是否复杂，有多复杂这些问题的时候，就很容易陷入各种主观直觉的争论中。 &emsp;&emsp;互联网时代，这些巨多的系统在细节上不一样，但是如果从抽象层面来看有很多共性，一个微服务系统由很多个微服务组成，这些微服务的行为产生出复杂的行为模式。整个微服务系统会通过 API 利用内部或外部的信号，同时在系统内部也是通过服务之间的接口进行信息传递。一个微服务系统并不是静态的，而是会不断适应业务变化，改变系统的 API 或者系统内部的组织方式来增加生存的机会。 &emsp;&emsp;微服务的复杂，不仅仅在于其系统本身，还需要考虑的是：高可用、服务自治、服务并发、服务限流、熔灾等。 &emsp;&emsp;服务的高可用在上一节中已经说明了，服务自治，目的其实是在对其修改时对其他部分造成尽可能小的影响；自治服务运营时也不会对其他服务的功能造成影响。服务几乎总是要依赖其他服务提供的数据。例如，网上商城都有一个购物车微服务，一些其他服务必须能向购物车添加商品，还必须能访问购物车内的商品并下单和配送。现在问题是，如何在保持服务尽可能自治的前提下实现对接。那需要遵循一定的模式：交互、信息传递。 &emsp;&emsp;交互模式：Request-Reply 还是 Publish-Subscribe Request-Reply（请求-应答）意味着一个服务处理信息的特定请求或者执行一些动作并返回一个应答。发起调用的服务需要知道去哪儿请求以及请求些什么？这种模式仍然可以被实现为异步执行，并且你还可以做一些抽象使服务调用方不需要知道被调用服务的物理地址，不能逃避的一点是服务必须明确的要求一个特定的信息和功能（或者执行动作）并等待应答。 Publish-Subscribe（发布-订阅） 这种模式下的服务将自己注册为对特定的信息感兴趣，或者能够处理特定的请求，相关的信息和请求将被交付给它，并且它可以决定怎么处理这些信息和请求。本文假定有一些中间件能够处理交付或者发布消息给订阅这些消息服务。 &emsp;&emsp;信息传递：Events 还是 Queries/Commands Events（事件）是没有争议的事实，比如订单号 123 的订单已经创建，事件只陈述发生了什么事，不描述这样一个事件会导致什么事情发生。 Queries/Commands（查询/命令）两者都传达了什么事情会发生，查询是对信息的特定请求，命令是要求一个服务执行一些动作的特定请求。 &emsp;&emsp;上面的四种即可作为微服务间对接的四种方式：REQUEST-REPLY WITH EVENTS、REQUEST-REPLY WITH COMMANDS/QUERIES、PUBLISH-SUBSCRIBE WITH EVENTS、PUBLISH-SUBSCRIBE WITH COMMANDS/QUERIES。 REQUEST-REPLY WITH EVENTS，在这种模式下，一个服务请求另一个导致事件发生的特定服务，这意味着这两种服务之间有很强的依赖。配送服务必须知道要连接那个服务来获得订单相关的事件，这也导致了运行时依赖，因为配送服务只有在订单服务可用的时候才能配送新订单。 既然配送服务只接收事件，它基于事件里的信息自己决定何时一个订单可以被配送，订单服务不需要知道配送服务的任何信息，它只是简单的提供事件表明当其他服务请求时订单进行怎样的处理，把响应事件的职责完全交给请求事件的服务。 REQUEST-REPLY WITH COMMANDS/QUERIES，如：订单服务将请求配送服务来配送一个订单，这意味着强烈的依赖，因为订单服务明确的请求一个特定的服务来处理配送，现在订单服务必须决定何时一个订单准备好配送，它意识到配送服务的存在，甚至知道怎样与配送服务交互，在订单配送前需要考虑是否有其他因素关联到订单（比如客户信用卡状态），订单服务在请求配送服务来配送订单前也需要考虑这一点。现在业务处理被混到了架构里，因此架构不能被简单的修改。这也是运行时依赖，因为订单服务必须确保配送请求成功交付给了配送服务。 PUBLISH-SUBSCRIBE WITH EVENTS，配送服务注册自己对订单相关的事件感兴趣，注册后，配送服务会收到订单的所有事件而不需要关心订单事件的来源，这是对订单事件来源的松散耦合，配送服务需要保留接收到事件的副本，这样就可以决定何时订单准备好配送。订单服务需要对配送无关，如果多个服务提供包含配送服务需要的相关数据的订单相关事件，配送服务应该不可识别，如果一个提供订单事件的服务宕机，配送服务也应该不知道，只是收到的事件变少了，配送服务不会因此阻塞。 PUBLISH-SUBSCRIBE WITH COMMANDS/QUERIES，配送服务自己注册为能够配送货物的服务，接受所有想要配送货物的命令，配送服务不需要意识到配送命令的来源，同样订单服务业不知道那些服务将处理配送，在这个意义上说，他们是松散耦合的，不过，订单服务知道既然发送了配送命令，订单必须被配送的事实，这确实让耦合更强了。 &emsp;&emsp;两种 Request-Reply 模式都意味着两个服务的运行时耦合和强耦合，两种 Command/Queries 模式意味着一个服务知道另一个服务应该做的事，这也意味着强耦合，但是这一次在功能级别。留下了一个选项：PUBLISH-SUBSCRIBE WITH EVENTS，这种情况下，两种服务从运行时和功能的角度都没有意识到彼此的存在。 &emsp;&emsp;但是，我们需要考虑更多的因素，一直使用这种方式交互是有代价的，例如，数据被复制、事件丢失、事件驱动的架构增加更多基础设施的需求、额外的延迟。 第二部分 原理与应用第五章 Kubernetes 介绍5.1 Kubernetes 的基本概念与特性&emsp;&emsp;在前面的章节中介绍了 Kubernetes 的由来，那么 Kubernetes 到底是干嘛的呢？这就涉及到 Kubernetes 的定义以及其特性，本节来描述下 Kubernetes 的特性以及应用场景。 &emsp;&emsp;Kubernetes，简称 K8s，是把 8 代替了 8 个字符“ubernete”而成的缩写。K8s 是一个一个开源的，用于管理云平台中多个主机上的容器化的应用编排工具。它的目标是让部署容器化的应用简单且高效，Kubernetes 提供了应用部署，规划，更新，维护的一种机制。 &emsp;&emsp;Kubernetes 是 Google 开源的一个容器编排引擎，支持自动化部署、大规模可伸缩、应用容器化管理。在生产环境中部署一个应用程序时，通常要部署该应用的多个实例以便对应用请求进行负载均衡。在 Kubernetes 中，我们可以创建多个容器，每个容器里面运行一个应用实例，然后通过内置的负载均衡策略，实现对这一组应用实例的管理、发现、访问，而这些细节都不需要运维人员去进行复杂的手工配置和处理。 &emsp;&emsp;特性： 可移植: 支持公有云，私有云，混合云 可扩展: 模块化，插件化，可挂载，可组合 自动化: 自动部署，自动重启，自动复制，自动弹性伸缩 &emsp;&emsp;可移植，意味着可以穿梭任何系统，不受系统的限制，也不受任何语言的限制，支持任何的其他的服务形式。 &emsp;&emsp;可扩展，是指 K8s 的各个模块之间是解耦合的，可以增加插件来丰富其功能，也可以替换其组件来达到想要的效果。 &emsp;&emsp;自动部署和回滚，K8s 采用滚动更新策略更新应用，一次更新一个 Pod，而不是同时删除所有 Pod，如果更新过程中出现问题，将回滚更改，确保升级不受影响业务。 &emsp;&emsp;弹性伸缩，使用命令、UI 或者基于 CPU 使用情况自动快速扩容和缩容应用程序实例，保证应用业务高峰并发时的高可用性；业务低峰时回收资源，以最小成本运行服务。 &emsp;&emsp;自动重启，是说在集群节点宕机、机器重启后，其 K8s 集群具有自动重启的功能，同时，会拉起集群中所有的应用服务，这就是其编排能力的一个体现。 &emsp;&emsp;自动复制，是指所有相关的数据，可以被备份到 etcd 或其它插件中，以便 K8s 可以通过 controllers、scheduler 来很好的编排。 &emsp;&emsp;可挂载，是指存储编排，挂载外部存储系统，无论是来自本地存储，公有云（如 AWS），还是网络存储（如 NFS、GlusterFS、Ceph）都作为集群资源的一部分使用，极大提高存储使用灵活性。 &emsp;&emsp;自我修复，在节点故障时重新启动失败的容器，替换和重新部署，保证预期的副本数量；杀死健康检查失败的容器，并且在未准备好之前不会处理客户端请求，确保线上服务不中断。 5.2 部署 Kubernetes 集群&emsp;&emsp;如果想要了解 K8s 的一些特性，并且将其应运的很好，那就需要动手部署一个 K8s 集群。下面讲解下 K8s 集群部署流程。 单机版 K8s&emsp;&emsp;环境： Ubuntu 16.04 GPU 驱动 418.56 Docker 18.06 K8s 1.13.5 以上的环境，针对的是高版本的 K8s，而且 Docker 的版本必须要注意。另外 GPU 驱动的话，如果大家是非 GPU 机器的话，可以考虑不用。如果含有 GPU 机器的话，需要安装驱动，并且驱动版本不能过低喔。 设置环境在配置环境前，首先备份一下源配置： 1cp &#x2F;etc&#x2F;apt&#x2F;sources.list &#x2F;etc&#x2F;apt&#x2F;sources.list.cp 然后我们重新写一份配置，编辑内容，加上阿里源： 12345678910111213141516171819vim &#x2F;etc&#x2F;apt&#x2F;sources.listdeb-src http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial main restricteddeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial main restricteddeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial main restricted multiverse universedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-updates main restricteddeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-updates main restricted multiverse universedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial universedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-updates universedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial multiversedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-updates multiversedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-backports main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F;xenial-backports main restricted universe multiversedeb http:&#x2F;&#x2F;archive.canonical.com&#x2F;ubuntu xenial partnerdeb-src http:&#x2F;&#x2F;archive.canonical.com&#x2F;ubuntu xenial partnerdeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-security main restricteddeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-security main restricted multiverse universedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-security universedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-security multiverse 添加好后，可以执行如下命令，更新一下源： 1apt-get update 如果出现问题，可以执行如下命令来自动修复安装出现 broken 的 package： 1apt --fix-broken install 执行升级命令时，注意：对于 GPU 机器可不执行，否则可能升级 GPU 驱动导致问题。 1apt-get upgrade 由于 K8s 安装要求，需要关闭防火墙： 1ufw disable 安装 SELinux： 1apt install selinux-utils SELinux 防火墙配置： 12345setenforce 0vim&#x2F;etc&#x2F;selinux&#x2F;conifgSELINUX&#x3D;disabled 设置网络，将桥接的 IPV4 流量传递到 iptables 的链： 12345tee &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf &lt;&lt;-&#39;EOF&#39;net.bridge.bridge-nf-call-ip6tables &#x3D; 1net.bridge.bridge-nf-call-iptables &#x3D; 1net.ipv4.ip_forward &#x3D; 1EOF 为了防止下面执行的会报错，可以先执行一下： 1modprobe br_netfilter 最后，查看 IPV4 与 v6 配置是否生效： 1sysctl --system 配置 iptables： 1234iptables -P FORWARD ACCEPTvim &#x2F;etc&#x2F;rc.local&#x2F;usr&#x2F;sbin&#x2F;iptables -P FORWARD ACCEPT 需要永久关闭 swap 分区： 1sed -i &#39;s&#x2F;.*swap.*&#x2F;#&amp;&#x2F;&#39; &#x2F;etc&#x2F;fstab 以上为 K8s 系统的环境配置，这个条件是硬性的，必须要处理。接下来就是安装需要的配套工具了。 安装 Docker设置环境，在 Docker 安装前设置： 12345apt-get install apt-transport-https ca-certificates curl software-properties-commoncurl -fsSL https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;ubuntu&#x2F;gpg | apt-key add -add-apt-repository &quot;deb [arch&#x3D;amd64] https:&#x2F;&#x2F;download.docker.com&#x2F;linux&#x2F;ubuntu $(lsb_release -cs) stable&quot; apt-get update 删除已经存在的低版本 Docker： 123apt-get purge docker-ce docker docker-engine docker.io &amp;&amp; rm -rf &#x2F;var&#x2F;lib&#x2F;dockerapt-get autoremove docker-ce docker docker-engine docker.io 安装指定版本的 Docker： 1apt-get install -y docker-ce&#x3D;18.06.3~ce~3-0~ubuntu 启动 Docker 并设置开机自重启： 1systemctl enable docker &amp;&amp; systemctl start docker 安装好 Docker 后，需要配置一下 Docker 以让其生效： 1234567891011121314151617vim &#x2F;etc&#x2F;docker&#x2F;daemon.json&#123; &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: &#123; &quot;max-size&quot;: &quot;100m&quot;, &quot;max-file&quot;: &quot;10&quot; &#125;, &quot;insecure-registries&quot;: [&quot;http:&#x2F;&#x2F;k8s.gcr.io&quot;], &quot;data-root&quot;: &quot;&quot;, &quot;default-runtime&quot;: &quot;nvidia&quot;, &quot;runtimes&quot;: &#123; &quot;nvidia&quot;: &#123; &quot;path&quot;: &quot;&#x2F;usr&#x2F;bin&#x2F;nvidia-container-runtime&quot;, &quot;runtimeArgs&quot;: [] &#125; &#125;&#125; 上面是含 GPU 的配置，如果你的机器不含 GPU，可按照下面来配置： 123456789101112131415&#123;&quot;registry-mirrors&quot;:[&quot;https:&#x2F;&#x2F;registry.docker-cn.com&quot;],&quot;storage-driver&quot;:&quot;overlay2&quot;,&quot;log-driver&quot;:&quot;json-file&quot;,&quot;log-opts&quot;:&#123;&quot;max-size&quot;:&quot;100m&quot;&#125;,&quot;exec-opts&quot;:[&quot;native.cgroupdriver&#x3D;systemd&quot;],&quot;insecure-registries&quot;:[&quot;http:&#x2F;&#x2F;k8s.gcr.io&quot;],&quot;live-restore&quot;:true&#125; 最后，我们重启 Docker 服务并设置开机自动重启，重启后可以看到 Docker 的相关信息： 1systemctl daemon-reload &amp;&amp; systemctl restart docker &amp;&amp; docker info 安装 K8s在安装 K8s 之前，我们需要设置一下环境，以便很快的拉取相关的镜像，这里选择了阿里源： 1234567apt-get update &amp;&amp; apt-get install -y apt-transport-https curlcurl -s https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;apt&#x2F;doc&#x2F;apt-key.gpg | apt-key add -tee &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;kubernetes.list &lt;&lt;-&#39;EOF&#39;deb https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;apt kubernetes-xenial mainEOF 配置完阿里源后，我们更新一下资源： 1apt-get update 更新完后，先摒弃不可用或损坏的 K8s 组件 kubectl、kubeadm、kubelet 的残留： 12apt-get purge kubelet&#x3D;1.13.5-00 kubeadm&#x3D;1.13.5-00 kubectl&#x3D;1.13.5-00apt-get autoremove kubelet&#x3D;1.13.5-00 kubeadm&#x3D;1.13.5-00 kubectl&#x3D;1.13.5-00 重新更新、安装一份指定版本的 K8s 组件： 12apt-get install -y kubelet&#x3D;1.13.5-00 kubeadm&#x3D;1.13.5-00 kubectl&#x3D;1.13.5-00apt-mark hold kubelet&#x3D;1.13.5-00 kubeadm&#x3D;1.13.5-00 kubectl&#x3D;1.13.5-00 这里的三大组件 kubectl、kubeadm、kubelet，都是比较重要的，下面简单介绍下： kubectl 是 K8s 集群的命令行工具，通过 kubectl 能够对集群本身进行管理，并能够在集群上进行容器化应用的安装部署。 kubeadm 是部署、安装 K8s 的一种命令工具。它提供了 kubeadm init 以及 kubeadm join 这两个命令作为快速创建 Kubernetes 集群的最佳实践。 关于 kubelet，在 K8s 集群中，在每个 Node 上都会启动一个 kubelet 服务的进程。该进程用于处理 Master 下发到本节点的任务，管理 Pod 及 Pod 中的容器。每个 kubelet 进程都会在 API Server 上注册节点自身的信息，定期向 Master 汇报节点资源的使用情况，并通过 cAdvisor 监控容器和节点资源。 接下来启动服务并设置开机自动重启: 1systemctl enable kubelet &amp;&amp; sudo systemctl start kubelet 组件安装好了，接下来安装 K8s 相关镜像，由于 gcr.io 网络访问不了，从 registry.cn-hangzhou.aliyuncs.com 镜像地址下载： 12345678910111213docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;gg-gcr-io&#x2F;kube-apiserver:v1.13.5docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;gg-gcr-io&#x2F;kube-controller-manager:v1.13.5docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;gg-gcr-io&#x2F;kube-scheduler:v1.13.5docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;gg-gcr-io&#x2F;kube-proxy:v1.13.5docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;kuberimages&#x2F;pause:3.1docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;kuberimages&#x2F;etcd:3.2.24docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;kuberimages&#x2F;coredns:1.2.6 注意：每个镜像的版本要注意，而且对应的 CoreDNS、Etcd 版本也要对应，每个版本的 K8s 对应的不一样的。否则，可能会出问题的。 拉取镜像后，我们需要打标签，因为 kubeadm init 的时候，标签是固定的，具体如下： 12345678910111213docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;gg-gcr-io&#x2F;kube-apiserver:v1.13.5 k8s.gcr.io&#x2F;kube-apiserver:v1.13.5docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;gg-gcr-io&#x2F;kube-controller-manager:v1.13.5 k8s.gcr.io&#x2F;kube-controller-manager:v1.13.5docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;gg-gcr-io&#x2F;kube-scheduler:v1.13.5 k8s.gcr.io&#x2F;kube-scheduler:v1.13.5docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;gg-gcr-io&#x2F;kube-proxy:v1.13.5 k8s.gcr.io&#x2F;kube-proxy:v1.13.5docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;kuberimages&#x2F;pause:3.1 k8s.gcr.io&#x2F;pause:3.1docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;kuberimages&#x2F;etcd:3.2.24 k8s.gcr.io&#x2F;etcd:3.2.24docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;kuberimages&#x2F;coredns:1.2.6 k8s.gcr.io&#x2F;coredns:1.2.6 kubeadm 初始化上面的操作步骤走完后，接下来就是利用 kubeadm 初始化 K8s，其中主机 IP 根据自己的实际情况输入，通过 kubeadm init [flags] 形式可以启动一个 master 节点： 1kubeadm init --kubernetes-version&#x3D;v1.13.5 --pod-network-cidr&#x3D;10.244.0.0&#x2F;16 --service-cidr&#x3D;10.16.0.0&#x2F;16 --apiserver-advertise-address&#x3D;$&#123;masterIp&#125; | tee kubeadm-init.log 此时，如果未知主机 IP，也可利用 yaml 文件动态初始化，我们通过 hosts 来进行动态加载： 12345678910111213141516vi &#x2F;etc&#x2F;hosts10.10.5.100 k8s.api.servervi kube-init.yamlapiVersion: kubeadm.k8s.io&#x2F;v1beta1kind: ClusterConfigurationkubernetesVersion: v1.13.5imageRepository: registry.aliyuncs.com&#x2F;google_containersapiServer: certSANs: - &quot;k8s.api.server&quot;controlPlaneEndpoint: &quot;k8s.api.server:6443&quot;networking: serviceSubnet: &quot;10.1.0.0&#x2F;16&quot; podSubnet: &quot;10.244.0.0&#x2F;16&quot; 设置 Etcd HA 版本： 1234567891011121314151617apiVersion: kubeadm.k8s.io&#x2F;v1beta1kind: ClusterConfigurationkubernetesVersion: v1.13.5imageRepository: registry.aliyuncs.com&#x2F;google_containersapiServer: certSANs: - &quot;api.k8s.com&quot;controlPlaneEndpoint: &quot;api.k8s.com:6443&quot;etcd: external: endpoints: - https:&#x2F;&#x2F;ETCD_0_IP:2379 - https:&#x2F;&#x2F;ETCD_1_IP:2379 - https:&#x2F;&#x2F;ETCD_2_IP:2379networking: serviceSubnet: 10.1.0.0&#x2F;16 podSubnet: 10.244.0.0&#x2F;16 注意：apiVersion 中用 kubeadm，因为需要用 kubeadm 来初始化，最后执行下面来初始化。 1kubeadm init --config&#x3D;kube-init.yaml 请耐心等几分钟直到结束。 出现问题，解决后，执行： 1kubeadm reset 如果需要更多，可执行下面来查看： 1kubeadm --help 部署如果没问题，查看当前的版本： 1kubelet --version 部署出现问题先删除 node 节点（集群版）： 123kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsetskubectl delete node &lt;node name&gt; 清空 init 配置在需要删除的节点上执行（注意，当执行 init 或者 join 后出现任何错误，都可以使用此命令返回）: 1kubeadm reset 查问题初始化后出现问题，可以通过以下命令先查看其容器状态以及网络情况： 12345678910111213141516171819sudo docker ps -a | grep kube | grep -v pausesudo docker logs CONTAINERIDsudo docker images &amp;&amp; systemctl status -l kubeletnetstat -nlptkubectl describe ep kuberneteskubectl describe svc kuberneteskubectl get svc kuberneteskubectl get epnetstat -nlpt | grep apiservi &#x2F;var&#x2F;log&#x2F;syslog 给当前用户配置 K8s apiserver 访问公钥12345sudo mkdir -p $HOME&#x2F;.kubesudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;configsudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config 网络插件在上面的步骤后，如果查看节点情况： 1kubectl get nodes 查看 nodes 状态信息，看到 node 节点的状态为 NotReady，这是因为缺少容器网络的配置。 接下来需要部署网络插件： 123456789101112kubectl apply -f https:&#x2F;&#x2F;docs.projectcalico.org&#x2F;v3.3&#x2F;getting-started&#x2F;kubernetes&#x2F;installation&#x2F;hosted&#x2F;rbac-kdd.yamlwget https:&#x2F;&#x2F;docs.projectcalico.org&#x2F;v3.3&#x2F;getting-started&#x2F;kubernetes&#x2F;installation&#x2F;hosted&#x2F;kubernetes-datastore&#x2F;calico-networking&#x2F;1.7&#x2F;calico.yamlvi calico.yaml- name: CALICO_IPV4POOL_IPIP value:&quot;off&quot;- name: CALICO_IPV4POOL_CIDR value: &quot;10.244.0.0&#x2F;16kubectl apply -f calico.yaml 单机下允许 master 节点部署 pod 命令如下： 1kubectl taint nodes --all node-role.kubernetes.io&#x2F;master- 禁止 master 部署 pod： 1kubectl taint nodes k8s node-role.kubernetes.io&#x2F;master&#x3D;true:NoSchedule 以上单机版部署结束，如果你的项目中，交付的是软硬件结合的一体机，那么到此就结束了。记得单机下要允许 master 节点部署哟！ K8s 集群版实战以上面部署的机器为例，作为 master 节点，我们备份一些配置到节点机器，执行： 123456789scp &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $nodeUser@$nodeIp:&#x2F;home&#x2F;$nodeUserscp &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;* $nodeUser@$nodeIp:&#x2F;home&#x2F;$nodeUser&#x2F;etcdkubeadm token generatekubeadm token create $token_name --print-join-command --ttl&#x3D;0kubeadm join $masterIP:6443 --token $token_name --discovery-token-ca-cert-hash $hash 注意，这个 token 24 小时后会失效，如果后面有其他节点要加入的话，处理方法： 12345kubeadm token generatekubeadm token listopenssl x509 -pubkey -in &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt | openssl rsa -pubin -outform der 2&gt;&#x2F;dev&#x2F;null | openssl dgst -sha256 -hex | sed &#39;s&#x2F;^.* &#x2F;&#x2F;&#39; 然后拿到 token 和一个 sha256 密钥后执行下面即可加入集群： 1kubeadm join $masterIP:6443 --token $token_name --discovery-token-ca-cert-hash $hash Node 机器执行时，如果需要 Cuda，可以参考以下资料： https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#ubuntu-installation https://blog.csdn.net/u012235003/article/details/54575758 https://blog.csdn.net/qq_39670011/article/details/90404111 正式执行： 12345vim &#x2F;etc&#x2F;modprobe.d&#x2F;blacklist-nouveau.confblacklist nouveauoptions nouveau modeset&#x3D;0update-initramfs -u 重启 Ubuntu 查看是否禁用成功： 1234567lsmod | grep nouveauapt-get remove --purge nvidia*https:&#x2F;&#x2F;developer.nvidia.com&#x2F;cuda-downloadssudo apt-get install freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev 安装 Cuda： 123456789101112131415acceptselect &quot;Install&quot; &#x2F; Enterselect &quot;Yes&quot;sh cuda_10.1.168_418.67_linux.runecho &#39;export PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;bin:$PATH&#39; &gt;&gt; ~&#x2F;.bashrcecho &#39;export PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;NsightCompute-2019.3:$PATH&#39; &gt;&gt; ~&#x2F;.bashrcecho &#39;export LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;lib64:$LD_LIBRARY_PATH&#39; &gt;&gt; ~&#x2F;.bashrcsource ~&#x2F;.bashrc 重启机器，检查 Cuda 是否安装成功。 查看是否有 nvidia* 的设备： 1cd &#x2F;dev &amp;&amp; ls -al 如果没有，创建一个 nv.sh： 123456789101112131415161718192021222324252627282930313233343536vi nv.sh#!&#x2F;bin&#x2F;bash &#x2F;sbin&#x2F;modprobe nvidiaif [ &quot;$?&quot; -eq 0 ];thenNVDEVS&#x3D;&#96;lspci | grep -i NVIDIA&#96;N3D&#x3D;&#96;echo&quot;$NVDEVS&quot;| grep &quot;3D controller&quot; | wc -l&#96;NVGA&#x3D;&#96;echo&quot;$NVDEVS&quot;| grep &quot;VGA compatible controller&quot; | wc -l&#96;N&#x3D;&#96;expr $N3D + $NVGA -1&#96;for i in &#96;seq0 $N&#96;; do mknod -m 666 &#x2F;dev&#x2F;nvidia$i c 195 $idone mknod -m 666 &#x2F;dev&#x2F;nvidiactl c 195 255else exit 1fichmod +x nv.sh &amp;&amp; bash nv.sh 再次重启机器查看 Cuda 版本： 1nvcc -V 编译： 123cd &#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;samples &amp;&amp; makecd &#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;samples&#x2F;bin&#x2F;x86_64&#x2F;linux&#x2F;release .&#x2F;deviceQuery 以上如果输出“Result = PASS”，代表 Cuda 安装成功。 安装 nvdocker： 12345678910111213141516171819vim &#x2F;etc&#x2F;docker&#x2F;daemon.json&#123;&quot;runtimes&quot;:&#123; &quot;nvidia&quot;:&#123; &quot;path&quot;:&quot;nvidia-container-runtime&quot;, &quot;runtimeArgs&quot;:[] &#125;&#125;,&quot;registry-mirrors&quot;:[&quot;https:&#x2F;&#x2F;registry.docker-cn.com&quot;],&quot;storage-driver&quot;:&quot;overlay2&quot;,&quot;default-runtime&quot;:&quot;nvidia&quot;,&quot;log-driver&quot;:&quot;json-file&quot;,&quot;log-opts&quot;:&#123; &quot;max-size&quot;:&quot;100m&quot;&#125;,&quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;],&quot;insecure-registries&quot;: [$harborRgistry],&quot;live-restore&quot;: true&#125; 重启 Docker： 1sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker &amp;&amp; docker info 检查 nvidia-docker 安装是否成功： 1docker run --runtime&#x3D;nvidia --rm nvidia&#x2F;cuda:9.0-base nvidia-smi 在节点机器进入 su 模式： 1su $nodeUser 给当前节点用户配置 K8s apiserver 访问公钥： 123456789101112131415mkdir -p $HOME&#x2F;.kubecp -i admin.conf $HOME&#x2F;.kube&#x2F;configchown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;configmkdir -p $HOME&#x2F;etcdsudo rm -rf &#x2F;etc&#x2F;kubernetessudo mkdir -p &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcdsudo cp &#x2F;home&#x2F;$nodeUser&#x2F;etcd&#x2F;* &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcdsudo kubeadm join $masterIP:6443 --token $token_name --discovery-token-ca-cert-hash $hash 如： 1sudo kubeadm join 192.168.8.116:6443 --token vyi4ga.foyxqr2iz9i391q3 --discovery-token-ca-cert-hash sha256:929143bcdaa3e23c6faf20bc51ef6a57df02edf9df86cedf200320a9b4d3220a 检查 node 是否加入 master： 1kubectl get node &emsp;&emsp;到此，K8s 单机、集群版部署流程就结束了，后面我们会将 K8s 与微服务结合一起来分析 K8s 的组件的特性。 5.3 Kubernetes 的组件及负载均衡&emsp;&emsp;上面介绍了 K8s 的由来、概念以及特性，接下来，我们看看 K8s 的组件。K8s 的组件分为 Master 组件、Node 组件。 Master 组件1、kube-apiserver &emsp;&emsp;Kubernetes API 服务器的主要实现是 kube-apiserver。 kube-apiserver 设计上考虑了水平伸缩，也就是说，它可通过部署多个实例进行伸缩。 你可以运行 kube-apiserver 的多个实例，并在这些实例之间平衡流量。 2、etcd &emsp;&emsp;etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。通常，集群的 etcd 数据库通常需要有个备份计划。 3、kube-controller-manager &emsp;&emsp;在主节点上运行控制器的组件。 4、cloud-controller-manager &emsp;&emsp;cloud-controller-manager 仅运行特定于云平台的控制回路。 如果你在自己的环境中运行 Kubernetes，或者在本地计算机中运行学习环境， 所部署的环境中不需要云控制器管理器。 5、kube-scheduler &emsp;&emsp;控制平面组件，负责监视新创建的、未指定运行节点（node）的 Pods，选择节点让 Pod 在上面运行。调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限。 Node 组件&emsp;&emsp;节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。如果 Master 也被设置允许为工作节点，则节点组件同样运行在 Master 上。 1、kubelet &emsp;&emsp;一个在集群中每个节点（node）上运行的代理。 它保证容器（containers）都 运行在 Pod 中。 2、kube-proxy &emsp;&emsp;kube-proxy 是集群中每个节点上运行的网络代理， 实现 Kubernetes 服务（Service） 概念的一部分。kube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。 1.kube-proxy 主要是处理集群外部通过 nodePort 访问集群内服务，通过 iptables 规则，解析 cluterIP 到 PodIp 的过程，并提供服务的负载均衡能力。 2.kube-proxy 还可以提供集群内部服务间通过 clusterIP 访问，也会经过 kube-proxy 负责转发。 3.kube-dns 主要在 Pod 内通过 serviceName 访问其他服务，找到服务对应的 clusterIP 的关系，和一些基本的域名解析功能。 4.kube-dns 是和 kube-proxy 协同工作的，前者通过 servicename 找到指定 clusterIP，后者完成通过 clusterIP 到 PodIP 的过程。 这里，K8s 通过虚拟出一个集群 IP，利用 kube-proxy 为 service 提供 cluster 内的服务发现和负载均衡。 3、Container Runtime &emsp;&emsp;容器运行环境是负责运行容器的软件。Kubernetes 支持多个容器运行环境: Docker、 containerd、CRI-O 以及任何实现 Kubernetes CRI (容器运行环境接口)。 4、插件 Addons &emsp;&emsp;插件使用 Kubernetes 资源（DaemonSet、 Deployment 等）实现集群功能。 因为这些插件提供集群级别的功能，插件中命名空间域的资源属于 kube-system 命名空间。 5、DNS &emsp;&emsp;尽管其他插件都并非严格意义上的必需组件，但几乎所有 Kubernetes 集群都应该 有集群 DNS， 因为很多示例都需要 DNS 服务。 6、Dashboard &emsp;&emsp;Dashboard 是 Kubernetes 集群的通用的、基于 Web 的用户界面。 它使用户可以管理集群中运行的应用程序以及集群本身并进行故障排除。 K8s如何实现服务注册与发现&emsp;&emsp;上面介绍了K8s的各种组件，接下来，我们看看K8s是如何实现服务的注册与发现，然后如何做到服务的转发、实现负载均衡的能力。 &emsp;&emsp;服务在K8s中，也定义了一种资源：Service，Service，顾名思义是一个服务，什么样的服务呢？它是定义了一个服务的多种 pod 的逻辑合集以及一种访问 pod 的策略。 service 的类型有四种： ExternalName：创建一个 DNS 别名指向 service name，这样可以防止 service name 发生变化，但需要配合 DNS 插件使用。 ClusterIP：默认的类型，用于为集群内 Pod 访问时，提供的固定访问地址,默认是自动分配地址,可使用 ClusterIP 关键字指定固定 IP。 NodePort：基于 ClusterIp，用于为集群外部访问 Service 后面 Pod 提供访问接入端口。 LoadBalancer：它是基于 NodePort。 从上面讲的 Service，我们可以看到一种场景：所有的微服务在一个局域网内，或者说在一个 K8s 集群下，那么可以通过 Service 用于集群内 Pod 的访问，这就是 Service 默认的一种类型 ClusterIP，ClusterIP 这种的默认会自动分配地址。 &emsp;&emsp;那么问题来了，既然可以通过上面的 ClusterIp 来实现集群内部的服务访问，那么如何注册服务呢？其实 K8s 并没有引入任何的注册中心，使用的就是 K8s 的 kube-dns 组件。然后 K8s 将 Service 的名称当做域名注册到 kube-dns 中，每一个Service在kube-dns中都有一条DNS记录，同时，如果有服务的ip更换，kube-dns自动会同步，对服务来说是不需要改动的。通过 Service 的名称就可以访问其提供的服务。那么问题又来了，如果一个服务的 pod 对应有多个，那么如何实现 LB？其实，最终通过 kube-proxy，实现负载均衡。也就是说kube-dns通过 servicename 找到指定 clusterIP，kube-proxy完成通过 clusterIP 到 PodIP 的过程。 说到这，我们来看下 Service 的服务发现与负载均衡的策略，Service 负载分发策略有两种： RoundRobin：轮询模式，即轮询将请求转发到后端的各个 pod 上，其为默认模式。 SessionAffinity：基于客户端 IP 地址进行会话保持的模式，类似 IP Hash 的方式，来实现服务的负载均衡。 下面写一个很简单的例子： 123456789101112apiVersion: v1kind: Servicemetadata: name: cas-server-service namespace: defaultspec: ports: - name: cas-server01 port: 2000 targetPort: cas-server01 selector: app: cas-server 可以看到执行 kubectl apply -f service.yaml 后： 12345root@ubuntu:~$ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEadmin-web-service ClusterIP 10.16.129.24 &lt;none&gt; 2001&#x2F;TCP 84dcas-server-service ClusterIP 10.16.230.167 &lt;none&gt; 2000&#x2F;TCP 67dcloud-admin-service-service ClusterIP 10.16.25.178 &lt;none&gt; 1001&#x2F;TCP 190d 这样，我们可以看到默认的类型是 ClusterIP，用于为集群内 Pod 访问时，可以先通过域名来解析到多个服务地址信息，然后再通过 LB 策略来选择其中一个作为请求的对象。 K8s 如何处理微服务中常用的配置&emsp;&emsp;接下来我们看看微服务中场景的居多配置该如何来利用K8s实现统一管理。其实，在K8s中，定义了一种资源：ConfigMap，我们来看看这种资源。 &emsp;&emsp;ConfigMap，看到这个名字可以理解：它是用于保存配置信息的键值对，可以用来保存单个属性，也可以保存配置文件。对于一些非敏感的信息，比如应用的配置信息，则可以使用 ConfigMap。 创建一个 ConfigMap 有多种方式如下。 key-value 字符串创建 1kubectl create configmap test-config --from-literal&#x3D;baseDir&#x3D;&#x2F;usr &emsp;&emsp;上面的命令创建了一个名为 test-config，拥有一条 key 为 baseDir，value 为 “/usr” 的键值对数据。 根据 yml 描述文件创建 123456apiVersion: v1kind: ConfigMapmetadata: name: test-configdata: baseDir: &#x2F;usr 也可以这样，创建一个 yml 文件，选择不同的环境配置不同的信息： 123456789101112131415161718192021kind: ConfigMapapiVersion: v1metadata: name: cas-serverdata: application.yaml: |- greeting: message: Say Hello to the World --- spring: profiles: dev greeting: message: Say Hello to the Dev spring: profiles: test greeting: message: Say Hello to the Test spring: profiles: prod greeting: message: Say Hello to the Prod 注意点： ConfigMap 必须在 Pod 使用其之前创建。 Pod 只能使用同一个命名空间的 ConfigMap。 &emsp;&emsp;当然，还有其他更多用途，具体可以参考官网(https://kubernetes.io/zh/docs/concepts/configuration/configmap/)。 前面讲述了几种创建ConfigMap的方式，其中有一种在 Java 中常常用到：通过创建 yml 文件来实现配置管理。比如： 123456789101112131415161718192021kind: ConfigMapapiVersion: v1metadata: name: cas-serverdata: application.yaml: |- greeting: message: Say Hello to the World --- spring: profiles: dev greeting: message: Say Hello to the Dev spring: profiles: test greeting: message: Say Hello to the Test spring: profiles: prod greeting: message: Say Hello to the Prod &emsp;&emsp;这样，当我们启动容器时，通过 –spring.profiles.active=dev 来指定当前容器的活跃环境，即可获取 ConfigMap 中对应的配置。是不是感觉跟 Java 中的 Config 配置多个环境的配置有点类似呢？但是，我们不用那么复杂，这些统统可以交给 K8s 来处理。只需要你启动这一命令即可，是不是很简单？ 第六章 为什么选择 Kubernetes6.1 Kubernetes 与微服务的天生绝配&emsp;&emsp;其实，为什么我们需要 K8s，它到底能做什么呢？ &emsp;&emsp;容器是打包和运行应用程序的好方式。在生产环境中，你需要管理运行应用程序的容器，并确保不会停机。 例如，如果一个容器发生故障，则需要启动另一个容器。如果系统处理此行为，会不会更容易？这就是 Kubernetes 来解决这些问题的方法！ Kubernetes 为你提供了一个可弹性运行分布式系统的框架。 Kubernetes 会满足你的扩展要求、故障转移、部署模式等。 例如，Kubernetes 可以轻松管理系统的 Canary 部署。 Kubernetes 会提供： 服务发现和负载均衡，Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。 自动部署和回滚，你可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态更改为期望状态。例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。 自我修复，Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。 存储编排，Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。 自动完成装箱计算，Kubernetes 允许你指定每个容器所需 CPU 和内存（RAM）。 当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。 密钥与配置管理，Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。 &emsp;&emsp;那么对于 K8s 提供的这些功能，其实对于微服务来讲，都是很好的一个平台提供。换句话说，对于微服务来说，如果使用 K8s 的话，可以不用考虑语言上的限制，更不用考虑各种开发语言的框架的限制；对于各种语言来说，在前面也介绍过，都有很多不同的框架，那如果运用这些框架时，就需要考虑不同服务之间如果属于不同的语言，那么该如何来实现微服务的架构呢？从这一角度来分析，微服务与 K8s 属于天作之合，它们的结合可以说是天衣无缝、完美至极。在后面章节中，将会介绍它们的天衣无缝：自治与无缝迁移。 6.2 基于 Kubernetes 集群的服务治理&emsp;&emsp;其实，做微服务架构设计，我们希望得到什么呢？看下图： &emsp;&emsp;从上面这张图中可以看到，微服务的解耦、封装，从而简化开发人员的开发。调用方便，主要体现在sdk或者说client的提供者很容易被调用，这就体现了K8s的服务注册与发现。安全性考虑，基于K8s集群的保障，可以让微服务们处于一个堡垒中，这样避免外部的干扰。同时，服务之间直接走内部网络，可以大大提升性能。 &emsp;&emsp;说到这些，其实服务的自动化才是一个重点，自治能力体现了系统的健壮性。在前面章节中说到了K8s具有自动修复的能力，可以将失败或出现问题的容器进行重新编排、启动。容器本身的健康检查会被监视，当不响应用户定义的运行检查的容器就会被杀死。 &emsp;&emsp;同时，K8s提供自动部署能力，可以通过简单的命令来执行即可发挥K8s的作用。同时，K8s会根据用户的节点选择，将pod分配到对应的节点，这样对于运维人员来说，即使出现节点宕机，pod可以迅速的在其他节点被启动。 6.3 基于 Kubernetes 的服务无缝迁移&emsp;&emsp;在K8s集群中的服务，如果想要被迁移到其他的机器或其他集群。在传统的实现中，可能需要考虑到很多点：服务包的转移、共享，配置的转移，数据库的转移等等。但对于容器化来说，这些都被打包成image，而这些image可以被上传到一个仓库Habor，当需要迁移环境的时候，这些服务的镜像其实都可以不动，运维人员将要做的是：将开发人员编写的基于K8s的yaml文件在对应的集群中进行部署即可。运维人员无需关心任何其他事情，只需要在部署前，将需要的相关配置处理好即可。这将大大减少开发人员、运维的成本，让他们专注于部署，而不用关心其他的琐碎的事情。因为K8s是可以跨平台、跨系统的。主要存在K8s集群，服务都可以无缝的进行迁移过去。这也是微服务基于K8s的一个优势。下图为K8s基于Habor的架构图。 通过kubectl命令工具发起资源创建kubectl create -f xxx.yaml k8s处理相关请求后kube-scheduler服务为pod寻找一个合适的“家”node2并创建pod。 node2上的kubelet处理相关资源，使用docker拉取相关镜像并运行。 &emsp;&emsp;从上面的流程操作来看，将微服务从一个K8s集群，迁移到另一个集群，其操作是可以无缝对接的，可以同配置、同环境参数的无缝的迁移，这就是云原生下K8s带来的优势。也是需要企业现在一直推荐属性docker、K8s等技术的一个关键性要素。 第七章 第一个基于 K8s 的多语言微服务架构7.1 基于 K8s 的 Java 微服务&emsp;&emsp;前面很多的都是在说K8s为什么可以实现配置化，为什么可以提供负载均衡能力，接下来，我们举个电商系统来实现体验下K8s带来的效果，手写Java代码。 &emsp;&emsp;如下图，我们简单的画了一个系统图，从客户端到网关，再到订单服务、后台管理系统以及鉴权中心的流程。 认证中心&emsp;&emsp;基于Java现在许多比较流行的框架，作者选择了SpringCloud，因为很多基础特性SpringCloud都具备了，接下来我们看如何实现鉴权。 环境： Ubuntu 16.04 Docker 18.06 K8s 1.13.5 springboot 2.3.8.RELEASE springcloud Hoxton.SR9 首先新建一个Java项目： 引入依赖配置： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-oauth2&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes-config&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;&#x2F;artifactId&gt; &lt;scope&gt;test&lt;&#x2F;scope&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;&#x2F;groupId&gt; &lt;artifactId&gt;jjwt&lt;&#x2F;artifactId&gt; &lt;version&gt;0.9.0&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;cn.hutool&lt;&#x2F;groupId&gt; &lt;artifactId&gt;hutool-all&lt;&#x2F;artifactId&gt; &lt;version&gt;4.6.3&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;&#x2F;groupId&gt; &lt;artifactId&gt;guava&lt;&#x2F;artifactId&gt; &lt;version&gt;19.0&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;&#x2F;groupId&gt; &lt;artifactId&gt;commons-lang3&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;commons-collections&lt;&#x2F;groupId&gt; &lt;artifactId&gt;commons-collections&lt;&#x2F;artifactId&gt; &lt;version&gt;3.2.2&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt;&lt;!-- mybatis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;&#x2F;artifactId&gt; &lt;version&gt;1.1.1&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt;&lt;!-- datasource pool--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt; &lt;artifactId&gt;druid&lt;&#x2F;artifactId&gt; &lt;version&gt;1.1.3&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt; 新建服务启动类： 常见的配置文件bootstrap.yml、application.yml，主要是配置服务启动时，需要加载的参数、配置： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152management: endpoint: restart: enabled: true health: enabled: true info: enabled: truespring: application: name: cas-server cloud: kubernetes: config: sources: - name: $&#123;spring.application.name&#125; namespace: system-server discovery: all-namespaces: true reload: #自动更新配置的开关设置为打开 enabled: true #更新配置信息的模式：polling是主动拉取，event是事件通知 mode: polling #主动拉取的间隔时间是500毫秒 period: 500 redis: #redis相关配置 database: 8 host: 10.10.3.15 #localhost port: 6379 password: xxxxx #有密码时设置 jedis: pool: max-active: 8 max-idle: 8 min-idle: 0 timeout: 10000ms http: encoding: charset: UTF-8 enabled: true force: true mvc: throw-exception-if-no-handler-found: true main: allow-bean-definition-overriding: truelogging: path: &#x2F;data&#x2F;$&#123;spring.application.name&#125;&#x2F;logs &emsp;&emsp;第一个配置中，介绍了该服务的信息，以及springboot2结合K8s的一个特性：自动刷新、加载配置，该模式有两种： 主动拉取：polling，每隔一定时间拉取，自定义设置。 事件通知，event &emsp;&emsp;这里主要用主动拉取模式。后面就是配置一些插件redis、日志配置。 &emsp;&emsp;第二个配置文件可以设置一些环境、mybatis配置以及设置http协议的超时： 1234567891011121314151617181920212223spring: profiles: active: devserver: port: 2000 undertow: accesslog: enabled: false pattern: combined servlet: session: timeout: PT120Mclient: http: request: connectTimeout: 8000 readTimeout: 30000mybatis: mapperLocations: classpath:mapper&#x2F;*.xml typeAliasesPackage: com.damon.*.model &emsp;&emsp;由于我们可以设置spring.profiles.active=dev，所以可以设置几个不同环境的文件来设置日志的级别。当然，你也可以直接在启动服务时，设置参数： 1-- spring.profiles.active&#x3D;dev --logging.level.org.springframework.web&#x3D;INFO --logging.level.com.damon&#x3D;INFO 到目前为止，一些基础配置都已经写完了，接下来，我们看看鉴权的核心逻辑了。首先我们来写一个认证服务器配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156package com.damon.config;import java.util.ArrayList;import java.util.List;import javax.sql.DataSource;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.context.annotation.Configuration;import org.springframework.core.env.Environment;import org.springframework.security.authentication.AuthenticationManager;import org.springframework.security.crypto.password.PasswordEncoder;import org.springframework.security.oauth2.config.annotation.builders.ClientDetailsServiceBuilder;import org.springframework.security.oauth2.config.annotation.builders.InMemoryClientDetailsServiceBuilder;import org.springframework.security.oauth2.config.annotation.configurers.ClientDetailsServiceConfigurer;import org.springframework.security.oauth2.config.annotation.web.configuration.AuthorizationServerConfigurerAdapter;import org.springframework.security.oauth2.config.annotation.web.configuration.EnableAuthorizationServer;import org.springframework.security.oauth2.config.annotation.web.configurers.AuthorizationServerEndpointsConfigurer;import org.springframework.security.oauth2.config.annotation.web.configurers.AuthorizationServerSecurityConfigurer;import org.springframework.security.oauth2.provider.error.WebResponseExceptionTranslator;import org.springframework.security.oauth2.provider.token.TokenEnhancer;import org.springframework.security.oauth2.provider.token.TokenEnhancerChain;import org.springframework.security.oauth2.provider.token.TokenStore;import org.springframework.security.oauth2.provider.token.store.JwtAccessTokenConverter;import com.damon.component.JwtTokenEnhancer;import com.damon.login.service.LoginService;&#x2F;** * * 认证服务器配置 * @author Damon * @date 2020年1月13日 下午3:03:30 * *&#x2F;@Configuration@EnableAuthorizationServerpublic class AuthorizationServerConfig extends AuthorizationServerConfigurerAdapter &#123; @Autowired private PasswordEncoder passwordEncoder; @Autowired private AuthenticationManager authenticationManager; @Autowired private LoginService loginService; @Autowired &#x2F;&#x2F;@Qualifier(&quot;jwtTokenStore&quot;) @Qualifier(&quot;redisTokenStore&quot;) private TokenStore tokenStore; &#x2F;*@Autowired private JwtAccessTokenConverter jwtAccessTokenConverter; @Autowired private JwtTokenEnhancer jwtTokenEnhancer;*&#x2F; @Autowired private Environment env; @Autowired private DataSource dataSource; @Autowired private WebResponseExceptionTranslator userOAuth2WebResponseExceptionTranslator; &#x2F;* @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) &#123; TokenEnhancerChain enhancerChain &#x3D; new TokenEnhancerChain(); List&lt;TokenEnhancer&gt; delegates &#x3D; new ArrayList&lt;&gt;(); delegates.add(jwtTokenEnhancer); &#x2F;&#x2F;配置JWT的内容增强器 delegates.add(jwtAccessTokenConverter); enhancerChain.setTokenEnhancers(delegates); endpoints.authenticationManager(authenticationManager)&#x2F;&#x2F;支持 password 模式 .userDetailsService(loginService) .tokenStore(tokenStore) &#x2F;&#x2F;配置令牌存储策略 .accessTokenConverter(jwtAccessTokenConverter) .tokenEnhancer(enhancerChain); &#125;*&#x2F; &#x2F;** * redis token 方式 *&#x2F; @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception &#123; &#x2F;&#x2F;验证时发生的情况处理 endpoints.authenticationManager(authenticationManager) &#x2F;&#x2F;支持 password 模式 .exceptionTranslator(userOAuth2WebResponseExceptionTranslator)&#x2F;&#x2F;自定义异常处理类添加到认证服务器配置 .userDetailsService(loginService) .tokenStore(tokenStore); &#125; &#x2F;** * 客户端配置（给谁发令牌） * 不同客户端配置不同 * * authorizedGrantTypes 可以包括如下几种设置中的一种或多种： authorization_code：授权码类型。需要redirect_uri implicit：隐式授权类型。需要redirect_uri password：资源所有者（即用户）密码类型。 client_credentials：客户端凭据（客户端ID以及Key）类型。 refresh_token：通过以上授权获得的刷新令牌来获取新的令牌。 accessTokenValiditySeconds：token 的有效期 scopes：用来限制客户端访问的权限，在换取的 token 的时候会带上 scope 参数，只有在 scopes 定义内的，才可以正常换取 token。 * @param clients * @throws Exception * @author Damon * @date 2020年1月13日 * *&#x2F; @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123; clients.inMemory() .withClient(&quot;admin-web&quot;) .secret(passwordEncoder.encode(&quot;admin-web-123&quot;)) .accessTokenValiditySeconds(3600) .refreshTokenValiditySeconds(864000)&#x2F;&#x2F;配置刷新token的有效期 .autoApprove(true) &#x2F;&#x2F;自动授权配置 .scopes(&quot;all&quot;)&#x2F;&#x2F;配置申请的权限范围 .authorizedGrantTypes(&quot;password&quot;, &quot;authorization_code&quot;, &quot;client_credentials&quot;, &quot;refresh_token&quot;)&#x2F;&#x2F;配置授权模式 .redirectUris(&quot;http:&#x2F;&#x2F;localhost:2001&#x2F;login&quot;)&#x2F;&#x2F;授权码模式开启后必须指定 .and() .withClient(&quot;order-service&quot;) .secret(passwordEncoder.encode(&quot;order-service-123&quot;)) .accessTokenValiditySeconds(3600) .refreshTokenValiditySeconds(864000)&#x2F;&#x2F;配置刷新token的有效期 .autoApprove(true) &#x2F;&#x2F;自动授权配置 .scopes(&quot;all&quot;) .authorizedGrantTypes(&quot;password&quot;, &quot;authorization_code&quot;, &quot;client_credentials&quot;, &quot;refresh_token&quot;)&#x2F;&#x2F;配置授权模式 .redirectUris(&quot;http:&#x2F;&#x2F;localhost:2003&#x2F;login&quot;)&#x2F;&#x2F;授权码模式开启后必须指定 .and() .withClient(&quot;customer-service&quot;) .secret(passwordEncoder.encode(&quot;customer-service-123&quot;)) .accessTokenValiditySeconds(3600) .refreshTokenValiditySeconds(864000)&#x2F;&#x2F;配置刷新token的有效期 .autoApprove(true) &#x2F;&#x2F;自动授权配置 .scopes(&quot;all&quot;) .authorizedGrantTypes(&quot;password&quot;, &quot;authorization_code&quot;, &quot;client_credentials&quot;, &quot;refresh_token&quot;)&#x2F;&#x2F;配置授权模式 .redirectUris(&quot;http:&#x2F;&#x2F;localhost:6000&#x2F;login&quot;)&#x2F;&#x2F;授权码模式开启后必须指定 ; &#125; @Override public void configure(AuthorizationServerSecurityConfigurer security) &#123; security.allowFormAuthenticationForClients();&#x2F;&#x2F;是允许客户端访问 OAuth2 授权接口，否则请求 token 会返回 401 security.checkTokenAccess(&quot;isAuthenticated()&quot;);&#x2F;&#x2F;是允许已授权用户访问 checkToken 接口 security.tokenKeyAccess(&quot;isAuthenticated()&quot;); &#x2F;&#x2F; security.tokenKeyAccess(&quot;permitAll()&quot;);获取密钥需要身份认证，使用单点登录时必须配置，是允许已授权用户获取 token 接口 &#125;&#125; &emsp;&emsp;在这个配置中，我们redis来记录token，自定义了登录认证的逻辑LoginService，自定义异常处理类添加到认证服务器配置。同时函数configure加载了居多需要认证的客户端服务，配置了授权模式：”password”、”authorization_code”、”client_credentials”、”refresh_token”，配置刷新token的有效期。 &emsp;&emsp;我们先来看看自定义的认证逻辑，先来看看接口类： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.damon.login.service;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import org.springframework.security.core.userdetails.UserDetails;import org.springframework.security.core.userdetails.UserDetailsService;import org.springframework.security.core.userdetails.UsernameNotFoundException;import com.damon.commons.Response;&#x2F;** * @author Damon * @date 2018年11月15日 上午11:59:24 * *&#x2F;public interface LoginService extends UserDetailsService &#123; &#x2F;** * * Spring Security默认函数 * @param username * @return * @throws UsernameNotFoundException * @author Damon * @date 2020年1月13日 * *&#x2F; UserDetails loadUserByUsername(String username) throws UsernameNotFoundException; &#x2F;&#x2F;以下自定义 Response&lt;Object&gt; login(String username, String password); &#x2F;** * * 校验token合法性 * @param request * @param token * @return * @author Damon * @date 2019年8月15日 * *&#x2F; Response&lt;Object&gt; verify(HttpServletRequest request, String token); Response&lt;Object&gt; updatePwd(HttpServletRequest req, String username, String oldPwd, String newPwd); &#x2F;&#x2F;Response&lt;Object&gt; logout(HttpServletRequest req, HttpServletResponse res);&#125; &emsp;&emsp;在Spring Security中，有默认的函数loadUserByUsername来实现鉴权逻辑： 1234567891011121314151617181920212223242526272829303132&#x2F;** * Auth * 登录认证 * 实际中从数据库获取信息 * 这里为了做演示，把用户名、密码和所属角色都写在代码里了，正式环境中，这里应该是从数据库或者其他地方根据用户名将加密后的密码及所属角色查出来的。账号 damon ， * 密码123456，稍后在换取 token 的时候会用到。并且给这个用户设置 &quot;ROLE_ADMIN&quot; 角色。 * *&#x2F; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; logger.info(&quot;clientIp is: &#123;&#125; ,username: &#123;&#125;&quot;, IpUtil.getClientIp(req), username); logger.info(&quot;serverIp is: &#123;&#125;&quot;, IpUtil.getCurrentIp()); &#x2F;&#x2F; 查询数据库操作 SysUser user &#x3D; userMapper.getUserByUsername(username); if (user &#x3D;&#x3D; null) &#123; logger.error(&quot;user not exist&quot;); throw new UsernameNotFoundException(&quot;username is not exist&quot;); &#125; else &#123; &#x2F;&#x2F; 用户角色也应在数据库中获取，这里简化 String role &#x3D; &quot;&quot;; if(user.getIsAdmin() &#x3D;&#x3D; 1) &#123; role &#x3D; &quot;admin&quot;; &#125; List&lt;SimpleGrantedAuthority&gt; authorities &#x3D; Lists.newArrayList(); authorities.add(new SimpleGrantedAuthority(role)); &#x2F;&#x2F;String password &#x3D; passwordEncoder.encode(&quot;123456&quot;);&#x2F;&#x2F; 123456是密码 &#x2F;&#x2F;return new User(username, password, authorities); &#x2F;&#x2F; 线上环境应该通过用户名查询数据库获取加密后的密码 return new User(username, user.getPassword(), authorities); &#125; &#125; &emsp;&emsp;可以看到通过用户信息获取用户的权限，然后记录用户信息。这里使用redis来记录用户信息以及token信息： 123456789101112131415161718192021222324252627package com.damon.config;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.security.oauth2.provider.token.TokenStore;import org.springframework.security.oauth2.provider.token.store.redis.RedisTokenStore;&#x2F;** * 使用redis存储token的配置 * @author Damon * @date 2020年1月13日 下午3:03:19 * *&#x2F;@Configurationpublic class RedisTokenStoreConfig &#123; @Autowired private RedisConnectionFactory redisConnectionFactory; @Bean public TokenStore redisTokenStore ()&#123; &#x2F;&#x2F;return new RedisTokenStore(redisConnectionFactory); return new MyRedisTokenStore(redisConnectionFactory); &#125;&#125; &emsp;&emsp;同时，我们自定义了异常处理类，统一处理异常： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127package com.damon.config;import java.io.IOException;import org.springframework.http.HttpHeaders;import org.springframework.http.HttpStatus;import org.springframework.http.ResponseEntity;import org.springframework.security.access.AccessDeniedException;import org.springframework.security.core.AuthenticationException;import org.springframework.security.oauth2.common.DefaultThrowableAnalyzer;import org.springframework.security.oauth2.common.exceptions.InsufficientScopeException;import org.springframework.security.oauth2.common.exceptions.OAuth2Exception;import org.springframework.security.oauth2.provider.error.WebResponseExceptionTranslator;import org.springframework.security.web.util.ThrowableAnalyzer;import org.springframework.stereotype.Component;import org.springframework.web.HttpRequestMethodNotSupportedException;import com.damon.exception.UserOAuth2Exception;&#x2F;** * * 自定义异常转换类 * @author Damon * @date 2020年2月27日 上午10:28:19 * *&#x2F;@Component(&quot;userOAuth2WebResponseExceptionTranslator&quot;)public class UserOAuth2WebResponseExceptionTranslator implements WebResponseExceptionTranslator &#123; private ThrowableAnalyzer throwableAnalyzer &#x3D; new DefaultThrowableAnalyzer(); @Override public ResponseEntity&lt;OAuth2Exception&gt; translate(Exception e) throws Exception &#123; Throwable[] causeChain &#x3D; this.throwableAnalyzer.determineCauseChain(e); Exception ase &#x3D; (OAuth2Exception)this.throwableAnalyzer.getFirstThrowableOfType(OAuth2Exception.class, causeChain); &#x2F;&#x2F;异常链中有OAuth2Exception异常 if (ase !&#x3D; null) &#123; return this.handleOAuth2Exception((OAuth2Exception)ase); &#125; &#x2F;&#x2F;身份验证相关异常 ase &#x3D; (AuthenticationException)this.throwableAnalyzer.getFirstThrowableOfType(AuthenticationException.class, causeChain); if (ase !&#x3D; null) &#123; return this.handleOAuth2Exception(new UserOAuth2WebResponseExceptionTranslator.UnauthorizedException(e.getMessage(), e)); &#125; &#x2F;&#x2F;异常链中包含拒绝访问异常 ase &#x3D; (AccessDeniedException)this.throwableAnalyzer.getFirstThrowableOfType(AccessDeniedException.class, causeChain); if (ase instanceof AccessDeniedException) &#123; return this.handleOAuth2Exception(new UserOAuth2WebResponseExceptionTranslator.ForbiddenException(ase.getMessage(), ase)); &#125; &#x2F;&#x2F;异常链中包含Http方法请求异常 ase &#x3D; (HttpRequestMethodNotSupportedException)this.throwableAnalyzer.getFirstThrowableOfType(HttpRequestMethodNotSupportedException.class, causeChain); if(ase instanceof HttpRequestMethodNotSupportedException)&#123; return this.handleOAuth2Exception(new UserOAuth2WebResponseExceptionTranslator.MethodNotAllowed(ase.getMessage(), ase)); &#125; return this.handleOAuth2Exception(new UserOAuth2WebResponseExceptionTranslator.ServerErrorException(HttpStatus.INTERNAL_SERVER_ERROR.getReasonPhrase(), e)); &#125; private ResponseEntity&lt;OAuth2Exception&gt; handleOAuth2Exception(OAuth2Exception e) throws IOException &#123; int status &#x3D; e.getHttpErrorCode(); HttpHeaders headers &#x3D; new HttpHeaders(); headers.set(&quot;Cache-Control&quot;, &quot;no-store&quot;); headers.set(&quot;Pragma&quot;, &quot;no-cache&quot;); if (status &#x3D;&#x3D; HttpStatus.UNAUTHORIZED.value() || e instanceof InsufficientScopeException) &#123; headers.set(&quot;WWW-Authenticate&quot;, String.format(&quot;%s %s&quot;, &quot;Bearer&quot;, e.getSummary())); &#125; UserOAuth2Exception exception &#x3D; new UserOAuth2Exception(e.getMessage(),e); ResponseEntity&lt;OAuth2Exception&gt; response &#x3D; new ResponseEntity(exception, headers, HttpStatus.valueOf(status)); return response; &#125; private static class MethodNotAllowed extends OAuth2Exception &#123; public MethodNotAllowed(String msg, Throwable t) &#123; super(msg, t); &#125; @Override public String getOAuth2ErrorCode() &#123; return &quot;method_not_allowed&quot;; &#125; @Override public int getHttpErrorCode() &#123; return 405; &#125; &#125; private static class UnauthorizedException extends OAuth2Exception &#123; public UnauthorizedException(String msg, Throwable t) &#123; super(msg, t); &#125; @Override public String getOAuth2ErrorCode() &#123; return &quot;unauthorized&quot;; &#125; @Override public int getHttpErrorCode() &#123; return 401; &#125; &#125; private static class ServerErrorException extends OAuth2Exception &#123; public ServerErrorException(String msg, Throwable t) &#123; super(msg, t); &#125; @Override public String getOAuth2ErrorCode() &#123; return &quot;server_error&quot;; &#125; @Override public int getHttpErrorCode() &#123; return 500; &#125; &#125; private static class ForbiddenException extends OAuth2Exception &#123; public ForbiddenException(String msg, Throwable t) &#123; super(msg, t); &#125; @Override public String getOAuth2ErrorCode() &#123; return &quot;access_denied&quot;; &#125; @Override public int getHttpErrorCode() &#123; return 403; &#125; &#125;&#125; &emsp;&emsp;前面是对认证服务进行配置的详解，接下来，我们看看对于资源服务的配置： 12345678910111213141516171819202122232425262728293031323334353637package com.damon.config;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.oauth2.config.annotation.web.configuration.EnableResourceServer;import org.springframework.security.oauth2.config.annotation.web.configuration.ResourceServerConfigurerAdapter;&#x2F;** * * 资源服务器配置 * @author Damon * @date 2020年1月13日 下午3:03:48 * *&#x2F;@Configuration@EnableResourceServerpublic class ResourceServerConfig extends ResourceServerConfigurerAdapter &#123; @Override public void configure(HttpSecurity http) throws Exception &#123; http.csrf().disable() .exceptionHandling() .authenticationEntryPoint(new AuthenticationEntryPointHandle()) &#x2F;&#x2F;.authenticationEntryPoint((request, response, authException) -&gt; response.sendError(HttpServletResponse.SC_UNAUTHORIZED)) .and() .requestMatchers().antMatchers(&quot;&#x2F;api&#x2F;**&quot;) .and() .authorizeRequests() .antMatchers(&quot;&#x2F;api&#x2F;**&quot;).authenticated() .and() .httpBasic(); &#125;&#125; &emsp;&emsp;这里也配置了对于资源拦截的统一处理： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.damon.config;import java.io.IOException;import javax.servlet.ServletException;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import org.springframework.http.HttpStatus;import org.springframework.security.core.AuthenticationException;import org.springframework.security.web.AuthenticationEntryPoint;import com.alibaba.fastjson.JSON;import com.damon.commons.Response;&#x2F;** * * 统一结果处理 * * @author Damon * @date 2020年1月16日 上午11:11:44 * *&#x2F;public class AuthenticationEntryPointHandle implements AuthenticationEntryPoint &#123; &#x2F;** * * @author Damon * @date 2020年1月16日 * *&#x2F; @Override public void commence(HttpServletRequest request, HttpServletResponse response, AuthenticationException authException) throws IOException, ServletException &#123; &#x2F;&#x2F;response.setStatus(HttpServletResponse.SC_FORBIDDEN); &#x2F;&#x2F;response.setStatus(HttpStatus.OK.value()); &#x2F;&#x2F;response.setHeader(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;); &#x2F;&#x2F;gateway已加，无需再加 &#x2F;&#x2F;response.setHeader(&quot;Access-Control-Allow-Headers&quot;, &quot;token&quot;); &#x2F;&#x2F;解决低危漏洞点击劫持 X-Frame-Options Header未配置 response.setHeader(&quot;X-Frame-Options&quot;, &quot;SAMEORIGIN&quot;); response.setCharacterEncoding(&quot;UTF-8&quot;); response.setContentType(&quot;application&#x2F;json; charset&#x3D;utf-8&quot;); response.getWriter() .write(JSON.toJSONString(Response.ok(response.getStatus(), -2, authException.getMessage(), null))); &#x2F;*response.getWriter() .write(JSON.toJSONString(Response.ok(200, -2, &quot;Internal Server Error&quot;, authException.getMessage())));*&#x2F; &#125;&#125; &emsp;&emsp;以上就是认证中心的核心代码了，这里有一些需要注意的地方：有2个拦截器，鉴权服务配置、资源权限配置，鉴权时可以通过几种模式来进行。授权码模式交互比较多，密码模式比较常见应用。 &emsp;&emsp;接下来，我们新建一个电商系统的订单服务，作为鉴权的客户端，我们来看看代码。 订单服务客户端&emsp;&emsp;订单系统我们考虑到系统的高可用，以及系统的TPS，我们采用负载均衡手段，加上一些中间件来处理，使得服务更加具有代表性。 &emsp;&emsp;订单系统的环境这就不介绍了，这里主要还是看依赖K8s的组件： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes-config&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt;&lt;!-- springcloud-k8s-discovery --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-commons&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-kubernetes-core&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-kubernetes-discovery&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes-ribbon&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &emsp;&emsp;上面的依赖是K8s对于Java的sdk-client，接下来配置一些环境使其生效： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354spring: application: name: order-service #redis: #redis相关配置 #password: 123456 #有密码时设置 cloud: kubernetes: config: sources: - name: $&#123;spring.application.name&#125; namespace: system-server discovery: all-namespaces: true reload: #自动更新配置的开关设置为打开 enabled: true #更新配置信息的模式：polling是主动拉取，event是事件通知 mode: polling #主动拉取的间隔时间是500毫秒 period: 500 http: encoding: charset: UTF-8 enabled: true force: true mvc: throw-exception-if-no-handler-found: true main: allow-bean-definition-overriding: true # 当遇到同样名称时，是否允许覆盖注册logging: path: &#x2F;data&#x2F;$&#123;spring.application.name&#125;&#x2F;logscas-server-url: http:&#x2F;&#x2F;cas-server-service #http:&#x2F;&#x2F;localhost:2000#设置可以访问的地址security: oauth2: #与cas-server对应的配置 client: client-id: order-service client-secret: order-service-123 user-authorization-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;authorize #是授权码认证方式需要的 access-token-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;token #是密码模式需要用到的获取 token 的接口 resource: loadBalanced: true #jwt: #jwt存储token时开启 #key-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;token_key #key-value: test_jwt_sign_key id: order-service #指定用户信息地址 user-info-uri: $&#123;cas-server-url&#125;&#x2F;api&#x2F;user #指定user info的URI，原生地址后缀为&#x2F;auth&#x2F;user prefer-token-info: false #token-info-uri: authorization: check-token-access: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;check_token #当此web服务端接收到来自UI客户端的请求后，需要拿着请求中的 token 到认证服务端做 token 验证，就是请求的这个接口 &emsp;&emsp;最上面是配置了加载服务的环境变量，采用K8s的ConfigMap实现，不再使用复杂的各种大厂提供的插件。下面是订单服务接入鉴权时需要的鉴权配置。这里为了鉴权的健壮性，采用了分布式部署： 1cas-server-url: http:&#x2F;&#x2F;cas-server-service &emsp;&emsp;通过K8s的Service来进行及安全认证，实现服务的高可用。接下来是订单服务的各种模式的支持，这里主要是授权码模式、密码模式。同时，开启了订单服务的负载均衡策略： 1234567891011121314151617181920212223242526272829303132333435363738394041package com.damon;import org.springframework.boot.SpringApplication;import org.springframework.boot.SpringBootConfiguration;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.boot.autoconfigure.security.oauth2.client.EnableOAuth2Sso;import org.springframework.boot.context.properties.EnableConfigurationProperties;import org.springframework.cloud.client.circuitbreaker.EnableCircuitBreaker;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.netflix.hystrix.EnableHystrix;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;import com.damon.config.EnvConfig;&#x2F;** * @author Damon * @date 2020年1月13日 下午3:23:06 * *&#x2F;@EnableOAuth2Sso@Configuration&#x2F;&#x2F;@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(basePackages &#x3D; &#123;&quot;com.damon&quot;&#125;)@EnableConfigurationProperties(EnvConfig.class)@EnableDiscoveryClient@EnableCircuitBreaker&#x2F;&#x2F;@EnableHystrix&#x2F;&#x2F;@RibbonClients针对多个服务源进行策略的指定 ,这里注意这种方式时，RibbonConfiguration类不能被包含在@ComponentScan的扫描包中&#x2F;*@RibbonClients(value &#x3D; &#123; @RibbonClient(name&#x3D;&quot;cas-server-service&quot;, configuration &#x3D; RibbonConfiguration.class), @RibbonClient(name&#x3D;&quot;admin-web-service&quot;, configuration &#x3D; RibbonConfiguration.class)&#125;)*&#x2F;public class OrderApp &#123; public static void main(String[] args) &#123; SpringApplication.run(OrderApp.class, args); &#125;&#125; &emsp;&emsp;接下来，我们看下客户端的资源配置： 12345678910111213141516171819202122232425262728293031323334353637package com.damon.config;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.oauth2.config.annotation.web.configuration.EnableResourceServer;import org.springframework.security.oauth2.config.annotation.web.configuration.ResourceServerConfigurerAdapter;import javax.servlet.http.HttpServletResponse;&#x2F;** * * * @author Damon * @date 2020年1月16日 下午6:28:35 * *&#x2F;@Configuration@EnableResourceServerpublic class ResourceServerConfig extends ResourceServerConfigurerAdapter &#123; @Override public void configure(HttpSecurity http) throws Exception &#123; http.csrf().disable() .exceptionHandling() .authenticationEntryPoint(new AuthenticationEntryPointHandle()) &#x2F;&#x2F;.authenticationEntryPoint((request, response, authException) -&gt; response.sendError(HttpServletResponse.SC_UNAUTHORIZED)) .and() .requestMatchers().antMatchers(&quot;&#x2F;api&#x2F;**&quot;) .and() .authorizeRequests() .antMatchers(&quot;&#x2F;api&#x2F;**&quot;).authenticated() .and() .httpBasic(); &#125;&#125; &emsp;&emsp;这里加了一个统一结果处理类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.damon.config;import java.io.IOException;import javax.servlet.ServletException;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import org.springframework.security.core.AuthenticationException;import org.springframework.security.web.AuthenticationEntryPoint;import com.alibaba.fastjson.JSON;import com.damon.commons.Response;&#x2F;** * * 统一结果处理 * * @author Damon * @date 2020年1月16日 上午11:11:44 * *&#x2F;public class AuthenticationEntryPointHandle implements AuthenticationEntryPoint &#123; &#x2F;** * * @author Damon * @date 2020年1月16日 * *&#x2F; @Override public void commence(HttpServletRequest request, HttpServletResponse response, AuthenticationException authException) throws IOException, ServletException &#123; &#x2F;&#x2F;response.setStatus(HttpServletResponse.SC_FORBIDDEN); &#x2F;&#x2F;response.setStatus(HttpStatus.OK.value()); &#x2F;&#x2F;response.setHeader(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;); &#x2F;&#x2F;gateway已加，无需再加 &#x2F;&#x2F;response.setHeader(&quot;Access-Control-Allow-Headers&quot;, &quot;token&quot;); &#x2F;&#x2F;解决低危漏洞点击劫持 X-Frame-Options Header未配置 response.setHeader(&quot;X-Frame-Options&quot;, &quot;SAMEORIGIN&quot;); response.setCharacterEncoding(&quot;UTF-8&quot;); response.setContentType(&quot;application&#x2F;json; charset&#x3D;utf-8&quot;); response.getWriter() .write(JSON.toJSONString(Response.ok(response.getStatus(), -2, authException.getMessage(), null))); &#x2F;*response.getWriter() .write(JSON.toJSONString(Response.ok(200, -2, &quot;Internal Server Error&quot;, authException.getMessage())));*&#x2F; &#125; &#x2F;*@Override public void commence(HttpServletRequest request, HttpServletResponse response, AuthenticationException authException) throws IOException, ServletException &#123; response.setCharacterEncoding(&quot;UTF-8&quot;); response.setContentType(&quot;application&#x2F;json; charset&#x3D;utf-8&quot;); response.setStatus(HttpServletResponse.SC_FORBIDDEN); response.getWriter() .write(JSON.toJSONString(Response.ok(200, -2, &quot;Internal Server Error&quot;, authException.getMessage()))); &#125;*&#x2F;&#125; &emsp;&emsp;前面说增加了负载均衡策略，此处我们引入的是Ribbon作为负载均衡器： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152server: port: 2003 undertow: accesslog: enabled: false pattern: combined servlet: session: timeout: PT120M cookie: name: ORDER-SERVICE-SESSIONID #防止Cookie冲突，冲突会导致登录验证不通过client: http: request: connectTimeout: 8000 readTimeout: 30000backend: ribbon: eureka: enabled: false client: enabled: true ServerListRefreshInterval: 5000ribbon: ConnectTimeout: 2000 ReadTimeout: 3000 eager-load: enabled: true clients: cas-server-service,admin-web-service MaxAutoRetries: 1 #对第一次请求的服务的重试次数 MaxAutoRetriesNextServer: 1 #要重试的下一个服务的最大数量（不包括第一个服务） #listOfServers: localhost:5556,localhost:5557 #ServerListRefreshInterval: 2000 OkToRetryOnAllOperations: true NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RoundRobinRule #com.damon.config.RibbonConfiguration #自定义策略类hystrix: command: BackendCall: #default or commandKey的值 execution: isolation: thread: timeoutInMilliseconds: 5000 threadpool: BackendCallThread: coreSize: 5 &emsp;&emsp;同时，这里采用的是默认的轮训策略，当然可以自定义一种策略： 123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.damon.config;import org.springframework.context.annotation.Bean;import com.netflix.client.config.IClientConfig;import com.netflix.loadbalancer.IPing;import com.netflix.loadbalancer.IRule;import com.netflix.loadbalancer.PingUrl;import com.netflix.loadbalancer.RoundRobinRule;&#x2F;** * @author Damon * @date 2019年10月30日 下午5:03:54 * *&#x2F;public class RibbonConfiguration &#123; &#x2F;** * 检查服务是否可用的实例， * 此地址返回的响应的返回码如果是200表示服务可用 * @param config * @return *&#x2F; @Bean public IPing ribbonPing(IClientConfig config)&#123; return new PingUrl(); &#125; &#x2F;** * 轮询规则 * @param config * @return *&#x2F; @Bean public IRule ribbonRule(IClientConfig config)&#123; &#x2F;&#x2F;return new AvailabilityFilteringRule(); return new RoundRobinRule();&#x2F;&#x2F;轮询 &#x2F;&#x2F;return new RetryRule();&#x2F;&#x2F;重试 &#x2F;&#x2F;return new RandomRule();&#x2F;&#x2F;这里配置策略，和配置文件对应 &#x2F;&#x2F;return new WeightedResponseTimeRule();&#x2F;&#x2F;这里配置策略，和配置文件对应 &#x2F;&#x2F;return new BestAvailableRule();&#x2F;&#x2F;选择一个最小的并发请求的server &#x2F;&#x2F;return new MyProbabilityRandomRule();&#x2F;&#x2F;自定义 &#125;&#125; &emsp;&emsp;在上面的策略函数ribbonRule中，实现自定义策略： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.damon.config;import java.util.List;import java.util.Random;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import com.netflix.loadbalancer.BaseLoadBalancer;import com.netflix.loadbalancer.ILoadBalancer;import com.netflix.loadbalancer.IRule;import com.netflix.loadbalancer.Server;&#x2F;** * 实现自定义负载均衡策略 * @author Damon * @date 2019年10月30日 上午9:08:49 * *&#x2F;public class MyProbabilityRandomRule implements IRule &#123; Logger log &#x3D; LoggerFactory.getLogger(MyProbabilityRandomRule.class); ILoadBalancer balancer &#x3D; new BaseLoadBalancer(); @Override public Server choose(Object key) &#123; List&lt;Server&gt; allServers &#x3D; balancer.getAllServers(); Random random &#x3D; new Random(); final int number &#x3D; random.nextInt(10); if (number &lt; 7) &#123; return findServer(allServers,8091); &#125; return findServer(allServers,8092); &#125; private Server findServer(List&lt;Server&gt; allServers, int port) &#123; for (Server server : allServers) &#123; if (server.getPort() &#x3D;&#x3D; port) &#123; return server; &#125; &#125; log.info(&quot;NULL port&#x3D;&quot;+port); return null; &#125; @Override public void setLoadBalancer(ILoadBalancer lb) &#123; this.balancer &#x3D; lb; &#125; @Override public ILoadBalancer getLoadBalancer() &#123; return this.balancer; &#125;&#125; &emsp;&emsp;到此，关于客户端的配置全部解决完了，接下来简单写一个API： 1234567891011121314@GetMapping(&quot;&#x2F;getCurrentOrderUser&quot;)@PreAuthorize(&quot;hasAuthority(&#39;admin&#39;)&quot;)public Object getCurrentOrderUser(Authentication authentication) &#123; logger.info(&quot;test password&quot;); return authentication;&#125;&#x2F;&#x2F;@PreAuthorize(&quot;hasAuthority(&#39;admin&#39;)&quot;)@PreAuthorize(&quot;hasAuthority(&#39;admin1&#39;)&quot;)@GetMapping(&quot;&#x2F;auth&#x2F;admin&quot;)public Object adminAuth() &#123; logger.info(&quot;aAaqsqwsw&quot;); return &quot;Has admin auth!&quot;;&#125; &emsp;&emsp;我们简单的实现了一个客户端的代码，以及鉴权认证。 基于Java阐述K8s的服务发现&emsp;&emsp;前面从K8s组件以及资源的角度，来分析了K8s的服务注册与发现的缘由。接下来，我们看看基于Java，K8s如何提供该项功能。 &emsp;&emsp;在pom.xml中，有对spring-cloud-kubernetes框架的依赖配置： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-kubernetes-discovery&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt; 我们看下这个依赖的源码： &emsp;&emsp;spring容器启动时，会找到classpath下的spring.factories文件，spring.factories文件中有两个类：KubernetesDiscoveryClientAutoConfiguration和KubernetesDiscoveryClientConfigClientBootstrapConfiguration都会被实例化； &emsp;&emsp;再看KubernetesDiscoveryClientAutoConfiguration源码，注意kubernetesDiscoveryClient方法，这里面实例化了DiscoveryController所需的DiscoveryClient接口实现: 12345678910@Bean@ConditionalOnMissingBean@ConditionalOnProperty(name &#x3D; &quot;spring.cloud.kubernetes.discovery.enabled&quot;, matchIfMissing &#x3D; true)public KubernetesDiscoveryClient kubernetesDiscoveryClient(KubernetesClient client, KubernetesDiscoveryProperties properties, KubernetesClientServicesFunction kubernetesClientServicesFunction, DefaultIsServicePortSecureResolver isServicePortSecureResolver) &#123; return new KubernetesDiscoveryClient(client, properties, kubernetesClientServicesFunction, isServicePortSecureResolver);&#125; &emsp;&emsp;其实最终是向kubernetes的API Server发起http请求，获取service资源的数据列表；在K8s的API Server收到请求后，由API Server从etcd中取得service的数据返回。 &emsp;&emsp;可见 spring-cloud-kubernetes 的 DiscoveryClient 服务将 kubernetes 中的”service”资源与 SpringCloud 中的服务对应起来了，有了这个 DiscoveryClient，我们在 kubernetes 环境就不需要 eureka 来做注册发现了，而是直接使用 kubernetes 的服务机制，此时不得不感慨 SpringCloud 的对 DiscoveryClient 的设计是如此的精妙。 7.2 手写第一个Golang微服务&emsp;&emsp;现在有不少的Golang框架：Beego、Gin等，今天为了简单起见，我们直接使用Beego。 环境 GoLand 2020.3.2 Golang 1.12+ 第一个Golang的main函数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package mainimport ( &quot;github.com&#x2F;astaxie&#x2F;beego&quot; &quot;github.com&#x2F;astaxie&#x2F;beego&#x2F;logs&quot; &quot;github.com&#x2F;spf13&#x2F;pflag&quot; _ &quot;pay-service&#x2F;routers&quot; &quot;time&quot;)func init() &#123; logs.SetLogFuncCall(true) logs.SetLogFuncCallDepth(3) logs.SetLogger(logs.AdapterFile, &#96;&#123;&quot;filename&quot;:&quot;&#x2F;data&#x2F;pay-service&#x2F;log&#x2F;pay-service.log&quot;,&quot;level&quot;:7,&quot;daily&quot;:true,&quot;maxdays&quot;:7,&quot;color&quot;:true&#125;&#96;)&#125;var logFlushFreq &#x3D; pflag.Duration(&quot;log-flush-frequency&quot;, 5*time.Second, &quot;Maximum number of seconds between log flushes&quot;)func main() &#123; &#x2F;&#x2F;先在先执行，后者不再执行 &#x2F;*http.HandleFunc(&quot;&#x2F;health&quot;, func(w http.ResponseWriter, r *http.Request) &#123; fmt.Fprintf(w, fmt.Sprintf(&quot;%v&quot;, true)) &#125;) http.ListenAndServe(&quot;:38080&quot;, nil)*&#x2F; beego.Run() &#x2F;&#x2F;cache health &#x2F;&#x2F;var configFile &#x3D; flag.String(&quot;config&quot;, &quot;conf&#x2F;config-dev.yaml&quot;, &quot;the config file of pay service&quot;) &#x2F;*flag.Parse() go wait.Until(glog.Flush, *logFlushFreq, wait.NeverStop) defer glog.Flush() config, err :&#x3D; config.ParseFromYaml(*configFile) if config.ProcessNum &gt; 0 &#123; runtime.GOMAXPROCS(config.ProcessNum) &#125; else &#123; runtime.GOMAXPROCS(runtime.NumCPU()) &#125; runtime.NumCPU() cache :&#x3D; cache.New(&amp;cache.Config&#123;&#125;) done :&#x3D; make(chan struct&#123;&#125;) cache.Run(done) &lt;-done*&#x2F;&#125; &emsp;&emsp;这里启动时，很简单，就是beego的线程启动，当然，如果你需要添加一些其他的，可以在init函数中添加逻辑。 &emsp;&emsp;接下来，我们新建一个routers目录，新建router文件来设置路由： 1234567891011121314151617181920&#x2F;&#x2F; @APIVersion V1.0.0&#x2F;&#x2F; @Title 支付服务 API&#x2F;&#x2F; @Description 提供支付服务相关的API集合.package routersimport ( &quot;github.com&#x2F;astaxie&#x2F;beego&quot; &quot;pay-service&#x2F;api&quot; &quot;common-core&#x2F;common&#x2F;check&quot;)func init() &#123; ns :&#x3D; beego.NewNamespace(&quot;&#x2F;api&#x2F;v1&quot;, beego.NSRouter(&quot;&#x2F;healthz&quot;, &amp;api.ProbeHealthAPI&#123;&#125;), beego.NSRouter(&quot;&#x2F;pay&#x2F;:orderId&quot;, &amp;api.PayDetailAPI&#123;&#125;), ) beego.InsertFilter(&quot;&#x2F;api&#x2F;v1&#x2F;*&quot;, beego.BeforeRouter, check.CheckAuth) beego.AddNamespace(ns)&#125; &emsp;&emsp;上面的路由包括两个接口，一个是服务自身的健康检测healthz，还有一个基于订单号查询支付服务信息。 12345678910111213141516package apiimport ( base_api &quot;common-core&#x2F;common&#x2F;api&quot; &quot;strings&quot;)type PayDetailAPI struct &#123; base_api.BaseAPI&#125;func (pd *PayDetailAPI) Get() &#123; orderId :&#x3D; strings.TrimSpace(pd.GetString(&quot;:orderId&quot;)) pd.RespSucc(&quot;success: &quot; + orderId)&#125; &emsp;&emsp;这里由于API需要集成Beego的控制类，所以新建一个common目录，提供基础功能，包括api： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package apiimport ( &quot;common-core&#x2F;common&#x2F;handler&quot; &quot;common-core&#x2F;common&#x2F;res&quot; &quot;encoding&#x2F;json&quot; &quot;github.com&#x2F;astaxie&#x2F;beego&quot; &quot;github.com&#x2F;astaxie&#x2F;beego&#x2F;logs&quot; &quot;io&#x2F;ioutil&quot; &quot;net&#x2F;http&quot;)type BaseAPI struct &#123; beego.Controller&#125;func (ba *BaseAPI) GetBody(body interface&#123;&#125;) error &#123; bodyByte, err :&#x3D; ioutil.ReadAll(ba.Ctx.Request.Body) if err !&#x3D; nil &#123; logs.Error(err.Error()) return handler.NewBadReqError(handler.ComErrorCodeInvalidPara, err.Error()) &#125; if err :&#x3D; json.Unmarshal(bodyByte, body); err !&#x3D; nil &#123; return handler.NewBadReqError(handler.ComErrorCodeInvalidPara, err.Error()) &#125; return nil&#125;func (ba *BaseAPI) RespSucc(body interface&#123;&#125;) &#123; ba.Resp(http.StatusOK, (&amp;res.DefaultResponse&#123;&#125;).Succ(body))&#125;func (ba *BaseAPI) RespError(err error) &#123; if handler.IsBadRequestError(err) &#123; ba.Resp(http.StatusBadRequest, (&amp;res.DefaultResponse&#123;&#125;).Fail(err)) &#125; else &#123; ba.Resp(http.StatusInternalServerError, (&amp;res.DefaultResponse&#123;&#125;).Fail(err)) &#125;&#125;func (ba *BaseAPI) RespSuccError(err error) &#123; if handler.IsBadRequestError(err) &#123; ba.Resp(http.StatusOK, (&amp;res.DefaultResponse&#123;&#125;).Fail(err)) &#125; else &#123; ba.Resp(http.StatusOK, (&amp;res.DefaultResponse&#123;&#125;).Fail(err)) &#125;&#125;func (ba *BaseAPI) Resp(httpStatus int, res res.Response) &#123; ba.Data[&quot;json&quot;] &#x3D; res ba.Ctx.Output.SetStatus(httpStatus) ba.ServeJSON()&#125; &emsp;&emsp;同时，提供了一些异常处理： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package handlerimport (&quot;fmt&quot;)const ( &#x2F;&#x2F;公共错误码 0-100 ComErrorCodeSuccess &#x3D; 0 ComErrorCodeNoVistAuth &#x3D; 1 ComErrorCodeInvalidPara &#x3D; 2 ComErrorCodeUnknown &#x3D; 3)const ( badRequestType &#x3D; &quot;[bad request]&quot; innerErrorType &#x3D; &quot;[inner error]&quot; successType &#x3D; &quot;[success]&quot;)type ExceptionHandler struct &#123; Code int &#96;json:&quot;code&quot;&#96; ErrorType string &#96;json:&quot;-&quot;&#96; Msg string &#96;json:&quot;msg&quot;&#96;&#125;func (ec ExceptionHandler) Error() string &#123; return ec.Msg&#125;func NewBadReqError(code int, format string, paras ...interface&#123;&#125;) error &#123; return newError(code, badRequestType, format, paras...)&#125;func NewInnerError(code int, format string, paras ...interface&#123;&#125;) error &#123; return newError(code, innerErrorType, format, paras...)&#125;func NewSuccess(format string, paras ...interface&#123;&#125;) error &#123; return newError(ComErrorCodeSuccess, successType, format, paras...)&#125;func NewUnkownError(format string, paras ...interface&#123;&#125;) error &#123; return newError(ComErrorCodeUnknown, innerErrorType, format, paras...)&#125;func IsInnerError(err error) bool &#123; if errCustom, ok :&#x3D; err.(ExceptionHandler); ok &#123; return errCustom.ErrorType &#x3D;&#x3D; innerErrorType &#125; return false&#125;func IsBadRequestError(err error) bool &#123; if errCustom, ok :&#x3D; err.(ExceptionHandler); ok &#123; return errCustom.ErrorType &#x3D;&#x3D; badRequestType &#125; return false&#125;func IsSuccess(err error) bool &#123; if err &#x3D;&#x3D; nil &#123; return true &#125; if errCustom, ok :&#x3D; err.(*ExceptionHandler); ok &#123; return errCustom.ErrorType &#x3D;&#x3D; successType &#125; return false&#125;func NewError(code int, format string, paras ...interface&#123;&#125;) *ExceptionHandler &#123; return &amp;ExceptionHandler&#123;Code: code, Msg: fmt.Sprintf(format, paras...)&#125;&#125;func newError(code int, errorType string, format string, paras ...interface&#123;&#125;) error &#123; return &amp;ExceptionHandler&#123;Code: code, ErrorType: errorType, Msg: fmt.Sprintf(format, paras...)&#125;&#125; &emsp;&emsp;返回响应体： 12345678910111213141516171819202122232425262728293031323334353637package resimport ( &#x2F;&#x2F; http client driver &quot;github.com&#x2F;astaxie&#x2F;beego&#x2F;logs&quot; &quot;common-core&#x2F;common&#x2F;handler&quot;)type Response interface &#123; Succ(body interface&#123;&#125;) Response Fail(err error) Response&#125;type DefaultResponse struct &#123; Status handler.ExceptionHandler &#96;json:&quot;status&quot;&#96; Data interface&#123;&#125; &#96;json:&quot;data&quot;&#96;&#125;func (dr *DefaultResponse) Succ(body interface&#123;&#125;) Response &#123; success, _ :&#x3D; handler.NewSuccess(&quot;success&quot;).(*handler.ExceptionHandler) dr.Status &#x3D; *success dr.Data &#x3D; body return dr&#125;func (dr *DefaultResponse) Fail(err error) Response &#123; errCustom, ok :&#x3D; err.(*handler.ExceptionHandler) if !ok &#123; logs.Error(&quot;error type invalid, please use custom error&quot;) unkownError, _ :&#x3D; handler.NewUnkownError(err.Error()).(*handler.ExceptionHandler) dr.Status &#x3D; *unkownError &#125; else &#123; dr.Status &#x3D; *errCustom &#125; return dr&#125; &emsp;&emsp;到此，一个简单的微服务就写完了，如果需要借助一些缓存，或插件来实现一些功能，可以自行添加。 启动微服务，可以运行main函数，输出： 12021&#x2F;04&#x2F;13 16:51:38.785 [I] [app.go:214] http server Running on http:&#x2F;&#x2F;:8080 通过在浏览器输入请求地址： 123http:&#x2F;&#x2F;localhost:8080&#x2F;api&#x2F;v1&#x2F;healthzhttp:&#x2F;&#x2F;localhost:8080&#x2F;api&#x2F;v1&#x2F;pay&#x2F;232 如果增加鉴权认证，则会返回： 1&#123;&quot;status&quot;:&#123;&quot;code&quot;:1,&quot;msg&quot;:&quot;no token&quot;&#125;,&quot;data&quot;:null&#125; 7.3 部署微服务应用部署服务&emsp;&emsp;这里引入了部署框架，具体内容将在后续公开，现在先来看看如何部署cas-server： 安装教程 git clone https://gitee.com/damon_one/microservice-k8s.git kubectl create namespace system-server 在microservice-k8s目录下 sh install_requirement.sh cd build 修改对应的配置 vi ../deployment/quick-start/quick-start-AIO-example.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041default: cluster_id: singlebox gpu_version: 440.31 mysql_password: ssswswcluster_server: default: ssh-username: damon #机器用户名 ssh-password: wwwww #机器密码 gpu: type: debug count: 2 cpu: 24 mem: 187 master: - ip: 10.12.3.17 hostname: damon username: damon passworld: wwwww gpu: # required if type: debug count: 2# compute:# - ip: &lt;compute node ip&gt;# hostname: &lt;compute node hostname&gt;# gpu:# # the type and count must be configured at same time# type: &lt;gpu type that is different from default&gt;# count: &lt;gpu type that is different from default&gt;registry_info: # the registry address of docker,format [ip:port] eg. 10.10.8.100:5000 domain: 10.10.8.100:5000 # the registry&#39;s username username: admin # the registry&#39;s password password: wdwwdwwdw # the namespace of kubeletes k8s_namespace: singlebox_google_containers # the namespace of services leinao_namespace: hub sudo python ../controller.py config generate -i /home/damon/microservice-k8s/deployment/quick-start/quick-start-AIO-example.yaml -o /home/damon/deployment/output_config 按照config部署k8s(可不操作，默认已经部署k8s) sudo python ../controller.py cluster k8s-clean -p /home/damon/deployment/output_config sudo python ../controller.py cluster k8s-bootup -p /home/damon/deployment/output_config 把config配置push到远程,k8s重新部署时需要执行: sudo python ../controller.py config push -p /home/damon/deployment/output_configone 编译镜像： sudo python pai_build.py build -c /home/damon/deployment/output_config -s cas-server sudo python pai_build.py push -c /home/damon/deployment/output_config -i cas-server 部署服务：one sudo python ../controller.py service delete -n cas-server or mysql sudo python ../controller.py service start -n cas-server 测试部署后，我们可以执行K8s命令查看服务pod： 123456tom@PK001:~$ kubectl get po -n system-serverNAME READY STATUS RESTARTS AGEcas-server-deployment-6bdw56dwde-5pdwf 1&#x2F;1 Running 0 3dgateway-service-deployment-6b7856bc99-5pk56 1&#x2F;1 Running 0 5d3horder-service-76f57dbd7c-rpfls 1&#x2F;1 Running 0 10dpay-service-68bcc4db8-v42gm 1&#x2F;1 Running 0 10d 不同 NS 之间服务的互通： kube-system —&gt; system-server 12345678910111213tom@PK001:~&#x2F;damon$ kubectl exec -it order-service-76f57dbd7c-rpfls sh -n kube-system### curl http:&#x2F;&#x2F;pay-service-svc.system-server.svc.cluster.local:8090&#x2F;api&#x2F;v1&#x2F;healthz&#123; &quot;status&quot;: &#123; &quot;code&quot;: 0, &quot;msg&quot;: &quot;success&quot; &#125;, &quot;data&quot;: &quot;success&quot;&#125;### system-server —&gt; kube-system 12345678tom@PK001:~&#x2F;damon$ kubectl exec -it pay-service-768bcc4db8-v42gm sh -n system-server### curl http:&#x2F;&#x2F;order-service-svc.kube-system.svc.cluster.local:9300&#x2F;api&#x2F;v1&#x2F;orders&#123;&quot;status&quot;:&#123;&quot;code&quot;:1,&quot;msg&quot;:&quot;no token&quot;&#125;,&quot;data&quot;:null&#125;#### 当出现鉴权认证时，需要获取鉴权令牌Token： 授权码模式 获取授权码 页面打开url： 1localhost:2000&#x2F;oauth&#x2F;authorize?response_type&#x3D;code&amp;client_id&#x3D;order-service&amp;redirect_uri&#x3D;http:&#x2F;&#x2F;order-service&#x2F;login&amp;scope&#x3D;all 此时，页面被拦截到登录页： 在点击 Approve、Authorize 后，输入用户名、密码，跳转到上面的重定向地址，并带有 code 属性参数: 1http:&#x2F;&#x2F;order-service&#x2F;login?code&#x3D;nbkiUe 根据 code 获取 access_token 1curl -i -X POST -d &quot;grant_type&#x3D;authorization_code&amp;code&#x3D;nbkiUe&amp;client_id&#x3D;order-service&amp;client_secret&#x3D;order-service-123&amp;redirect_uri&#x3D;http:&#x2F;&#x2F;order-service&#x2F;login&quot; http:&#x2F;&#x2F;localhost:2000&#x2F;oauth&#x2F;token 返回信息: 1&#123;&quot;access_token&quot;:&quot;a2af3f0b-27da-41b8-90c0-3bd2a1ed0421&quot;,&quot;token_type&quot;:&quot;bearer&quot;,&quot;refresh_token&quot;:&quot;91c22287-aa24-4305-95cf-38f7903865f3&quot;,&quot;expires_in&quot;:3283,&quot;scope&quot;:&quot;all&quot;&#125; 拿到 token 获取用户信息 头部携带 1curl -i -H &quot;Accept: application&#x2F;json&quot; -H &quot;Authorization:bearer a2af3f0b-27da-41b8-90c0-3bd2a1ed0421&quot; -X GET http:&#x2F;&#x2F;localhost:2003&#x2F;api&#x2F;order&#x2F;getCurrentOrderUser 直接 get 方式传 token 123curl http:&#x2F;&#x2F;localhost:2003&#x2F;api&#x2F;order&#x2F;getCurrentOrderUser?access_token&#x3D;a2af3f0b-27da-41b8-90c0-3bd2a1ed0421curl -i -X POST http:&#x2F;&#x2F;localhost:2003&#x2F;api&#x2F;order&#x2F;***?access_token&#x3D;a2af3f0b-27da-41b8-90c0-3bd2a1ed0421 刷新token 通过上面的 “refresh_token” 来刷新获取新 token: 1curl -i -X POST -d &quot;grant_type&#x3D;refresh_token&amp;refresh_token&#x3D;91c22287-aa24-4305-95cf-38f7903865f3&amp;client_id&#x3D;provider-service&amp;client_secret&#x3D;provider-service-123&quot; http:&#x2F;&#x2F;localhost:2000&#x2F;oauth&#x2F;token 退出 1curl -i -H &quot;Accept: application&#x2F;json&quot; -H &quot;Authorization:bearer 01f1beaa-ae21-4920-a349-be786ba327e7&quot; -X DELETE http:&#x2F;&#x2F;localhost:2000&#x2F;api&#x2F;logout 密码模式密码模式由于都是通过直接调用服务来获取token，所以，咱们可以直接在容器内运行，看看效果： 1234curl -i -X POST -d &quot;username&#x3D;admin&amp;password&#x3D;123456&amp;grant_type&#x3D;password&amp;client_id&#x3D;order-service&amp;client_secret&#x3D;order-service-123&amp;scope&#x3D;all&quot; http:&#x2F;&#x2F;cas-server-service.system-server.svc.cluster.local:2000&#x2F;oauth&#x2F;tokenBasic模式认证：curl -i -X POST -H &quot;Authorization:Basic YWRtaW4td2ViOmFkbWluLXdlYi0xMjM&#x3D;&quot; -H &quot;Content-Type:application&#x2F;x-www-form-urlencoded&quot; -d &quot;username&#x3D;admin&amp;password&#x3D;123456&amp;grant_type&#x3D;password&amp;scope&#x3D;all&quot; http:&#x2F;&#x2F;cas-server-service.system-server.svc.cluster.local:2000&#x2F;oauth&#x2F;token 认证成功后，会返回如下结果: 1&#123;&quot;access_token&quot;:&quot;d2066f68-665b-4038-9dbe-5dd1035e75a0&quot;,&quot;token_type&quot;:&quot;bearer&quot;,&quot;refresh_token&quot;:&quot;44009836-731c-4e6a-9cc3-274ce3af8c6b&quot;,&quot;expires_in&quot;:3599,&quot;scope&quot;:&quot;all&quot;&#125; 接下来，我们通过 token 来访问接口: 123curl -i -H &quot;Accept: application&#x2F;json&quot; -H &quot;Authorization:bearer 9f7a31d6-8d8e-4ff5-a532-a334e630ebcf&quot; -X GET http:&#x2F;&#x2F;order-service-service.system-server.svc.cluster.local:2003&#x2F;api&#x2F;order&#x2F;auth&#x2F;admincurl -i -H &quot;Accept: application&#x2F;json&quot; -H &quot;Authorization:bearer cb5e8a95-0071-4c1e-b427-5fa58c0c7737&quot; -X GET http:&#x2F;&#x2F;pay-service-svc.system-server.svc.cluster.local:8090&#x2F;api&#x2F;v1&#x2F;pay&#x2F;wwwwcurl -i -H &quot;Accept: application&#x2F;json&quot; -H &quot;Authorization:bearer 9f7a31d6-8d8e-4ff5-a532-a334e630ebcf&quot; -X GET http:&#x2F;&#x2F;admin-web-service.system-server.svc.cluster.local:2001&#x2F;api&#x2F;user&#x2F;services 成功会返回结果: 1Has admin auth! token 如果失效，会返回: 1&#123;&quot;error&quot;:&quot;invalid_token&quot;,&quot;error_description&quot;:&quot;d2066f68-665b-4038-9dbe-5dd1035e75a01&quot;&#125; 这里主要通过Service来进行服务访问，实现基于K8s的负载均衡： 1234567tom@PK001:~$ kubectl get svc -n system-serverNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEgateway-service-service ClusterIP 20.16.1.5 &lt;none&gt; 5556&#x2F;TCP 74dpay-service-svc ClusterIP 20.16.63.79 &lt;none&gt; 8090&#x2F;TCP 10dadmin-web-service ClusterIP 10.16.129.24 &lt;none&gt; 2001&#x2F;TCP 84dcas-server-service ClusterIP 10.16.230.167 &lt;none&gt; 2000&#x2F;TCP 67dcloud-admin-service-service ClusterIP 10.16.25.178 &lt;none&gt; 1001&#x2F;TCP 190d 结束语云原生技术与微服务架构的天衣无缝&emsp;&emsp;云原生的微服务架构是云原生技术和微服务架构的完美结合。微服务作为一种架构风格，所解决的问题是交纵复杂的软件系统的架构与设计；云原生技术乃一种实现方式，所解决的问题是软件系统的运行、维护和治理。微服务架构可以选择不同的实现方式，如 Java 中的 Dubbo、Spring Cloud、Spring Cloud Alibaba，Golang 中的 Beego，Python 中的 Flask 等。但这些不同语言的服务之间的访问与运行可能存在一定得困难性与复杂性。但，云原生和微服务架构的结合，使得它们相得益彰。这其中的原因在于：云原生技术可以有效地弥补微服务架构所带来的实现上的复杂度；微服务架构难以落地的一个重要原因是它过于复杂，对开发团队的组织管理、技术水平和运维能力都提出了极高的要求。因此，一直以来只有少数技术实力雄厚的大企业会采用微服务架构。随着云原生技术的流行，在弥补了微服务架构的这一个短板之后，极大地降低了微服务架构实现的复杂度，使得广大的中小企业有能力在实践中应用微服务架构。云原生技术促进了微服务架构的推广，也是微服务架构落地的最佳搭配。 云原生时代的微服务的未来&emsp;&emsp;云原生的第一个发展趋势：标准化和规范化，该技术的基础是容器化和容器编排技术，最经常会用到的技术是 Kubernetes 和 Docker 等。随着云原生技术的发展，云原生技术的标准化和规范化工作正在不断推进，其目的是促进技术的发展和避免供应商锁定的问题，这对于整个云原生技术的生态系统是至关重要的。 &emsp;&emsp;云原生的第二个发展趋势：平台化，以服务网格技术为代表，这一趋势的出发点是增强云平台的能力，从而降低运维的复杂度。流量控制、身份认证和访问控制、性能指标数据收集、分布式服务追踪和集中式日志管理等功能，都可以由底层平台来提供，这就极大地降低了中小企业在运行和维护云原生应用时的复杂度，服务网格以 Istio 和 Linkerd 为开源代表。 &emsp;&emsp;云原生的第三个发展趋势：应用管理技术的进步，如在 Kubernetes 平台上部署和更新应用一直以来都比较复杂，传统的基于资源声明 YAML 文件的做法，已经逐步被 Helm 所替代。操作员模式在 Helm 的基础上更进一步，以更高效、自动化和可扩展的方式对应用部署进行管理。 开源项目&emsp;&emsp;实践项目开源：云原生微服务架构设计实战代码。 欢迎大家star、fork，欢迎联系我，一起学习。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"Service Mesh 是下一代微服务架构","date":"2021-03-30T07:09:35.000Z","path":"2021/03/30/ServiceMesh-01/","text":"Service Mesh 背后的诉求一种技术的出现必然是有各种推动的因素，Service Mesh 也一样，它的出现就得益于微服务架构的发展。那 Service Mesh 出现时，其背后的诉求是什么呢？ 1. 微服务架构的复杂性在微服务架构中，应用系统往往被拆分成很多个微服务（可以多达成百上千），数量庞大的微服务实例使得服务治理具有一定的挑战，比如说常见的服务注册、服务发现、服务实例的负载均衡，以及为了保护服务器实现熔断、重试等基础功能。除此之外，应用程序中还加上了大量的非功能性代码。 归根结底，在微服务架构中，微服务组件复杂、上手门槛比较高成了痛点问题。业务开发团队需要一定的学习周期才能上手微服务架构的开发，而人力资源的昂贵以及人员的流动性使得开发成本变高。业务开发团队更加擅长的是某一具体领域的业务，而不是技术的深度。应用系统的核心价值在于实现相应的业务，所以对于业务开发人员来说，微服务仅仅是手段，不是最终的目标。我们需要对业务开发人员“屏蔽”微服务的基础组件，使得微服务之间的通信对于业务开发人员透明。 为应对这个问题，有一些实践是利用 API 网关接收请求，网关作为代理处理外部服务的请求，并提供服务注册与发现、负载均衡、日志监控、容错等功能。然而，这种方案也存在不足，比如网关的单点故障、系统架构变得异常庞大；从功能来看，API 网关主要是面向用户，也就是说它可以解决从用户到各个后端服务的流量问题，至于其他问题，它可能就无能为力了。而我们需要的是一个完整的贯穿整个请求周期的方案，或者至少是一些能够与 API 网关互补的方案和工具。 2. 微服务本身的挑战微服务还有其自身引入的复杂度，有比学习微服务框架更艰巨的挑战，如微服务的划分、设计良好的声明式 API、单体旧应用的迁移，还涉及跨多个服务的数据一致性，这都会令大部分团队疲于应付。 除此之外，版本兼容性也是一个挑战。微服务框架很难一开始就完美无缺，在现实的软件工程中一般不存在这样完美无缺的框架，功能会分为多个里程碑迭代，发布之后就会有补丁修复……没有任何问题，这只是一种理想状态。业务服务中引入微服务的基础组件，这样业务服务的代码和微服务的 SDK 强耦合在一起，导致业务升级和微服务 SDK 的升级强绑定在了一起。如果客户端 SDK 和服务器端版本不一致，那就得谨慎对待客户端与服务端的兼容性问题。版本兼容性的处理非常复杂，特别是在服务端和客户端数量庞大的情况下，每对客户端和服务端的版本都有可能不同，这对于兼容性测试也会造成很大的压力。同时，对于异构的系统，还需要开发多语言的 SDK，维护成本很高。 3. 本质接下来我们探讨下业务服务最关心的是什么，比如写一个商品服务，对商品做增删改查的操作，你会发现基础设施、跨语言、兼容性和商品服务本身并没关系，而服务间的通讯才是最需要解决的问题。 比如，为了保证将客户端发出的业务请求发去一个正确的地方，需要用什么样的负载均衡？要不要做灰度？最终这些解决方案，都是让请求去访问正确的后端服务。整个过程当中，这个请求是从来不发生更改的。 既然在开发微服务的时候不用特别关心服务的通讯层，那是不是可以把微服务的技术栈向下移呢？ 微服务的早期先驱，如 Netflix、Twitter 等大型互联网公司，它们通过建立内部库的方式处理这些问题，然后提供给所有服务使用。但这种方法的问题在于这些库相对来说是比较“脆弱”的，很难保证它们可以适应所有的技术堆栈选择，且很难把库扩展到成百上千个微服务中。 为了应对上述的问题，Service Mesh 出现了， Service Mesh 通过独立进程的方式隔离微服务基础组件，对这个独立进程升级、运维要比传统的微服务方式简单得多。 什么是 Service MeshService Mesh（服务网格），最早在 2016 年 9 月，由开发 Linkerd 的 Buoyant 公司提出。2017 年，Linkerd 加入 CNCF，由 CNCF 托管孵化，Linkerd 是第一个加入 CNCF 的 Service Mesh 项目。Service Mesh 开始变得流行起来，特别是在技术社区，有人指出 Service Mesh 会是下一代的微服务架构基础。 关于 Service Mesh 的定义，目前比较认同的是 Buoyant 的 CEO William Morgan 在博客中给出的定义： 1Service Mesh 是用于处理服务到服务通信的专用基础架构层。云原生有着复杂的服务拓扑，它负责可靠的传递请求。实际上，Service Mesh 通常是作为一组轻量级网络代理实现，这些代理与应用程序代码部署在一起，应用程序无感知。 Service Mesh 模式的核心在于将客户端 SDK 剥离，以 Proxy 独立进程运行，目标是将原来存在于 SDK 中的各种能力下沉，为应用减负，以帮助应用云原生化。 Service Mesh 的第一代产品，如 Linkerd 1 和 Envoy，天然支持虚拟机。随着云原生的崛起，到了 Istio 和 Linkerd 2 ，不支持虚拟机。相比虚拟机，Kubernetes 提供了太多便利。 绝大部分Service Mesh 的实现都支持 Kubernetes，有些实现甚至只支持 Kubernetes。就这样，Service Mesh 逐步发展为一个独立的基础设施层。 在云原生架构下，应用系统可能由数百个微服务组成，微服务一般又是多实例部署，并且每一个实例都可能处于不断变化的状态，因为它们是由 Kubernetes 之类的资源调度系统动态调度。 Kubernetes 中的 Service Mesh 实现模式被命名为 Sidecar（边车模式，因为类似连接到摩托车的边车）。 在模式库中，Sidecar 模式的定义是：将应用程序的组件部署到单独的进程或容器中以提供隔离和封装。这种模式还可以使应用程序由异构组件和技术组成。 在 Sidecar 模式中，“边车”与父应用程序（即业务服务）是两个独立的进程，二者生命周期相同，同时被创建和退出。“边车”附加到业务服务，并为应用提供支持功能。 业务所有的流量都转发到 Service Mesh 的代理服务 Sidecar 中，Sidecar 承担了微服务框架基础的功能，包括服务注册与发现、负载均衡、熔断限流、认证鉴权、日志、监控和缓存加速等。不同的是，Service Mesh 强调的是通过独立进程的代理方式。总体来说，Service Mesh 帮助应用程序在复杂的软件架构和网络中建立稳定的通信机制。 Service Mesh 的开源组件近几年 Service Mesh 社区比较活跃，其对应的开源组件也很丰富，从最早的 Linkerd 到当前火热的 Istio、Envoy 等组件，下面我们就来重点介绍下这三个开源组件。 1. IstioIstio 由 Google、IBM 和 Lyft 合作开源，所以 Istio 自诞生之日起就备受瞩目。在 Istio 中，直接使用了 Lyft 公司的 Envoy 作为 Sidecar。2017 年 5 月 Istio 发布了 0.1 版本，现在已经发展到 1.9 版本。Istio 是 Service Mesh 的第二代产品，在刚开始发布时还曾计划提供对非 Kubernetes 的支持，发展到现在基本只支持 Kubernetes 上的使用，实质性取消了对虚拟机的支持。 Istio 功能十分丰富，包括： 流量管理：Istio 的基本功能，Istio 的流量路由规则使得你可以轻松控制服务之间的流量和 API 调用。 策略控制：应用策略并确保其得到执行，并且资源在消费者之间公平分配。 可观测性：通过自动链路追踪、监控和服务的日志，可以全面了解受监视服务如何与其他服务以及 Istio 组件本身进行交互。 安全认证：通过托管的身份验证，授权和服务之间通信的加密自动保护服务。Istio Security 提供了全面的安全解决方案来解决这些问题。 Istio 针对现有的服务网络，提供了一种简单的方式将连接、安全、控制和观测的模块，与应用程序或服务隔离开来，从而使开发人员可以将更多的精力放在核心的业务逻辑上。另外，Istio 直接基于成熟的 Envoy 代理进行构建，控制面组件则都是使用 Go 编写，在不侵入应用程序代码的前提下实现可视性与控制能力。总之，Istio 的设计理念是非常新颖前卫的。 2. Linkerd2016 年 1 月，前 Twitter 工程师 William Morgan 和 Oliver Gould 组建了一个名为 Buoyant 的公司，同时在 GitHub 上发布了 Linkerd 0.0.7 版本。Linkerd 由 Buoyant 推出，使用 Scala 语言实现，是业界第一个 Service Mesh。2017 年 1 月，Linkerd 加入 CNCF； 4 月，发布了 1.0 版本。 Linkerd 的架构由两部分组成：数据平面和控制平面。其中，数据平面由轻量级代理组成，它们作为 Sidecar 容器与服务代码的每个实例一起部署；控制平面是一组在专用 Kubernetes 命名空间中运行的服务（默认情况下）。这些服务承担聚合遥测数据、提供面向用户的 API、向数据平面代理提供控制数据等功能，它们共同驱动着数据平面的行为。 Linkerd 作为 Service Mesh 的先驱开源组件，在生产环境得到了大规模使用。Linkerd 2 的定位是 Kubernetes 的 Service Mesh，其提供了运行时调试、可观察性、可靠性和安全性，使得运行服务变得更容易、更安全，而无须更改代码。但是随着 Istio 的诞生，前景并不是特别乐观。 3. Envoy2016 年 9 月，Lyft 公司开源 Envoy ，并在 GitHub 上发布了 1.0.0 版本。Envoy 由 C++ 实现，性能和资源消耗上表现优秀。2017 年 9 月，Envoy 加入 CNCF，成为继 Linkerd 之后的第二个 Service Mesh 项目。Envoy 发展平稳，被 Istio 收编之后，Envoy 将自身定义为数据平面，并希望使用者可以通过控制平面来为 Envoy 提供动态配置。Envoy 用于云原生应用，为应用服务提供高性能分布式代理，以及作为大规模微服务架构的 Service Mesh 通信总线和通用数据平面。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"什么是服务网格（Service Mesh）","date":"2021-03-30T06:41:28.000Z","path":"2021/03/30/Service-Mesh/","text":"服务网格（Service Mesh）是随着 Kubernetes 和微服务架构的流行而出现的新技术，它的目的是解决微服务架构的服务之间相互调用时可能存在的各种问题。微服务架构的服务之间采用进程间的通讯方式进行交互，比如 REST 或 gRPC 等。在第 01 课时介绍微服务架构的时候，我提到过影响微服务架构复杂度的一个重要因素就是微服务之间的相互调用，这使得应用需要对服务调用时产生的错误进行处理。比如，当调用一个服务出现超时错误时，应该进行重试；如果对某个服务的调用在一段时间内频繁出错，说明该服务可能已经崩溃或是负载过大，没有必要再继续进行尝试下去了。 除了错误处理之外，我们还可能需要对服务之间的调用添加一些策略，比如限制服务被调用的速率，或是添加安全相关的访问控制规则等。这些需求从服务之间的调用而来，并且所有微服务架构的应用都有同样的需求，这些横切的需求，应该由平台或工具来处理，而不需要应用来实现，应用要做的只是提供相关的配置即可。 在 Kubernetes 出现之前，微服务架构已经在很多企业内部得到了应用。同样的，在服务网格之前也有相似的工具来解决服务调用相关的问题，比如 Netflix OSS 栈中的 Hystrix，但服务网格技术是在已有工具上的升级，它提供了一个更完整的解决方案。 严格说来，服务网格并不直接依赖 Kubernetes，但绝大部分服务网格实现都支持 Kubernetes，有些实现甚至只支持 Kubernetes。这是因为 Kubernetes 平台提供的功能可以简化服务网格的使用。下面我来为你介绍 Kubernetes 中的边车模式（Sidecar）。 边车模式在 Kubernetes 中，Pod 中的容器通常是紧密耦合的，它们共同完成应用的功能。如果需要实现横切功能，则需要在 Pod 中添加与应用无关的容器，这是因为横切功能的实现离不开对应用使用的存储和网络的访问，而 Pod 中的容器之间共享存储和网络。当我们把横切服务的容器添加到 Pod 中后，Pod中就多了与应用无关的容器，这种部署模式称为边车模式，这些容器被称为边车容器，下图是现实世界中的边车。 日志收集是边车模式的一个常见应用，它利用了 Pod 中容器共享存储的特性：应用容器往某个持久卷中写入日志，而日志收集工具的边车容器则监控同一个持久卷中的文件来读取日志。 边车容器在服务网格实现中至关重要。服务网格实现会在每个 Pod 上增加一个新的边车容器来作为其中应用服务的代理，这个容器的代理程序会作为外部调用者和实际服务提供者之间的桥梁。 如下图所示，Pod 某个端口上的请求，首先会被服务代理处理，然后再转发给实际的应用服务；同样的，应用服务对外的请求，也会先被服务代理处理，然后再转发给实际的接收者。代理边车容器的出现，为解决服务调用相关的问题提供了一种新的方案：服务调用的自动重试和断路器模式的实现，都可以由服务代理来完成，从而简化应用服务的实现。 如果仅从最基本的实现方式上来说，服务网格技术并不复杂。打个比方，如果一个 Pod 提供某个应用服务，只需要在该 Pod 中部署一个服务代理的边车容器，由该代理来处理应用容器发送和接收的数据，就实现了服务网格。 但是，服务网格实际上的解决方案非常复杂，我会在下面进行具体的介绍。 值得一提的是，边车模式并不是服务代理的唯一部署方式。有些服务网格实现可以在Kubernetes的节点上部署服务代理来处理该节点上的全部请求。 服务代理服务代理是服务网格技术实现的核心，可以说，服务代理决定了服务网格能力的上限。从作用上来说，服务代理与我们所熟悉的 Nginx 和 HAProxy 这类代理并没有太大区别。实际上， Nginx 和 HAProxy 同样可以作为服务代理来使用，但服务网格通常使用专门为服务间调用开发的服务代理实现。在下图所示的 OSI 七层模型中，服务代理一般工作在第 3/4 层和第 7 层。 下表列出了常见的服务代理，其中 Envoy、Traefix 和 Linkerd 2 都是新出现的服务代理实现。 服务发出和接收的所有调用都需要经过服务代理。服务代理的功能都与服务之间的调用相关，其主要方面如下表所示。 代理可以在请求层上工作。当服务 A 调用服务 B 时，服务 A 的代理可以使用负载均衡来动态选择实际调用的服务 B 实例，如果对服务 B 的调用失败，并且该调用是幂等的，则代理可以自动进行重试。服务 A 的代理还可以记录与调用相关的指标数据，服务 B 的代理可以根据访问控制的策略决定是否允许该请求，如果服务 B 当前所接收的请求过多，那么它的代理可以拒绝其中某些请求。 代理同样可以工作在连接层，服务 A 和服务 B 的代理之间可以建立 TLS 连接，并验证对方的身份。 由于服务代理需要处理服务所有接收和发送的请求，这对服务代理的性能要求很高，不能增加过长的延迟，这也是 Envoy 等服务代理流行的原因，这些新开发的服务代理对服务之间的调用进行了优化。除了性能之外，服务代理只占用很少的 CPU 和内存资源，这是因为每个服务实例的 Pod 上都可能运行着一个服务代理的容器，当服务数量增加时，服务代理自身的资源开销也会增加。 服务网格服务网格技术起源于 Linkerd 项目，从架构上来说，服务网格的实现很简单，它由服务代理和管理进程组成。服务代理称为服务网格的数据平面（Data Plane），负责拦截服务之间的调用并进行处理；管理进程称为服务网格的控制平面（Control Plane），负责协调代理并提供 API 来管理和监控服务网格。服务网格的能力由这两个平面的能力共同决定。 下图给出了服务网格的基本架构: 服务网格在数据平面的处理能力取决于所使用的服务代理，而服务网格实现通常使用已有的服务代理，因此它们在数据平面方面的能力差别并不大。服务网格实现的价值更多来源于它所提供的控制平面，比如，服务网格实现是否提供了 API 来更新配置，是否提供了图形化界面来查看服务状态，在 Kubernetes 上，是否可以使用自定义资源定义（Custom Resource Definition，CRD）来进行声明式配置。 服务网格技术的优势有以下几个方面: 它与服务实现使用的技术栈无关。服务代理工作在服务调用这个层次上。不论服务采用什么编程语言或框架来实现，服务代理都可以产生作用。Kubernetes 的流行，使得在微服务架构实现中使用多语言开发变得更简单。一个微服务应用的不同服务可以使用完全不同的技术栈来实现，这些服务之间的调用都可以由服务代理来处理。 服务网格技术与应用代码是解耦的，这意味着当我们需要对服务调用相关的策略进行调整时，并不需要修改应用的代码。以服务的访问频率为例，当需要控制对某个服务的调用频率时，可以通过服务网格的控制平面提供的 API 直接进行修改，并不需要对应用做任何改动。这种解耦使得服务网格成为应用运行平台所提供的能力之一，进而促成了新的开源项目和商业产品的出现。 对于大型项目，可以由专门的团队来负责管理服务网格的配置，进行更新和日常维护；对于小型项目，可以从开源社区选择合适的产品。 服务网格功能服务网格所能提供的功能非常多。每个服务网格实现所提供的功能也各有不同。下面我将对服务网格中的重要功能进行介绍。 自动代理注入为了使用服务网格提供的功能，应用服务的 Pod 需要添加服务代理容器，服务网格提供了自动的代理注入机制。在 Kubernetes 上，如果 Pod 或控制器对象中添加了某个特定的注解，则服务网格可以自动在 Pod 中添加服务代理容器并完成相关的配置。 流量管理 流量管理指的是管理服务之间的相互调用，由一系列的子功能组成。 服务发现 服务发现指的是发现系统中存在的服务及其对应的访问地址，服务网格会在内部维护一个注册表，包含所有发现的服务及其对应的服务端点。 负载均衡 每个服务通常都有多个运行的实例，在进行调用时，需要根据某些策略选择处理请求的实例。负载均衡的算法可以很简单，比如循环制（round robin）；也可以很复杂，比如根据被调用服务的各个实例的负载情况来动态选择。 流量控制 微服务架构的应用强调持续集成和持续部署，应用的每个服务都可以被单独部署。一个常见的需求是在进行更新时，让小部分用户使用新的版本，而大部分用户仍然使用当前的旧版本，这样的更新方式称为金丝雀部署（Canary Deployment）。为了支持这样的更新方式，我们可以同时部署服务的两个版本，并通过服务网格把调用请求分配到两个版本，比如，20% 的请求分配到新版本，剩下 80% 的请求分配到当前版本，经过一段时间的测试之后，再逐步把更多的请求分配到新版本，直到全部请求分配至新版本。 超时处理 服务网格对服务调用添加了超时处理机制。如果调用在设置的时间之后仍然没有返回，则会直接出错，这样就避免了在被调用的服务出现问题时，进行不必要的等待。不过，超时时间也不能设置得过短，否则会有大量相对耗时的调用产生不必要的错误，针对这一点，服务网格提供了基于配置的方式来调整服务的超时时间。 重试 当服务的调用出现错误时，服务网格可以选择进行重试，服务重试看似简单，但要正确的实现并不容易。简单的重试策略，比如固定时间间隔和最大重试次数的做法，很容易产生重试风暴（Retry Storm）。如果某些请求因为服务负载的原因而失败，简单的重试策略会在固定的时间间隔之后，重试全部失败请求，这些请求在重试时又会因为负载过大的原因而再次失败。所造成的结果就是产生大量失败的重试请求，影响整体的性能，有效的重试机制应该避免出现重试风暴。 断路器 断路器（Circuit Breaker）是微服务架构中的一种常见模式。通过断路器，可以在服务的每个实例上设置限制，比如同时允许的最大连接数量，或是调用失败的次数。当设定的限制达到时，断路器会自动断开，禁止对该实例的连接。 断路器的存在，使得服务调用可以快速失败，而不用尝试连接一个已经失败或过载的实例，所以它的一个重要作用是避免服务的级联失败。如果一个服务出现错误，可能导致它的调用者因为超时而积压很多未处理的请求，进而导致它的调用者也由于负载过大而崩溃，这样的级联效应，有可能导致整个应用的崩溃。使用断路器之后，出现错误的服务实例被自动隔离，不会影响系统中的其他服务。 错误注入 在使用服务网格配置了服务的错误处理策略之后，一个重要的需求是对这些策略进行测试。错误注入指的是往系统中引入错误来测试应用的故障恢复能力，比如，错误注入可以在服务调用时自动添加延迟，或是直接返回错误给调用者。 安全安全相关的功能解决应用的 3 个 A 需求，分别是认证（Authentication）、授权（Authorization）和审计（Audit）。这3个需求的英文名称都以字母A开头，所以称为3个A需求。 双向 TLS（mutual TLS，mTLS）指的是在服务调用者和被调用者的服务代理之间建立双向 TLS 连接，这个连接意味着客户端和服务器都需要认证对方的身份。通过 TLS 连接可以对通信进行加密，防止中间人攻击。 用户认证：服务网格应该可以和不同的用户认证服务进行集成，常用的认证方式包括 JWT 令牌认证，以及与 OpenID Connect 提供者进行集成。 访问策略访问策略用来描述服务调用时的策略。 访问速率控制：通过访问速率控制，可以限制服务的调用速度，防止服务因请求过多而崩溃。 服务访问控制：服务访问控制用来限制对服务的访问，限制的方式包括禁止服务、黑名单和白名单等。 可观察性服务网格可以收集与服务之间通信相关的遥测数据，这些数据使得运维人员可以观察服务的行为，发现服务可能存在的问题，并对服务进行优化。 性能指标：是指服务网格收集与服务调用相关的性能指标数据，包括延迟、访问量、错误和饱和度。除此之外，服务网格还收集与自身的控制平面相关的数据。 分布式追踪：可以查看单个请求在服务网格中的处理流程，在微服务架构中，应用接收到的请求可能由多个服务协同处理。在请求延迟过高时，需要查看请求在不同服务之间的调用流程，以及每个服务所带来的延迟。分布式追踪是服务网格提供的工具，可以用来收集相关的调用信息。 访问日志：用来记录每个服务实例所接收到的请求。 服务网格产品介绍Istio 项目由 Google、IBM 和 Lyft 共同发起。由于有大公司的支持，Istio 项目目前所提供的功能是最完备的，这也意味着 Istio 是最复杂的。Istio 所包含的组件非常多，对应的配置也非常复杂，它的学习曲线很陡，上手并不容易。值得一提的是，Lyft 的 Envoy 团队与 Istio 有很好的合作，这就保证了 Istio 有最好的 Envoy 支持。本专栏将使用 Istio 来作为服务网格的实现。 Linkerd 是最早的服务网格实现，目前作为 CNCF 的项目来开发。相对 Istio 而言，Linkerd 提供的功能较少，但是也更简单易用。对很多应用来说，Linkerd 所提供的功能已经足够好。 Maesh 是 Containous 提供的服务网格实现。Maesh 使用 Traefik 作为服务代理。相对于 Istio 和 Linkerd，Maesh 还是一个比较新的项目，需要更多的时间来考察。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"云产品年中大优惠","date":"2021-03-19T10:05:26.000Z","path":"2021/03/19/popularize-02/","text":"双十一阿里云全网最低：立即领取 阿里云产品大促：立即领取 专属折扣码： ============================================================================================================================= 【腾讯云】11.11 云上盛惠，云产品限时抢购，1核2G云服务器首年88元！立即秒杀 【腾讯云】境外1核2G服务器低至2折，半价续费券限量免费领取！立即秒杀 【腾讯云】新客户无门槛领取总价值高达2860元代金券，每种代金券限量500张，先到先得！立即秒杀 【腾讯云】热卖云产品3折起，云服务器、云数据库特惠，服务更稳，速度更快，价格更低！立即秒杀 【腾讯云】腾讯云数据库性能卓越稳定可靠，为您解决数据库运维难题立即秒杀 【腾讯云】云数据库MySQL基础版1元体验立即秒杀 腾讯云新客专属福利：立即秒杀 腾讯云十周年大促：立即秒杀 腾讯云服务器全球购：立即抢购 腾讯企业上云特惠活动：立即抢购 腾讯云产品三折：立即抢购 腾讯服务器X实时音视频 联合大促：立即抢购 腾讯云服务器购买页：立即抢购 腾讯云数据库购买页：立即抢购 腾讯云新客专属福利：立即抢购 腾讯云限时秒杀活动：立即抢购 腾讯云数据库Redis：立即抢购 ============================================================================================================================= 华为云新客专属福利：立即注册 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"备份Kubernetes的5个最佳实践","date":"2021-02-22T09:14:29.000Z","path":"2021/02/22/k8s-backup/","text":"备份应用程序和数据是组织经常需要处理的事情。尽管Kubernetes可以确保应用程序服务的高可用性和可伸缩性，但这些好处并不能有效地保护数据。因此，必须对Kubernetes应用程序进行数据管理和备份，并应将其纳入标准操作流程中。 但是，备份Kubernetes应用程序需要一种独特的方法，该方法与传统的备份解决方案大不相同。使用Kubernetes，经常会将应用程序部署在集群中跨节点的多个容器中，要备份应用程序以及数据和存储量，你需要考虑所有各种Kubernetes对象和配置数据，还必须适应应用程序快速的开发和部署周期，DevOps的“左移(shift-left)”理念，数据保护，安全要求等。 鉴于这些独特的要求，备份Kubernetes似乎是一项艰巨的任务，但是你可以采取一些步骤来简化该过程。以下是五个最佳做法： 1.考虑Kubernetes架构 一个典型的Kubernetes应用程序由数百个组件组成-Pod，服务(service)，证书，密钥(secret)等等。任何Kubernetes备份解决方案不仅要能够备份和还原数据，而且还要能够备份和还原所有这些组件。至关重要的是，备份解决方案要通过API自动与Kubernetes控制平面进行交互，以便不仅能够发现集群上运行的Kubernetes应用，而且还可以与基础计算，网络和存储基础架构集成。 存储也是一个重要的考虑因素，必须包含在备份计划中。与应用程序配置数据一样，Kubernetes存储（用于应用程序容器的持久卷）包含需要保护的重要业务数据。 最后，确定要备份存储的位置。你将其保留在本地s存储还是在云中？灵活性和易用性将成为任何数据备份存储的重要特征。 2.制定恢复计划 由于Kubernetes应用程序的分布式架构，还原数据需要很多步骤。例如，你需要验证集群依赖关系，创建新的Kubernetes视图的替代数据，并确定在何处启动恢复。然后，你需要标识备份数据源并准备目标存储。一旦计划了这些，就必须更新所有组件以创建新的存储资源。提前创建详细计划可以帮助你引导这个复杂的过程，幸运的是，有些Kubernetes备份解决方案可以自动为你执行此操作，你应该寻找一种支持此功能的解决方案。 但是可靠的执行计划仅仅是开始。你还应该确保你的备份平台可以将各个步骤转换为相关的Kubernetes API调用。这样可确保恢复功能所需的资源可用，并确保正确部署和配置了云原生应用程序的所有组件。 3.简化操作 如果备份需要编码，打包或部署，则开发人员可能会避免使用它们。他们的目标是快速开发和部署应用程序，而复杂的备份过程可能会阻碍其进展。 因此，备份应由API驱动，并且是无缝衔接的。确保你的解决方案具有针对应用程序而不是其单个组件的自动备份策略，并具有在部署新应用程序时检测和备份新应用程序的能力。最后，确保你的备份解决方案提供了简单的工作流程，并使你的运维团队能够顺畅地遵守任何法规和监控要求。 4.确保安全 与任何数据管理功能一样，安全性至关重要。执行Kubernetes备份时，要实施身份和访问管理以及基于角色的访问管理（RBAC）的控件，以确保只有授权的用户和组才能访问备份平台。这使你可以控制谁可以执行任务，例如监视和验证备份，执行还原等，并使你可以向开发人员授予从快照还原应用程序的权限。 你的解决方案应集成到云提供商的身份验证解决方案中，而无需任何其他工具或API。最后，请确保你的数据已加密-无论是在传输中还是在静止状态。 5.利用Kubernetes的可移植性 要利用Kubernetes的可移植性功能，你的备份解决方案应该能够兼容几种发行版和基础架构配置执行还原，并自动转换应用程序的备份版本以在新环境中运行。 备份解决方案要能够转换所有应用程序依赖项以与新环境兼容，这一点很重要。 Kubernetes原生备份是你的最佳选择 无论你的目标是保护Kubernetes应用程序免受数据丢失和损坏，为测试和开发目的备份数据，将应用程序迁移到新环境中，还是支持组织的灾难恢复计划，备份对于高效运维都是必不可少的。 使用传统解决方案而不是专门为Kubernetes环境设计的解决方案会增加意外数据丢失和配置错误的风险，并且无法提供保护应用程序数据所需的细粒度，可感知的应用程序备份和恢复功能。为了遵守Kubernetes环境中的备份和恢复最佳实践，Kubernetes原生备份解决方案是最佳方法。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 特别声明 原文作者：王延飞 本文原链：http://mtw.so/5GiprA 本文转载如有侵权，请联系站长删除，谢谢 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"Spring Cloud 与 K8s 在微服务层面的不同","date":"2021-02-04T02:18:16.000Z","path":"2021/02/04/spring-cloud-k8s/","text":"Spring Boot 1.x 与 2.x 的区别在《微服务 Spring Cloud 架构设计》一文中，笔者讲过 Spring Cloud 的架构设计。其实 Spring Boot 在一开始时，运用到的基本就是 Eureka、Config、Zuul、Ribbon、Feign、Hystrix 等。到了 Spring Boot 2.x 的时候，大量的组件开始风云崛起。下面简单列下这两个版本之间的区别如下。 Spring Boot 1.x 中，session 的超时时间是这样的： 1server.session.timeout&#x3D;3600 而在 2.x 中： 1server.servlet.session.timeout&#x3D;PT120M 截然不同的写法，cookie 也是一样的： 123456server: servlet: session: timeout: PT120M cookie: name: ORDER-SERVICE-SESSIONID 应用的 ContextPath 配置属性改动，跟上面的 session 一样，加上了一个 servlet。 Spring Boot 2.x 基于 Spring 5，而 Spring Boot 1.x 基于 Spring 4 或较低。 统一错误处理的基类 AbstarctErrorController 的改动。 配置文件的中文可以直接读取，不需要转码。 Acutator 变化很大，默认情况不再启用所有监控，需要定制化编写监控信息，完全需要重写，HealthIndicator,EndPoint 同理。 从 Spring Boot 2.x 开始，可以与 K8s 结合来实现服务的配置管理、负载均衡等，这是与 1.x 所不同的。 K8s 的一些资源的介绍上面说到 Spring Boot 2.x 可以结合 K8s 来作为微服务的架构设计，那么就先来说下 K8s 的一些组件吧。 ConfigMap，看到这个名字可以理解：它是用于保存配置信息的键值对，可以用来保存单个属性，也可以保存配置文件。对于一些非敏感的信息，比如应用的配置信息，则可以使用 ConfigMap。 创建一个 ConfigMap 有多种方式如下。 1. key-value 字符串创建 1kubectl create configmap test-config --from-literal&#x3D;baseDir&#x3D;&#x2F;usr 上面的命令创建了一个名为 test-config，拥有一条 key 为 baseDir，value 为 “/usr” 的键值对数据。 2. 根据 yml 描述文件创建 123456apiVersion: v1kind: ConfigMapmetadata: name: test-configdata: baseDir: &#x2F;usr 也可以这样，创建一个 yml 文件，选择不同的环境配置不同的信息： 123456789101112131415161718192021kind: ConfigMapapiVersion: v1metadata: name: cas-serverdata: application.yaml: |- greeting: message: Say Hello to the World --- spring: profiles: dev greeting: message: Say Hello to the Dev spring: profiles: test greeting: message: Say Hello to the Test spring: profiles: prod greeting: message: Say Hello to the Prod 注意点： ConfigMap 必须在 Pod 使用其之前创建。 Pod 只能使用同一个命名空间的 ConfigMap。 当然，还有其他更多用途，具体可以参考官网。 Service，顾名思义是一个服务，什么样的服务呢？它是定义了一个服务的多种 pod 的逻辑合集以及一种访问 pod 的策略。 service 的类型有四种： ExternalName：创建一个 DNS 别名指向 service name，这样可以防止 service name 发生变化，但需要配合 DNS 插件使用。 ClusterIP：默认的类型，用于为集群内 Pod 访问时，提供的固定访问地址,默认是自动分配地址,可使用 ClusterIP 关键字指定固定 IP。 NodePort：基于 ClusterIp，用于为集群外部访问 Service 后面 Pod 提供访问接入端口。 LoadBalancer：它是基于 NodePort。 如何使用 K8s 来实现服务注册与发现从上面讲的 Service，我们可以看到一种场景：所有的微服务在一个局域网内，或者说在一个 K8s 集群下，那么可以通过 Service 用于集群内 Pod 的访问，这就是 Service 默认的一种类型 ClusterIP，ClusterIP 这种的默认会自动分配地址。 那么问题来了，既然可以通过上面的 ClusterIp 来实现集群内部的服务访问，那么如何注册服务呢？其实 K8s 并没有引入任何的注册中心，使用的就是 K8s 的 kube-dns 组件。然后 K8s 将 Service 的名称当做域名注册到 kube-dns 中，通过 Service 的名称就可以访问其提供的服务。那么问题又来了，如果一个服务的 pod 对应有多个，那么如何实现 LB？其实，最终通过 kube-proxy，实现负载均衡。 说到这，我们来看下 Service 的服务发现与负载均衡的策略，Service 负载分发策略有两种： RoundRobin：轮询模式，即轮询将请求转发到后端的各个 pod 上，其为默认模式。 SessionAffinity：基于客户端 IP 地址进行会话保持的模式，类似 IP Hash 的方式，来实现服务的负载均衡。 其实，K8s 利用其 Service 实现服务的发现，其实说白了，就是通过域名进行层层解析，最后解析到容器内部的 ip 和 port 来找到对应的服务，以完成请求。 下面写一个很简单的例子： 123456789101112apiVersion: v1kind: Servicemetadata: name: cas-server-service namespace: defaultspec: ports: - name: cas-server01 port: 2000 targetPort: cas-server01 selector: app: cas-server 可以看到执行 kubectl apply -f service.yaml 后： 12345root@ubuntu:~$ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEadmin-web-service ClusterIP 10.16.129.24 &lt;none&gt; 2001&#x2F;TCP 84dcas-server-service ClusterIP 10.16.230.167 &lt;none&gt; 2000&#x2F;TCP 67dcloud-admin-service-service ClusterIP 10.16.25.178 &lt;none&gt; 1001&#x2F;TCP 190d 这样，我们可以看到默认的类型是 ClusterIP，用于为集群内 Pod 访问时，可以先通过域名来解析到多个服务地址信息，然后再通过 LB 策略来选择其中一个作为请求的对象。 K8s 如何来处理微服务中常用的配置在上面，我们讲过了几种创建 ConfigMap 的方式，其中有一种在 Java 中常常用到：通过创建 yml 文件来实现配置管理。 比如： 123456789101112131415161718192021kind: ConfigMapapiVersion: v1metadata: name: cas-serverdata: application.yaml: |- greeting: message: Say Hello to the World --- spring: profiles: dev greeting: message: Say Hello to the Dev spring: profiles: test greeting: message: Say Hello to the Test spring: profiles: prod greeting: message: Say Hello to the Prod 上面创建了一个 yml 文件，同时，通过 spring.profiles 指定了开发、测试、生产等每种环境的配置。 具体代码： 12345678910111213141516171819202122232425262728293031323334353637383940apiVersion: apps&#x2F;v1kind: Deploymentmetadata: name: cas-server-deployment labels: app: cas-serverspec: replicas: 1 selector: matchLabels: app: cas-server template: metadata: labels: app: cas-server spec: nodeSelector: cas-server: &quot;true&quot; containers: - name: cas-server image: &#123;&#123; cluster_cfg[&#39;cluster&#39;][&#39;docker-registry&#39;][&#39;prefix&#39;] &#125;&#125;cas-server imagePullPolicy: Always ports: - name: cas-server01 containerPort: 2000 volumeMounts: - mountPath: &#x2F;home&#x2F;cas-server name: cas-server-path args: [&quot;sh&quot;, &quot;-c&quot;, &quot;nohup java $JAVA_OPTS -jar -XX:MetaspaceSize&#x3D;128m -XX:MaxMetaspaceSize&#x3D;128m -Xms1024m -Xmx1024m -Xmn256m -Xss256k -XX:SurvivorRatio&#x3D;8 -XX:+UseConcMarkSweepGC cas-server.jar --spring.profiles.active&#x3D;dev&quot;, &quot;&amp;&quot;] hostAliases: - ip: &quot;127.0.0.1&quot; hostnames: - &quot;gemantic.localhost&quot; - ip: &quot;0.0.0.0&quot; hostnames: - &quot;gemantic.all&quot; volumes: - name: cas-server-path hostPath: path: &#x2F;var&#x2F;pai&#x2F;cas-server 这样，当我们启动容器时，通过 --spring.profiles.active=dev 来指定当前容器的活跃环境，即可获取 ConfigMap 中对应的配置。是不是感觉跟 Java 中的 Config 配置多个环境的配置有点类似呢？但是，我们不用那么复杂，这些统统可以交给 K8s 来处理。只需要你启动这一命令即可，是不是很简单？ Spring Boot 2.x 的新特性在第一节中，我们就讲到 1.x 与 2.x 的区别，其中最为凸显的是，Spring Boot 2.x 结合了 K8s 来实现微服务的架构设计。其实，在 K8s 中，更新 ConfigMap 后，pod 是不会自动刷新 configMap 中的变更，如果想要获取 ConfigMap 中最新的信息，需要重启 pod。 但 2.x 提供了自动刷新的功能： 123456789101112131415spring: application: name: cas-server cloud: kubernetes: config: sources: - name: $&#123;spring.application.name&#125; namespace: default discovery: all-namespaces: true reload: enabled: true mode: polling period: 500 如上，我们打开了自动更新配置的开关，并且设置了自动更新的方式为主动拉取，时间间隔为 500ms，同时，还提供了另外一种方式——event 事件通知模式。这样，在 ConfigMap 发生改变时，无需重启 pod 即可获取最新的数据信息。 同时，Spring Boot 2.x 结合了 K8s 来实现微服务的服务注册与发现： 123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-kubernetes-core&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-kubernetes-discovery&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt; 开启服务发现功能： 12345spring: cloud: kubernetes: discovery: all-namespaces: true 开启后，我们在《微服务 Spring Cloud 架构设计》一文中讲过，其实最终是向 K8s 的 API Server 发起 http 请求，获取 Service 资源的数据列表。然后根据底层的负载均衡策略来实现服务的发现，最终解析到某个 pod 上。那么为了同一服务的多个 pod 存在，我们需要执行： 1kubectl scale --replicas&#x3D;2 deployment admin-web-deployment 同时，我们如果通过 HTTP 的 RestTemplate Client 来作服务请求时，可以配置一些请求的策略，RestTemplate 一般与 Ribbon 结合使用： 12345678910111213141516171819202122232425client: http: request: connectTimeout: 8000 readTimeout: 3000backend: ribbon: eureka: enabled: false client: enabled: true ServerListRefreshInterval: 5000ribbon: ConnectTimeout: 8000 ReadTimeout: 3000 eager-load: enabled: true clients: cas-server-service,admin-web-service MaxAutoRetries: 1 #对第一次请求的服务的重试次数 MaxAutoRetriesNextServer: 1 #要重试的下一个服务的最大数量（不包括第一个服务） #ServerListRefreshInterval: 2000 OkToRetryOnAllOperations: true NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RoundRobinRule #com.damon.config.RibbonConfiguration #分布式负载均衡策略 可以配置一些服务列表，自定义一些负载均衡的策略。 如果你是使用 Feign 来作为 LB，其实与 Ribbon 只有一点点不一样，因为 Feign 本身是基于 Ribbon 来实现的，除了加上注解 @EnableFeignClients 后，还要配置： 1234567feign: client: config: default: #provider-service connectTimeout: 8000 #客户端连接超时时间 readTimeout: 3000 #客户端读超时设置 loggerLevel: full 其他的可以自定义负载均衡策略，这一点是基于 Ribbon 的，所以是一样的。 实战 Spring Boot 2.x 结合 K8s 来实现微服务架构设计微服务架构中，主要的就是服务消费者、服务的生产者可以互通，可以发生调用，在这基础上，还可以实现负载均衡，即一个服务调用另一个服务时，在该服务存在多个节点的情况下，可以通过一些策略来找到该服务的一个合适的节点访问。下面主要介绍服务的生产者与消费者。 先看生产者，引入常用的依赖： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;&#x2F;artifactId&gt; &lt;version&gt;2.1.13.RELEASE&lt;&#x2F;version&gt; &lt;relativePath&#x2F;&gt; &lt;&#x2F;parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;&#x2F;project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;&#x2F;java.version&gt; &lt;swagger.version&gt;2.6.1&lt;&#x2F;swagger.version&gt; &lt;xstream.version&gt;1.4.7&lt;&#x2F;xstream.version&gt; &lt;pageHelper.version&gt;4.1.6&lt;&#x2F;pageHelper.version&gt; &lt;fastjson.version&gt;1.2.51&lt;&#x2F;fastjson.version&gt; &lt;springcloud.version&gt;Greenwich.SR3&lt;&#x2F;springcloud.version&gt; &lt;springcloud.kubernetes.version&gt;1.1.1.RELEASE&lt;&#x2F;springcloud.kubernetes.version&gt; &lt;mysql.version&gt;5.1.46&lt;&#x2F;mysql.version&gt; &lt;&#x2F;properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;springcloud.version&#125;&lt;&#x2F;version&gt; &lt;type&gt;pom&lt;&#x2F;type&gt; &lt;scope&gt;import&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; &lt;&#x2F;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;&#x2F;artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;&#x2F;artifactId&gt; &lt;&#x2F;exclusion&gt; &lt;&#x2F;exclusions&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-undertow&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;!-- 配置加载依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes-config&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;&#x2F;artifactId&gt; &lt;scope&gt;test&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;&#x2F;groupId&gt; &lt;artifactId&gt;jjwt&lt;&#x2F;artifactId&gt; &lt;version&gt;0.9.0&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;cn.hutool&lt;&#x2F;groupId&gt; &lt;artifactId&gt;hutool-all&lt;&#x2F;artifactId&gt; &lt;version&gt;4.6.3&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;&#x2F;groupId&gt; &lt;artifactId&gt;guava&lt;&#x2F;artifactId&gt; &lt;version&gt;19.0&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;&#x2F;groupId&gt; &lt;artifactId&gt;commons-lang3&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-collections&lt;&#x2F;groupId&gt; &lt;artifactId&gt;commons-collections&lt;&#x2F;artifactId&gt; &lt;version&gt;3.2.2&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;&#x2F;groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;swagger.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;&#x2F;groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;swagger.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!-- 数据库分页依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;&#x2F;groupId&gt; &lt;artifactId&gt;pagehelper&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;pageHelper.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;&#x2F;artifactId&gt; &lt;version&gt;1.1.1&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!-- 数据库驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt; &lt;artifactId&gt;druid&lt;&#x2F;artifactId&gt; &lt;version&gt;1.1.3&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt; &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;fastjson.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jsoup&lt;&#x2F;groupId&gt; &lt;artifactId&gt;jsoup&lt;&#x2F;artifactId&gt; &lt;version&gt;1.11.3&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; 上面我们使用了比较新的版本：Spring Boot 2.1.13，Cloud 版本是 Greenwich.SR3，其次，我们配置了 K8s 的 ConfigMap 所用的依赖，加上了数据库的一些配置，具体其他的，实现过程中，大家可以自行添加。 接下来，我们看启动时加载的配置文件，这里加了关于 K8s ConfigMap 所管理的配置所在的信息，以及保证服务被发现，开启了所有的 namespace，同时还启动了配置自动刷新的功能，注意的是，该配置需要在 bootstrap 文件： 123456789101112131415161718spring: application: name: cas-server cloud: kubernetes: config: sources: - name: $&#123;spring.application.name&#125; namespace: default discovery: all-namespaces: true #发现所有的命令空间的服务 reload: enabled: true mode: polling #自动刷新模式为拉取模式，也可以是事件模式 event period: 500 #拉取模式下的频率logging: #日志路径设置 path: &#x2F;data&#x2F;$&#123;spring.application.name&#125;&#x2F;logs 剩下的一些配置可以在 application 文件中配置： 1234567891011121314151617181920212223spring: profiles: active: devserver: port: 2000 undertow: accesslog: enabled: false pattern: combined servlet: session: timeout: PT120M #session 超时时间client: http: request: connectTimeout: 8000 readTimeout: 30000mybatis: #持久层配置 mapperLocations: classpath:mapper&#x2F;*.xml typeAliasesPackage: com.damon.*.model 接下来看下启动类： 12345678910111213141516&#x2F;** * * @author Damon * @date 2020 年 1 月 13 日 下午 8:29:42 * *&#x2F;@Configuration@EnableAutoConfiguration@ComponentScan(basePackages &#x3D; &#123;&quot;com.damon&quot;&#125;)&#x2F;&#x2F;@SpringBootApplication(scanBasePackages &#x3D; &#123; &quot;com.damon&quot; &#125;)@EnableConfigurationProperties(EnvConfig.class)public class CasApp &#123; public static void main(String[] args) &#123; SpringApplication.run(CasApp.class, args); &#125;&#125; 这里我们没有直接用注解 @SpringBootApplication，因为主要用到的就是几个配置，没必要全部加载。 我们看到启动类中有一个引入的 EnvConfig.class： 12345678910111213141516171819&#x2F;** * @author Damon * @date 2019 年 10 月 25 日 下午 8:54:01 * *&#x2F;@Configuration@ConfigurationProperties(prefix &#x3D; &quot;greeting&quot;)public class EnvConfig &#123; private String message &#x3D; &quot;This is a dummy message&quot;; public String getMessage() &#123; return this.message; &#125; public void setMessage(String message) &#123; this.message &#x3D; message; &#125; 这就是配置 ConfigMap 中的属性的类。剩下的可以自己定义一个接口类，来实现服务生产者。 最后，我们需要在 K8s 下部署的话，需要准备几个脚本。 1. 创建 ConfigMap 123456789101112131415161718192021kind: ConfigMapapiVersion: v1metadata: name: cas-serverdata: application.yaml: |- greeting: message: Say Hello to the World --- spring: profiles: dev greeting: message: Say Hello to the Dev spring: profiles: test greeting: message: Say Hello to the Test spring: profiles: prod greeting: message: Say Hello to the Prod 设置了不同环境的配置，注意，这里的 namespace 需要与服务部署的 namespace 一致，这里默认的是 default，而且在创建服务之前，先得创建这个。 2. 创建服务部署脚本 12345678910111213141516171819202122232425262728293031323334353637383940414243apiVersion: apps&#x2F;v1kind: Deploymentmetadata: name: cas-server-deployment labels: app: cas-serverspec: replicas: 3 selector: matchLabels: app: cas-server template: metadata: labels: app: cas-server spec: nodeSelector: cas-server: &quot;true&quot; containers: - name: cas-server image: cas-server imagePullPolicy: Always ports: - name: cas-server01 containerPort: 2000 volumeMounts: - mountPath: &#x2F;home&#x2F;cas-server name: cas-server-path - mountPath: &#x2F;data&#x2F;cas-server name: cas-server-log-path - mountPath: &#x2F;etc&#x2F;kubernetes name: kube-config-path args: [&quot;sh&quot;, &quot;-c&quot;, &quot;nohup java $JAVA_OPTS -jar -XX:MetaspaceSize&#x3D;128m -XX:MaxMetaspaceSize&#x3D;128m -Xms1024m -Xmx1024m -Xmn256m -Xss256k -XX:SurvivorRatio&#x3D;8 -XX:+UseConcMarkSweepGC cas-server.jar --spring.profiles.active&#x3D;dev&quot;, &quot;&amp;&quot;] volumes: - name: cas-server-path hostPath: path: &#x2F;var&#x2F;pai&#x2F;cas-server - name: cas-server-log-path hostPath: path: &#x2F;data&#x2F;cas-server - name: kube-config-path hostPath: path: &#x2F;etc&#x2F;kubernetes 注意：这里有个属性 replicas，其作用是当前 pod 所启动的副本数，即我们常说的启动的节点个数，当然，你也可以通过前面讲的脚本来执行生成多个 pod 副本。如果这里没有设置多个的话，也可以通过命令来执行： 1kubectl scale --replicas&#x3D;3 deployment cas-server-deployment 这里，我建议使用 Deployment 类型的来创建 pod，因为 Deployment 类型更好的支持弹性伸缩与滚动更新。 同时，我们通过 --spring.profiles.active=dev 来指定当前 pod 的运行环境。 3. 创建一个 Service 最后，如果服务想被发现，需要创建一个 Service： 123456789101112apiVersion: v1kind: Servicemetadata: name: cas-server-service namespace: defaultspec: ports: - name: cas-server01 port: 2000 targetPort: cas-server01 selector: app: cas-server 注意，这里的 namespace 需要与服务部署的 namespace 一致，这里默认的是 default。 看看服务的消费者，同样，先看引入常用的依赖： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;&#x2F;artifactId&gt; &lt;version&gt;2.1.13.RELEASE&lt;&#x2F;version&gt; &lt;relativePath&#x2F;&gt; &lt;&#x2F;parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;&#x2F;project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;&#x2F;java.version&gt; &lt;swagger.version&gt;2.6.1&lt;&#x2F;swagger.version&gt; &lt;xstream.version&gt;1.4.7&lt;&#x2F;xstream.version&gt; &lt;pageHelper.version&gt;4.1.6&lt;&#x2F;pageHelper.version&gt; &lt;fastjson.version&gt;1.2.51&lt;&#x2F;fastjson.version&gt; &lt;springcloud.version&gt;Greenwich.SR3&lt;&#x2F;springcloud.version&gt; &lt;!-- &lt;springcloud.version&gt;2.1.8.RELEASE&lt;&#x2F;springcloud.version&gt; --&gt; &lt;springcloud.kubernetes.version&gt;1.1.1.RELEASE&lt;&#x2F;springcloud.kubernetes.version&gt; &lt;mysql.version&gt;5.1.46&lt;&#x2F;mysql.version&gt; &lt;&#x2F;properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;springcloud.version&#125;&lt;&#x2F;version&gt; &lt;type&gt;pom&lt;&#x2F;type&gt; &lt;scope&gt;import&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; &lt;&#x2F;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;&#x2F;artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;&#x2F;artifactId&gt; &lt;&#x2F;exclusion&gt; &lt;&#x2F;exclusions&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-undertow&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;&#x2F;artifactId&gt; &lt;scope&gt;test&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;!-- 配置加载依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes-config&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-commons&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;!-- 结合 k8s 实现服务发现 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-kubernetes-core&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-kubernetes-discovery&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;!-- 负载均衡策略 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes-ribbon&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;!-- 熔断机制 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;cn.hutool&lt;&#x2F;groupId&gt; &lt;artifactId&gt;hutool-all&lt;&#x2F;artifactId&gt; &lt;version&gt;4.6.3&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt; &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;fastjson.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jsoup&lt;&#x2F;groupId&gt; &lt;artifactId&gt;jsoup&lt;&#x2F;artifactId&gt; &lt;version&gt;1.11.3&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;&#x2F;groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;swagger.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;&#x2F;groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;swagger.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;&#x2F;groupId&gt; &lt;artifactId&gt;commons-lang3&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-collections&lt;&#x2F;groupId&gt; &lt;artifactId&gt;commons-collections&lt;&#x2F;artifactId&gt; &lt;version&gt;3.2.2&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!-- 数据库分页 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;&#x2F;groupId&gt; &lt;artifactId&gt;pagehelper&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;pageHelper.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;&#x2F;artifactId&gt; &lt;version&gt;1.1.1&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!-- 数据库驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt; &lt;artifactId&gt;druid&lt;&#x2F;artifactId&gt; &lt;version&gt;1.1.3&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; 这里大部分的依赖跟生产者一样，但，需要加入服务发现的依赖，以及所用的负载均衡的策略依赖、服务的熔断机制。 接下来 bootstrap 文件中的配置跟生产者一样，这里不在说了，唯一不同的是 application 文件： 1234567891011121314151617181920212223242526272829backend: ribbon: eureka: enabled: false client: enabled: true ServerListRefreshInterval: 5000ribbon: ConnectTimeout: 3000 ReadTimeout: 1000 eager-load: enabled: true clients: cas-server-service,edge-cas-service,admin-web-service #负载均衡发现的服务列表 MaxAutoRetries: 1 #对第一次请求的服务的重试次数 MaxAutoRetriesNextServer: 1 #要重试的下一个服务的最大数量（不包括第一个服务） OkToRetryOnAllOperations: true NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RoundRobinRule #负载均衡策略hystrix: command: BackendCall: execution: isolation: thread: timeoutInMilliseconds: 5000 #熔断机制设置的超时时间 threadpool: BackendCallThread: coreSize: 5 引入了负载均衡的机制以及策略（可以自定义策略）。 接下来看启动类： 123456789101112131415161718&#x2F;** * @author Damon * @date 2020 年 1 月 13 日 下午 9:23:06 * *&#x2F;@Configuration@EnableAutoConfiguration@ComponentScan(basePackages &#x3D; &#123;&quot;com.damon&quot;&#125;)@EnableConfigurationProperties(EnvConfig.class)@EnableDiscoveryClientpublic class AdminApp &#123; public static void main(String[] args) &#123; SpringApplication.run(AdminApp.class, args); &#125;&#125; 同样的 EnvConfig 类，这里不再展示了。其他的比如：注解 @EnableDiscoveryClient 是为了服务发现。 同样，我们新建接口，假如我们生产者有一个接口是： 1http:&#x2F;&#x2F;cas-server-service&#x2F;api&#x2F;getUser 则，我们在调用它时，可以通过 RestTemplate Client 来直接调用，通过 Ribbon 来实现负载均衡： 123456789@LoadBalanced @Bean public RestTemplate restTemplate() &#123; SimpleClientHttpRequestFactory requestFactory &#x3D; new SimpleClientHttpRequestFactory(); requestFactory.setReadTimeout(env.getProperty(&quot;client.http.request.readTimeout&quot;, Integer.class, 15000)); requestFactory.setConnectTimeout(env.getProperty(&quot;client.http.request.connectTimeout&quot;, Integer.class, 3000)); RestTemplate rt &#x3D; new RestTemplate(requestFactory); return rt; &#125; 可以看到，这种方式的分布式负载均衡实现起来很简单，直接注入一个初始化 Bean，加上一个注解 @LoadBalanced 即可。 在实现类中，我们只要直接调用服务生产者： 1ResponseEntity&lt;String&gt; forEntity &#x3D; restTemplate.getForEntity(&quot;http:&#x2F;&#x2F;cas-server&#x2F;api&#x2F;getUser&quot;, String.class); 其中，URL 中 必须要加上 &quot;http://&quot;，这样即可实现服务的发现以及负载均衡，其中，LB 的策略，可以采用 Ribbon 的几种方式，也可以自定义一种。 最后，可以在实现类上加一个熔断机制： 12345678910@HystrixCommand(fallbackMethod &#x3D; &quot;admin_service_fallBack&quot;)public Response&lt;Object&gt; getUserInfo(HttpServletRequest req, HttpServletResponse res) &#123; ResponseEntity&lt;String&gt; forEntity &#x3D; restTemplate.getForEntity(envConfig.getCas_server_url() + &quot;&#x2F;api&#x2F;getUser&quot;, String.class); logger.info(&quot;test restTemplate.getForEntity(): &#123;&#125;&quot;, forEntity); if (forEntity.getStatusCodeValue() &#x3D;&#x3D; 200) &#123; logger.info(&quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;test restTemplate.getForEntity(): &#123;&#125;&quot;, JSON.toJSON(forEntity.getBody())); logger.info(JSON.toJSONString(forEntity.getBody())); &#125;&#125; 其中发生熔断时，回调方法： 12345private Response&lt;Object&gt; admin_service_fallBack(HttpServletRequest req, HttpServletResponse res) &#123; String token &#x3D; StrUtil.subAfter(req.getHeader(&quot;Authorization&quot;), &quot;bearer &quot;, false); logger.info(&quot;admin_service_fallBack token: &#123;&#125;&quot;, token); return Response.ok(200, -5, &quot;服务挂啦!&quot;, null); &#125; 其返回的对象必须与原函数一致，否则可能会报错。具体的可以参考《Spring cloud 之熔断机制》。 最后与生产者一样，需要创建 ConfigMap、Service、服务部署脚本，下面会开源这些代码，这里也就不一一展示了。最后，我们会发现：当请求 认证中心时，认证中心存在的多个 pod，可以被轮训的请求到。这就是基于 Ribbon 的轮训策略来实现分布式的负载均衡，并且基于 Redis 来实现信息共享。 结束福利开源几个微服务的架构设计项目： https://github.com/damon008/spring-cloud-oauth2 https://github.com/damon008/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号程序猿 Damon 发起人。个人微信 MrNull008，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"Volcano 作业资源预留设计原理解读","date":"2020-12-28T06:37:04.000Z","path":"2020/12/28/volcano-reserve/","text":"简介Volcano 是一个基于 Kubernetes 的云原生批量计算平台，也是 CNCF 的首个批量计算项目。Volcano 主要用于 AI、大数据、基因、渲染等诸多高性能计算场景，对主流通用计算框架均有很好的支持。它提供高性能计算任务调度，异构设备管理，任务运行时管理等能力。本篇文章将深度剖析 Volcano 重要特性之——资源预留。 场景分析在实际应用中，常见以下两种场景： 在集群资源不足的情况下，假设处于待调度状态的作业 A 和 B，A 资源申请量小于 B 或 A 优先级高于 B。基于默认调度策略，A 将优先于 B 进行调度。在最坏的情况下，若后续持续有高优先级或申请资源量较少的作业加入待调度队列，B 将长时间处于饥饿状态并永远等待下去。 在集群资源不足的情况下，假设存在待调度作业 A 和 B。A 优先级低于 B 但资源申请量小于 B。在基于集群吞吐量和资源利用率为核心的调度策略下，A 将优先被调度。在最坏的情况下，B 将持续饥饿下去。 以上两种场景出现的根因是缺少一种公平调度机制：保证长期处于饥饿状态的作业在达到某个临界条件后被优先调度。造成作业持久饥饿的原因很多，包括资源申请量长时间无法满足、优先级持续过低、抢占发生频率过高、亲和性无法满足（v1.1.0 暂不支持此场景）等，以资源申请量无法满足最为常见。 特性设计为了保证长期处于阻塞状态的作业能够拥有公平的调度机会，需要解决两个主要问题： 如何识别目标作业？ 如何为目标作业预留资源？ 目标作业识别作业条件作业条件的选定可以基于等待时间、资源申请量等单个维度或多个维度的组合。综合考虑，v1.1.0 实现版本选择优先级最高且等待时间最长的作业作为目标作业。这样不仅可以保证紧急任务优先被调度，等待时间长度的考虑默认筛选出了资源需求较多的作业。 作业数量客观来说，满足条件的作业通常不止一个，可以为目标作业组或单个目标作业预留资源。考虑到资源预留必然引起调度器性能在吞吐量和延时等方面的影响，v1.1.0 采用了单个目标作业的方式。 识别方式识别方式有两种：自定义配置和自动识别。v1.1.0 暂时仅支持自动识别方式，即调度器在每个调度周期自动识别符合条件和数量的目标作业，并为其预留资源。后续版本将考虑在全局和 Queue 粒度支持自定义配置。 资源预留算法资源预留算法是整个特性的核心。v1.1.0 采用节点组锁定的方式为目标作业预留资源，即选定一组符合某些约束条件的节点纳入节点组，节点组内的节点从纳入时刻起不再接受新作业投递，节点规格总和满足目标作业要求。需要强调的是，目标作业将可以在整个集群中进行调度，非目标作业仅可使用节点组外的节点进行调度。 节点选取在特性设计阶段，社区考虑过以下节点选取算法：规格优先、空闲优先。 规格优先是指集群中所有节点按照主要规格（目标作业申请资源规格）进行降序排序，选取前 N 个节点纳入节点组，这 N 个节点的资源总量满足申请量。这种方式的优点是实现简单、锁定节点数量最小化、对目标作业的调度友好（这种方式锁定的资源总量往往比申请总量大一些，且作业中各 Pod 容易聚集调度在锁定节点，有利于 Pod 间通信等）；缺点是锁定资源总量大概率不是最优解、综合调度性能损失（吞吐量、调度时长）、易产生大资源碎片。v1.1.0 的实现采用的是该算法。 空闲优先是指集群中所有节点按照主要资源类型（目标作业申请资源类型）的空闲资源量进行降序排序，选取前 N 个节点纳入节点组，这 N 个节点的资源总量满足申请量。这种方式的优点是较大概率最快腾出满足要求的资源总量；缺点是集群空闲资源分布的强动态性导致节点组不是最优解，所求解稳定性差。 节点数量为了尽可能减少锁定操作对调度器综合性能的影响，在满足预留资源申请量的前提下，无论采用哪种节点选取算法，都应保证所选节点数最少。 锁定方式锁定方式包括两个核心考量点：并行锁定数量、锁定节点已有负载处理手段。 并行锁定数量有三个选择：单节点锁定、多节点锁定、集群锁定。单节点锁定是指每个调度周期内基于当前集群资源分布选定一个符合要求的节点纳入节点组。这种方式可以尽量减少资源分布波动对所求解的稳定性的影响，缺点是要经过较多的调度周期才能完成锁定过程。v1.1.0 的实现选择的是这种方式。 以此类推，多节点锁定是指每个调度周期内选定 X（X&gt;1）个满足条件的节点进行锁定。这种方式能一定程度上弥补单节点锁定引入的锁定时长过长问题，缺点是 X 不易找到最优值，实现复杂度高。 集群锁定是指一次性锁定集群所有节点，直至目标作业完成调度。这种粗暴的方式实现最为简单，目标作业等待时间最短，非常适合超大目标作业的资源预留。 锁定节点已有负载的处理手段有两种：抢占式预留、非抢占式预留。顾名思义，抢占式预留将会强制驱逐锁定节点上的已有负载。这种方式可以保证最快腾出所需的资源申请量，但会对已有业务造成重大影响，因此仅适用于紧急任务的资源预留。非抢占式预留则在节点锁定后不做任何处理，等待运行在其上的负载自行结束。v1.1.0 采用的是非抢占式预留。 最佳实践基于 v1.1.0 的实现，社区当前仅支持目标作业的自动化识别与资源预留。为此，新引入了 2 个 action 和 1 个 plugin。elect action 用于选取目标作业；reserve action 用于执行资源预留动作；reservation plugin 中实现了具体的目标选取和资源预留逻辑。 若要开启资源预留特性，将以上 action 和 plugin 配置到 volcano 的配置文件中即可。 下面是推荐配置样例： 12345678910111213actions: \"enqueue, elect, allocate, backfill, reserve\"tiers:- plugins:- name: priority- name: gang- name: conformance- name: reservation- plugins:- name: drf- name: predicates- name: proportion- name: nodeorder- name: binpack 自行配置时，请注意以下事项： elect action 必须配置在 enqueue action 和 allocate action 之间 reserve action 必须配置在 allocate action 之后 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"Go 并发基础","date":"2020-12-23T07:17:50.000Z","path":"2020/12/23/study-go/","text":"协程（Goroutine）我们知道 Go 中，存在一个 defer 关键字用于修饰一个函数或者方法，使得该函数或者方法在返回前才会执行，也就说被延迟执行，但又一定会执行。但其实 Go 中也存在类似的异步，或者说多线程的概念，但在 Go 中不叫作线程，而是叫协程。 协程相对于线程来说，是一个非常轻量级的东西，它在一个程序中，可以启动很多个。协程也称为 goroutine。goroutine 被 Go runtime 所调度，这一点和线程不一样。也就是说，Go 语言的并发是由 Go 自己所调度的，自己决定同时执行多少个 goroutine，什么时候执行哪几个。这些对于我们开发者来说很透明，只需要在编码的时候告诉 Go 语言要启动几个 goroutine，至于如何调度执行，我们不用关心。 启动一个 goroutine 简单，Go 语言为我们提供了 go 关键字，相比其他编程语言简化了很多，如代码: 123456789func main() &#123; go fmt.Println(&quot;码疯窝在香嗝喱辣&quot;) fmt.Println(&quot;I am main goroutine&quot;) time.Sleep(time.Second)&#125; 这样就启动了一个 goroutine，用来调用 fmt.Println 函数，打印”码疯窝在香嗝喱辣”，所以这段代码里，其实有两个 goroutine，一个是 main 函数启动的 main goroutine，一个是通过 go 关键字启动的 goroutine。 也就是说，启动一个协程的关键字 go 即可，语法: 123go function()go 函数执行体 go 关键字后跟一个方法或者函数的调用，就可以启动一个 goroutine，让方法在这个新启动的 goroutine 中运行。运行以上示例，可以看到如下输出： 12345I am main goroutine#待一秒的同时输出下面码疯窝在香嗝喱辣 从输出结果也可以看出，程序是并发的，go 关键字启动的 goroutine 并不阻塞 main goroutine 的执行，所以我们看到如上打印。 在 Go 中，既然有了协程，那么这些协程之间如何通信呢？Go 提供了一个 channel（通道） 来解决。 声明一个 channel在 Go 语言中，声明一个 channel 非常简单，使用内置的 make 函数即可，如下: 1ch:&#x3D;make(chan string) 其中 chan 是一个关键字，表示是 channel 类型。后面的 string 表示 channel 里的数据是 string 类型。通过 channel 的声明也可以看到，chan 是一个集合类型。 定义好 chan 后就可以使用了，一个 chan 的操作只有两种：发送和接收: 发送：向 chan 发送值，把值放在 chan 中，操作符为 chan &lt;- 接收：获取 chan 中的值，操作符为 &lt;- chan 示例: 1234567891011121314151617181920212223package mainimport &quot;fmt&quot;func main() &#123; ch :&#x3D; make(chan string) go func() &#123; fmt.Println(&quot;码疯窝在香嗝喱辣&quot;) ch &lt;- &quot;发送数据者：码疯窝在香嗝喱辣&quot; &#125;() fmt.Println(&quot;I am main goroutine&quot;) v :&#x3D; &lt;- ch fmt.Println(&quot;接收到的chan中的值为：&quot;,v)&#125; 我们先来执行看看打印结果: 12345I am main goroutine码疯窝在香嗝喱辣接收到的chan中的值为：送数据者：码疯窝在香嗝喱辣 从运行结果可以看出：达到了使用 time.Sleep 函数的效果。 相信应该明白为什么程序不会在新的 goroutine 完成之前退出了，因为通过 make 创建的 chan 中没有值，而 main goroutine 又想从 chan 中获取值，获取不到就一直等待，等到另一个 goroutine 向 chan 发送值为止。 无缓冲 channel上面的示例中，使用 make 创建的 chan 就是一个无缓冲 channel，它的容量是 0，不能存储任何数据。所以无缓冲 channel 只起到传输数据的作用，数据并不会在 channel 中做任何停留。这也意味着，无缓冲 channel 的发送和接收操作是同时进行的，它也被称为同步 channel。 有缓冲 channel有缓冲 channel 类似一个可阻塞的队列，内部的元素先进先出。通过 make 函数的第二个参数可以指定 channel 容量的大小，进而创建一个有缓冲 channel，如: 1cacheCh :&#x3D; make(chan int,5) 定义了一个容量为 5 的元素为 int 类型的 chan。 一个有缓冲 channel 具备以下特点： 有缓冲 channel 的内部有一个缓冲队列 发送操作是向队列的尾部插入元素，如果队列已满，则阻塞等待，直到另一个 goroutine 执行，接收操作释放队列的空间 接收操作是从队列的头部获取元素并把它从队列中删除，如果队列为空，则阻塞等待，直到另一个 goroutine 执行，发送操作插入新的元素 1234567cache :&#x3D; make(chan int,5)cache &lt;- 2cache &lt;- 3fmt.Println(&quot;容量:&quot;,cap(cache),&quot;,元素个数：&quot;,len(cache)) 无缓冲 channel 其实就是一个容量大小为 0 的 channel。比如 make(chan int,0) 关闭 channel通过内置函数 close 即可关闭 channel。如果一个 channel 被关闭了，就不能向里面发送数据了，如果发送的话，会引起 painc 异常。但是还可以接收 channel 里的数据，如果 channel 里没有数据的话，接收的数据是元素类型的零值。 单向 channel所谓单向，即可要不发送，要么只能接收。所以单向 channel 的声明也很简单，只需要在声明的时候带上 &lt;- 操作符即可，如下: 12send :&#x3D; make(chan &lt;- int)receive :&#x3D; make(&lt;- chan int) 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"云原生时代跨语言间微服务的打法","date":"2020-12-22T10:28:34.000Z","path":"2020/12/22/microservice-k8s/","text":"1. K8s 组件 configMap kube-apiserver scheduler etcd controller kube-proxy K8s 中主要通过 kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡，它是 K8s 集群内部的负载均衡器，也是一个分布式代理服务器，在 K8s 的每个节点上都有一个，这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的 kube-proxy 就越多，高可用节点也随之增多。通过 K8s service 的 “ClusterIP” 来实现集群内服务的 LB，当然，如果集群外需要访问 Service 对应的所有具有相同功能的 pod 应用程序，则可以通过 K8s service 的另外一种方式来实现：”NodePort”。 2. 基于 Service 实现微服务负载均衡在 Java 语言，或其它语言中，通常需要做很多繁重的组件来实现服务的 LB。例如：Dubbo、SpringCloud、甚至 SpringCloudAlibaba 等。当然，对于 Python、Go 等语言，也有其 Restful API，所以也会集成标准的代理插件来进行做传统的 LB。但对于云原生时代的到来，服务容器化让微服务的访问更好了。K8s Service 提供的 LB，即为无语言边际的负载均衡，不用考虑任何语言的阻碍，只要是通用的 Restful API，即可借助 service 来进行处理集群内部微服务之间的 LB。 在 K8s 集群中，如果内部访问，可以简单的通过 servicename 来进行访问。例如： 123456789101112apiVersion: v1kind: Servicemetadata: name: web-server-service namespace: defaultspec: ports: - name: web-server port: 80 targetPort: web-server-port selector: app: web-server 通过 selector 将 service 与服务 pod 对应起来，创建一个微服务的 service，默认其形式是：ClusterIP，则可以通过如下来访问该 service 对应的后端 pod: 1curl http:&#x2F;&#x2F;$service_name.$namespace.svc.cluster.local:$service_port&#x2F;api&#x2F;v1&#x2F;*** 这里，K8s 通过虚拟出一个集群 IP，利用 kube-proxy 为 service 提供 cluster 内的服务发现和负载均衡，上面说了 kube-proxy 的功能。 123456789Name: web-server-serviceNamespace: defaultType: ClusterIPIP: 20.16.249.134Port: &lt;unset&gt; 80&#x2F;TCPTargetPort: 80&#x2F;TCPEndpoints: 20.162.35.223:80Session Affinity: NoneEvents: &lt;none&gt; 3. 高可用案例3.1 传统微服务请求案例传统的微服务中，不同语言构建的微服务架构很多，一般直接通过 http 协议进行访问，在同一种语言中，又会出现一种集成框架模式来实现微服务架构。如：Java 中 Dubbo、Springcloud 等，但其繁琐的框架结构导致了服务的繁重。 3.2 跨语言间微服务的互通在 k8s 集群内，通过 kube-proxy 结合 service 等一些功能组件来实现微服务之间的调用，不管是同语言也好，跨语言也罢。都会很好的进行处理，包括实现高可用以及负载均衡、服务治理等。 任何一个 k8s 集群中的 pod 都可以通过 http 协议来访问其它 pod 的服务: 12345678root@rest-server-ver2-ds-vcfc7:&#x2F;usr&#x2F;src&#x2F;app# curl http:&#x2F;&#x2F;web-server-service.kube-system.svc.cluster.local:80&#x2F;api&#x2F;v1&#x2F;healthz&#123; &quot;status&quot;: &#123; &quot;code&quot;: 0, &quot;msg&quot;: &quot;success&quot; &#125;, &quot;data&quot;: &quot;success&quot;&#125;root@rest-server-ver2-ds-vcfc7:&#x2F;usr&#x2F;src&#x2F;app# 其中的权限有的可以通过 namespace 来控制，有的可以通过服务本身的访问权限来控制，但一切都可以进行访问，不存在语言的差别对待。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"极客时间专栏分享","date":"2020-12-16T07:12:38.000Z","path":"2020/12/16/geekbang-time/","text":"body { background: white; } .container { margin: 50px auto; width: 456px; height: 938px; overflow: hidden; border: 10px solid; border-top-color: white; border-left-color: white; border-bottom-color: white; border-right-color: white; position: relative; } .photo { position: absolute; animation: round 16s infinite; opacity: 0; } @keyframes round { 25% { opacity: 1; } 40% { opacity: 0; } } img:nth-child(1) { animation-delay: 12s; } img:nth-child(2) { animation-delay: 8s; } img:nth-child(3) { animation-delay: 4s; } img:nth-child(4) { animation-delay: 0s; } (function(){function t(e,t){return[].slice.call((t||document).querySelectorAll(e))}if(!window.addEventListener)return;var e=window.StyleFix={link:function(t){try{if(t.rel!==\"stylesheet\"||t.hasAttribute(\"data-noprefix\"))return}catch(n){return}var r=t.href||t.getAttribute(\"data-href\"),i=r.replace(/[^\\/]+$/,\"\"),s=t.parentNode,o=new XMLHttpRequest,u;o.onreadystatechange=function(){o.readyState===4&&u()};u=function(){var n=o.responseText;if(n&&t.parentNode&&(!o.status||o.status600)){n=e.fix(n,!0,t);if(i){n=n.replace(/url\\(\\s*?((?:\"|')?)(.+?)\\1\\s*?\\)/gi,function(e,t,n){return/^([a-z]{3,10}:|\\/|#)/i.test(n)?e:'url(\"'+i+n+'\")'});var r=i.replace(/([\\\\\\^\\$*+[\\]?{}.=!:(|)])/g,\"\\\\$1\");n=n.replace(RegExp(\"\\\\b(behavior:\\\\s*?url\\\\('?\\\"?)\"+r,\"gi\"),\"$1\")}var u=document.createElement(\"style\");u.textContent=n;u.media=t.media;u.disabled=t.disabled;u.setAttribute(\"data-href\",t.getAttribute(\"href\"));s.insertBefore(u,t);s.removeChild(t);u.media=t.media}};try{o.open(\"GET\",r);o.send(null)}catch(n){if(typeof XDomainRequest!=\"undefined\"){o=new XDomainRequest;o.onerror=o.onprogress=function(){};o.onload=u;o.open(\"GET\",r);o.send(null)}}t.setAttribute(\"data-inprogress\",\"\")},styleElement:function(t){if(t.hasAttribute(\"data-noprefix\"))return;var n=t.disabled;t.textContent=e.fix(t.textContent,!0,t);t.disabled=n},styleAttribute:function(t){var n=t.getAttribute(\"style\");n=e.fix(n,!1,t);t.setAttribute(\"style\",n)},process:function(){t('link[rel=\"stylesheet\"]:not([data-inprogress])').forEach(StyleFix.link);t(\"style\").forEach(StyleFix.styleElement);t(\"[style]\").forEach(StyleFix.styleAttribute)},register:function(t,n){(e.fixers=e.fixers||[]).splice(n===undefined?e.fixers.length:n,0,t)},fix:function(t,n,r){for(var i=0;i-1&&(e=e.replace(/(\\s|:|,)(repeating-)?linear-gradient\\(\\s*(-?\\d*\\.?\\d*)deg/ig,function(e,t,n,r){return t+(n||\"\")+\"linear-gradient(\"+(90-r)+\"deg\"}));e=t(\"functions\",\"(\\\\s|:|,)\",\"\\\\s*\\\\(\",\"$1\"+s+\"$2(\",e);e=t(\"keywords\",\"(\\\\s|:)\",\"(\\\\s|;|\\\\}|$)\",\"$1\"+s+\"$2$3\",e);e=t(\"properties\",\"(^|\\\\{|\\\\s|;)\",\"\\\\s*:\",\"$1\"+s+\"$2:\",e);if(n.properties.length){var o=RegExp(\"\\\\b(\"+n.properties.join(\"|\")+\")(?!:)\",\"gi\");e=t(\"valueProperties\",\"\\\\b\",\":(.+?);\",function(e){return e.replace(o,s+\"$1\")},e)}if(r){e=t(\"selectors\",\"\",\"\\\\b\",n.prefixSelector,e);e=t(\"atrules\",\"@\",\"\\\\b\",\"@\"+s+\"$1\",e)}e=e.replace(RegExp(\"-\"+s,\"g\"),\"-\");e=e.replace(/-\\*-(?=[a-z]+)/gi,n.prefix);return e},property:function(e){return(n.properties.indexOf(e)?n.prefix:\"\")+e},value:function(e,r){e=t(\"functions\",\"(^|\\\\s|,)\",\"\\\\s*\\\\(\",\"$1\"+n.prefix+\"$2(\",e);e=t(\"keywords\",\"(^|\\\\s)\",\"(\\\\s|$)\",\"$1\"+n.prefix+\"$2$3\",e);return e},prefixSelector:function(e){return e.replace(/^:{1,2}/,function(e){return e+n.prefix})},prefixProperty:function(e,t){var r=n.prefix+e;return t?StyleFix.camelCase(r):r}};(function(){var e={},t=[],r={},i=getComputedStyle(document.documentElement,null),s=document.createElement(\"div\").style,o=function(n){if(n.charAt(0)===\"-\"){t.push(n);var r=n.split(\"-\"),i=r[1];e[i]=++e[i]||1;while(r.length>3){r.pop();var s=r.join(\"-\");u(s)&&t.indexOf(s)===-1&&t.push(s)}}},u=function(e){return StyleFix.camelCase(e)in s};if(i.length>0)for(var a=0;a","tags":[]},{"title":"k8s 集群下微服务 pod 的各种指标信息监控","date":"2020-11-04T08:28:57.000Z","path":"2020/11/04/monitor-pod-state-metrics/","text":"今天主要分享下，在 k8s 集群下，微服务的各种状态指标情况的监控，我们都知道Prometheus是做数据监控的，但说白点，其独特格式的数据，其实都是靠一些源来的，那么这些源有哪些呢？已经有了cadvisor、heapster、metric-server，几乎容器运行的所有指标都能拿到，但是下面这种情况却无能为力： 1234调度了N个replicas？现在可用的有 N 个？N 个 Pod 是 running&#x2F;stopped&#x2F;terminated 状态？Pod 重启了N次？我有 N 个job在运行中 而这些则是 kube-state-metrics 提供的内容，它基于client-go开发，轮询Kubernetes API，并将Kubernetes的结构化信息转换为metrics。kube-state-metrics是kubernetes开源的一个插件。 废话不多说，直接上教程。。。 部署教程下载 在官网 kube-state-metrics 下载相应的源码以及部署脚本，本次使用release1.9.7，即v1.9.7版本的 kube-state-metrics 执行 cd /kube-state-metrics/examples/standard，可以看到几个文件： 123456cluster-role-binding.yamlcluster-role.yamldeployment.yamlprometheus-configmap.yamlservice-account.yamlservice.yaml 如果Prometheus已经部署，且部署在kube-system空间下，则源码中的namespace不需更改，否则可自定义为monitoring。 更新 首先修改 service.yaml 123456789101112131415161718192021apiVersion: v1kind: Servicemetadata: annotations: prometheus.io&#x2F;scrape: &quot;true&quot; labels: app.kubernetes.io&#x2F;name: kube-state-metrics app.kubernetes.io&#x2F;version: v1.9.7 name: kube-state-metrics namespace: kube-systemspec: clusterIP: None ports: - name: http-metrics port: 8080 targetPort: http-metrics - name: telemetry port: 8081 targetPort: telemetry selector: app.kubernetes.io&#x2F;name: kube-state-metrics 很简单，增加了注解方便后面使用 坑：源码中的角色授权绑定的是其写的kind为ClusterRole的资源，但后来发现部署kube-state-metrics服务时，其无法成功访问k8s的api-server，故需要修改，弃用其ClusterRole，使用k8s系统最高权限cluster-admin。 更改访问权限 vi cluster-role-binding.yaml 123456789101112131415apiVersion: rbac.authorization.k8s.io&#x2F;v1kind: ClusterRoleBindingmetadata: labels: app.kubernetes.io&#x2F;name: kube-state-metrics app.kubernetes.io&#x2F;version: v1.9.7 name: kube-state-metricsroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin #kube-state-metricssubjects:- kind: ServiceAccount name: kube-state-metrics namespace: kube-system 部署12cd &#x2F;kube-state-metrics&#x2F;examples&#x2F;standardkubectl create -f . 此时还需要更新Prometheus的挂载的configMap，因为前面说了只抓取带有prometheus.io/scrape: “true”注解的endpoint vi prometheus-configmap.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142apiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: kube-systemdata: prometheus.yaml: | global: scrape_interval: 15s evaluation_interval: 15s scrape_configs: - job_name: &#39;kubernetes-apiservers&#39; kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;ca.crt bearer_token_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: &#39;kubernetes-nodes&#39; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;ca.crt bearer_token_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: &#x2F;api&#x2F;v1&#x2F;nodes&#x2F;$&#123;1&#125;&#x2F;proxy&#x2F;metrics - job_name: &#39;kubernetes-cadvisor&#39; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;ca.crt bearer_token_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: &#x2F;api&#x2F;v1&#x2F;nodes&#x2F;$&#123;1&#125;&#x2F;proxy&#x2F;metrics&#x2F;cadvisor - job_name: &#39;kubernetes-service-endpoints&#39; kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: service_name - job_name: &#39;kubernetes-services&#39; kubernetes_sd_configs: - role: service metrics_path: &#x2F;probe params: module: [http_2xx] relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name - job_name: &#39;kubernetes-ingresses&#39; kubernetes_sd_configs: - role: ingress relabel_configs: - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path] regex: (.+);(.+);(.+) replacement: $&#123;1&#125;:&#x2F;&#x2F;$&#123;2&#125;$&#123;3&#125; target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_ingress_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_ingress_name] target_label: kubernetes_name - job_name: &#39;kubernetes-pods&#39; kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name 更新 configmap 后，需要重启 Prometheus 使其生效，如果没部署，则创建 configmap 后执行脚本部署即可。 导入模板 最后从 grafana.com 下载 state-metrics 监控模版导入模板 导入到 grafana 后，即可看到效果咯： 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"如何利用k8s拉取私有仓库镜像","date":"2020-08-19T09:15:58.000Z","path":"2020/08/19/k8s-image/","text":"现象最近实战时，发现一个很奇怪的问题，在通过 k8s 创建 pod，拉取镜像时，总是显示如下信息： 1Error syncing pod, skipping: failed to &quot;StartContainer&quot; for &quot;POD&quot; with ImagePullBackOff: &quot;Back-off pulling image ...&quot; 该现象出现的原因可能是网络问题、docker 环境问题等。但如果访问的是一个公开的镜像仓库，在 pull image 的时候，不应该会提示：ImagePullBackOff，但如果访问的是私有仓库，那就有可能出现如下的错误： 这个错误出现的原因，刚才说了，有可能的网络问题，也有可能是 docker 问题，但有时候，这些不能解决的情况下，可以采用下面三种方式来解决。 方式一 第一种方式，我们可以使用文件生成 secret，然后通过 k8s 中的 imagePullSecrets 来解决拉取镜像时的验证问题。具体方式如下： 修改 /etc/docker/daemon.json在 k8s 集群节点上，修改 docker 的 daemon.json 配置文件: 1234&#123;&quot;registry-mirrors&quot;: [ &quot;https:&#x2F;&#x2F;registry.docker-cn.com&quot;],&quot;insecure-registries&quot;:[&quot;私有仓库服务地址&quot;]&#125; 在里面加上自己私有的仓库服务地址，然后重启 docker 服务，使其生效。 生成 ~/.docker/config.json 文件1docker login 私有服务地址 在命令行输入上面的命令，回车后，会提示输入用户名和密码。输入正确信息后，这会生成一个 /root/.docker/config.json 文件。同时会提示： 1Login Succeeded 生成 Secret 串根据上面生成的 ~/.docker/config.json 文件，我们可以生成一个密文秘钥： 1base64 -w 0 ~&#x2F;.docker&#x2F;config.json 执行上面的命令后，会生成一个长串，即为我们所要的 Secret 串。 我们会在 source 下看见一个新的文件夹，_drafts，这个里面会装我们所有的草稿文件。 创建 Secret通过 k8s 我们可以生成一个 Secret 资源： 12345678apiVersion: v1kind: Secretmetadata: name: docker_reg_secret namespace: defaultdata: .dockerconfigjson: ewoJImF1dGhjNWdlpHVnVaenB5Wld4aFFFeFdUa2xCVGtBeU1ERTMiCgkJfASEkidXJlZy5rOHMueXVud2VpLnJlbGEubWUiOiB7CgkJCSJhdXRoIjogIloyRnZaM1Z2WkdWdVSrsaaehoUUV4V1RrbEJUa0F5TURFMyIKCQl9Cgl9LAoJIkh0dHBIZWFkZXJzIjogewoJSetcaFTssZW50IjogIkRvY2tlci1DbGllbnQvMTguMDYuMS1jZSAobGludXgpIgoJfQp9type: kubernetes.io&#x2F;dockerconfigjson 执行这个资源的配置： 1kubectl create -f secret.yml 在服务配置加上依赖最后，可以在 我们的服务 yml 文件中加上拉取镜像时的依赖 secret，部分代码如下： 12imagePullSecrets:- name: docker_reg_secret 方式二 第二种方式，我们可以直接使用 docker 的用户信息来生成 secret： 1kubectl create secret docker-registry docker_reg_secret --docker-server&#x3D;XXX --docker-username&#x3D;XXX --docker-password&#x3D;XXX 参数含义： docker_reg_secret: 指定密钥的键名称, 自定义 docker-server: 指定 docker 仓库地址 docker-username: 指定 docker 仓库账号 docker-password: 指定 docker 仓库密码 创建完 Secret 资源后，其他的如方式一，这就简单了。 方式三 第三种方式所使用的是最简单的办法，即我们利用 k8s 的拉取镜像的策略来处理，主要有如下三种： Always：每次创建时都会拉取镜像 IfNotPresent：宿主机不存在时拉取镜像 Never： 从不主动拉取镜像 使用 IfNotPresent、Never 策略来处理。 以上三种方式，我比较推荐第二种，最中意第二种，因为假如密码修改了，就更新一下 secret 就好了，k8s node 不需要改动。而第一种需要改动，第三种会导致镜像丢失，毕竟只有本地存在。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"个站建立基础教程","date":"2020-08-16T10:12:17.000Z","path":"2020/08/16/new-web/","text":"什么是 HexoHexo 是一个静态网站生成器，基于 Hexo 框架，可以生成自己想要的网站风格，以及网站内容，样式自己可以定义。 实战 环境准备12345系统： win7 +nodejs：http:&#x2F;&#x2F;nodejs.cn&#x2F;download&#x2F;git-client：https:&#x2F;&#x2F;git-scm.com&#x2F;download&#x2F; 安装 Hexo准备好以上环境后，就可以准备安装 Hexo 基本框架： 1234567891011121314#安装基本框架npm install -g hexo#初始化hexo框架hexo init#安装所需要的组件npm install#编译生成静态页面hexo g#启动服务hexo s 这是一个基本的 Hexo 原型，当然，Hexo 有许多 themes，官方地址：https://hexo.io/themes/index.html，本文实战用的是Ayer。可以先从github官网拉取相关themes的基础源码。 拉取源码后，在其根目录下，进行一些基本的安装组件操作： 组件12345678910111213141516171819202122npm install hexo-renderer-jade@0.3.0 --savenpm install hexo-generator-json-content@2.2.0 --savenpm install hexo-renderer-stylus --save#用于搜索npm install hexo-generator-searchdb --save#用于生成RSS订阅npm install hexo-generator-feed --savenpm uninstall hexo-generator-index --save#用于文章置顶npm install hexo-generator-index-pin-top --save#用于文章加密，具体参考 https:&#x2F;&#x2F;github.com&#x2F;MikeCoder&#x2F;hexo-blog-encrypt&#x2F;blob&#x2F;master&#x2F;ReadMe.zh.mdnpm install --save hexo-blog-encrypt#音乐播放器参考：https:&#x2F;&#x2F;github.com&#x2F;MoePlayer&#x2F;hexo-tag-aplayer&#x2F;blob&#x2F;master&#x2F;docs&#x2F;README-zh_cn.md 新建草稿文章1hexo new draft b 我们会在 source 下看见一个新的文件夹，_drafts，这个里面会装我们所有的草稿文件。 预览草稿1hexo server --draft 发布草稿1hexo publish b 新建正式文章1hexo new a 在 hexo 目录下的 source/_post 下生成 a.md 打开 a.md，可以编辑文章 生成页面文件12345hexo generateorhexo g 生成页面1hexo new page about 这样直接在 source 下创建 about 目录，下面也会生成一个 index.md 启动服务1hexo server 以上关于 Hexo 的基本命令以及对应的功能操作介绍完了。 我们来看看我的网站吧：Damon | Micro-Service | Containerization | DevOps。 主页展示的是个人文章，这些对于 hexo 来说就是一个个页面： 在主页可以看到左侧的栏目，这些就是 hexo 的页面，比如：_关于我_： 由于上面我们还加入了搜索插件，所以，我们可以进行全文搜索： 当然，还有一些订阅模式，等等功能。 hexo 不管是页面也好，还是文章也好，都是通过 md 格式文件来生成静态页面的，所以看起来很简单。 其次，比较重要的是有一个文件中，可以配置各种开关或格式控制： 这个里面可以根据官网配置自己想要的功能，包括打赏： 到目前为止，基于 hexo 生成静态网站的主体就到此结束啦，欢迎大家关注个站哟：Damon | Micro-Service | Containerization | DevOps。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"如何保证NFS文件锁的一致性","date":"2020-08-06T03:18:42.000Z","path":"2020/08/06/nfs-01/","text":"简介 在存储系统中， NFS（Network File System，即网络文件系统）是一个重要的概念，已成为兼容POSIX语义的分布式文件系统的基础。它允许在多个主机之间共享公共文件系统，并提供数据共享的优势，从而最小化所需的存储空间。本文将通过分析NFS文件锁状态视图一致性的原理，帮助大家理解NFS的一致性设计思路。 文件锁 文件锁是文件系统的最基本特性之一，应用程序借助文件锁可以控制其他应用对文件的并发访问。NFS作为类UNIX系统的标准网络文件系统，在发展过程中逐步地原生地支持了文件锁(从NFSv4开始)。NFS从上个世界80年代诞生至今，共发布了3个版本：NFSv2、NFSv3、NFSv4。 NFSv4最大的变化是有“状态”了。某些操作需要服务端维持相关状态，如文件锁，例如客户端申请了文件锁，服务端就需要维护该文件锁的状态，否则和其他客户端冲突的访问就无法检测。如果是NFSv3就需要NLM协助才能实现文件锁功能，但是有的时候两者配合不够协调就会容易出错。而NFSv4设计成了一种有状态的协议，自身就可以实现文件锁功能，也就不需要NLM协议了。 应用接口 应用程序可以通过 fcntl() 或 flock() 系统调用管理NFS文件锁，下面是NAS使用NFSv4挂载时获取文件锁的调用过程： 从上图调用栈容易看出，NFS文件锁实现逻辑基本复用了VFS层设计和数据结构，在通过RPC从Server成功获取文件锁后，调用 locks_lock_inode_wait() 函数将获得的文件锁交给VFS层管理，关于VFS层文件锁设计的相关资料比较多，在此就不再赘述了。 EOS原理 文件锁是典型的非幂等操作，文件锁操作的重试和Failover会导致文件锁状态视图在客户端和服务端间的不一致。NFSv4借助SeqId机制设计了最多执行一次的机制，具体方法如下： 针对每个open/lock状态，Client和Server同时独立维护seqid，Client在发起会引起状态变化的操作时(open/close/lock/unlock/release_lockowner)会将seqid加1，并作为参数发送给Server，假定Client发送的seqid为R，Server维护的seqid为L，则： 若R == L +1，表示合法请求，正常处理之。若R == L，表示重试请求，Server将缓存的reply返回即可。其他情况均为非法请求，决绝访问。根据上述规则，Server可判断操作是否为正常、重试或非法请求。 该方法能够保证每个文件锁操作在服务端最多执行一次，解决了RPC重试带来的重复执行的问题，但是仅靠这一点是不够的。比如LOCK操作发送后调用线程被信号中断，此后服务端又成功接受并执行了该LOCK操作，这样服务端就记录了客户端持有了锁，但客户端中却因为中断而没有维护这把锁，于是就造成了客户端和服务端间的锁状态视图不一致。因此，客户端还需要配合处理异常场景，最终才能够保证文件锁视图一致性。 异常处理由上一节的分析可知，客户端需要配合处理异常场景才能够保证文件视图一致性，那么客户端设计者主要做了哪些配合的设计呢？目前客户端主要从SunRPC和NFS协议实现两个维度相互配合解决该问题，下面分别介绍这两个维度的设计如何保证文件锁状态视图一致性。 SunRPC设计 SunRPC是Sun公司专门为远程过程调用设计的网络通讯协议，这里从保障文件锁视图一致性的维度来了解一下SunRPC实现层面的设计理念： （1）客户端使用int32_t类型的xid标识上层使用者发起的每个远程过程调用过程，每个远程过程调用的多次RPC重试使用相同的xid标识，这样就保障了多次RPC重试中任何一个返回都可以告知上层远程过程调用已经成功，保证了服务端执行远程过程调用执行耗时较长时也能拿到结果，这一点和传统的netty/mina/brpc等都需要每个RPC都要有独立的xid/packetid不同。 （2）服务端设计了DRC(duplicate request cache)缓存最近执行的RPC结果，接收到RPC时会首先通过xid检索DRC缓存，若命中则表明RPC为重试操作，直接返回缓存的结果即可，这在一定程度上规避了RPC重试带来的重复执行的问题。为了避免xid复用导致DRC缓存返回非预期的结果，开发者通过下述设计进一步有效地减少复用引起错误的概率： 客户端建立新链接时初始xid采用随机值。服务端DRC会额外记录请求的校验信息，缓存命中时会同时校验这些信息。 （3）客户端允许在获得服务端响应前无限重试，保证调用者能够获得服务端确定性的执行结果，当然这样的策略会导致无响应时调用者会一直hang。 （4）NFS允许用户在挂载时通过soft/hard参数指定SunRPC的重试策略，其中soft模式禁止超时后重试，hard模式则持续重试。当用户使用soft模式挂载时NFS实现不保证客户端和服务端状态视图的一致性，在遇到远程过程调用返回超时要求应用程序配合状态的清理和恢复，比如关闭访问出错的文件等，然而实践中很少有应用程序会配合，所以一般情况下NAS用户都使用hard模式挂载。 总之，SunRPC要解决的核心问题之一是，远程过程调用执行时间是不可控的，协议设计者为此定制化设计，尽量避免非幂等操作RPC重试带来的副作用。 信号中断 应用程序等待远程过程调用结果时允许被信号中断。当发生信号中断时，由于没有得到远程过程调用的执行结果，所以客户端和服务端的状态很可能就不一致了，比如加锁操作在服务端已经成功执行，但客户端并不知道这个情况。这就要求客户端做额外的工作将状态和服务端恢复一致。下面简要分析获取文件锁被信号中断后的处理，来说明NFS协议实现层面的一致性设计。 通过获取NFSv4文件锁的过程可知，NFSv4获取文件锁最终会调用 _nfs4_do_setlk() 函数发起RPC操作，最终调用 nfs4_wait_for_completion_rpc_task() 等待，下面是相关代码： 12345678910111213141516static int _nfs4_do_setlk(struct nfs4_state *state, int cmd, struct file_lock *fl, int recovery_type)&#123; ...... task &#x3D; rpc_run_task(&amp;task_setup_data); if (IS_ERR(task)) return PTR_ERR(task); ret &#x3D; nfs4_wait_for_completion_rpc_task(task); if (ret &#x3D;&#x3D; 0) &#123; ret &#x3D; data-&gt;rpc_status; if (ret) nfs4_handle_setlk_error(data-&gt;server, data-&gt;lsp, data-&gt;arg.new_lock_owner, ret); &#125; else data-&gt;cancelled &#x3D; 1; ......&#125; 通过分析 nfs4_wait_for_completion_rpc_task() 实现可知，当ret &lt; 0时，表明获取锁过程被信号中断，并使用 struct nfs4_lockdata 的 cancelled 成员记录。继续查看rpc_task完成后释放时的回调函数 nfs4_lock_release()： 从上面红色框中的代码可知，nfs4_lock_release() 检测到存在信号中断时会调用 nfs4_do_unlck()函数尝试将可能成功获得文件锁释放掉，注意此时没有调用 nfs_free_seqid() 函数将持有的nfs_seqid释放掉，这是为了： 保证订正状态过程中不会有用户新发起的并发加锁或者释放锁操作，简化实现。保证hard模式下UNLOCK操作只会在LOCK操作返回后才会发送，保障已经获得锁能够被释放掉。客户端通过上面的方法能够有效地保证信号中断后客户端和服务端锁状态的最终一致性,但也是在损失一部分可用性为代价的。 总结 文件锁是文件系统原生支持的基础特性，NAS作为共享的文件系统要面临客户端和服务端锁状态视图一致性的问题，NFSv4.0在一定程度上解决了这个问题，当然，技术前进的脚步不会停止，NFS的更新迭代也就不会停止，未来的NFS将会有更多的期待。 最后 我们相信技术的力量，更相信拥有技术力量的人。我们期待存储的未来，更期待与你一起创造未来。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 特别声明 原文作者：茶什i 本文原链：https://developer.aliyun.com/article/769594?spm=a2c6h.12873581.0.dArticle769594.37366446qrd1Wv&groupCode=alitech 本文转载如有侵权，请联系站长删除，谢谢 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"电视剧 | 电影 | 影片 | 无广告","date":"2020-08-05T08:55:58.000Z","path":"2020/08/05/film-01/","text":"NO 视频 片库 茶杯狐 蓝光高清网 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"Kubernetes 经典命令","date":"2020-07-27T09:41:37.000Z","path":"2020/07/27/k8s-02/","text":"最近大家想了解 Kubernetes 常见命令，今天它来了。 如果想玩玩单机版、集群版 k8s，可参考：k8s部署手册，快速助力部署 k8s，还没毕业的都可以部署哟！ k8s 常用命令： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758​netstat -nlpt | grep 关键字: 查询相关的网络问题​systemctl status -l kubelet: 查看kubelet状态​systemctl restart kube-apiserver: 重启apiserver​kubelet --version: 查看k8s版本​history |grep 关键字: 查看相关操作历史​kubectl cluster-info: 查看集群信息 or kubectl cluster-info dump​kubectl -n kube-system get sa: 查看所有账号​kubectl get ep: 获取所有endpoints信息​kubectl get svc: 获取服务 -n 空间名称，指定命名空间​kubectl get pods --all-namespaces -o wide: 获取所有的pods​kubectl create -f *.yaml: 使用yaml文件创建pod，这个不可重复执行​kubectl apply -f *.yaml: 可重复执行​kubectl delete -f *.yaml: 使用yaml文件删除pod​kubectl logs POD_NAME -n 空间名称: 显示指定命名空间的pod的日志​kubectl get nodes: 获取集群所有节点信息​kubectl delete node ip: 删除节点​kubectl describe node ip: 显示节点信息​kubectl describe pod podName: 显示pod信息​kubectl describe ep kubernetes​kubectl describe svc kubernetes​kubectl get svc kubernetes​kubectl delete pod --all: 删除所有pod​kubectl exec -it podname bash or sh: 进入某个pod容器​kubectl logs podname: 查看某个pod日志​kubectl logs -f podname: 实时查看某个pod日志​kubectl logs -f --tail&#x3D;100 podname: 实时查看某个pod最新100条日志​kubectl log podname -c containername: 若 pod 只有一个容器，可以不加 -c​kubectl scale --replicas&#x3D;2 deployment edge-cas-deployment: 以deployment形式启动2个pod​kubectl explain pod: 查看pod的注释kubectl explain pod.apiVersion 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"Docker常用命令，你都会了吗","date":"2020-07-27T09:35:10.000Z","path":"2020/07/27/docker-01/","text":"应大家要求，今天整理下 Docker 常见的一些命令。 关于 docker 的安装，在 k8s部署手册 一文中，你可以快速安装docker的各种版本。 常见命令： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119docker images: 查看镜像，后可跟 &quot;| grep 内容&quot;，可根据内容进行筛选。如：docker images | grep nginx​docker images [OPTIONS] [REPOSITORY[:TAG]]OPTIONS说明:-a: 列出本地所有的镜像--digests: 显示镜像的摘要信息-f: 显示满足条件的镜像--format: 指定返回值的模板文件--no-trunc: 显示完整的镜像信息-q: 只显示镜像ID​docker run: 创建一个新的容器并运行一个命令docker run [OPTIONS] IMAGE [COMMAND] [ARG...]OPTIONS说明:-d: 后台运行容器，并返回容器ID-i: 以交互模式运行容器，通常与 -t 同时使用-p: 指定端口映射，格式为：主机(宿主)端口:容器端口-t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用--name &quot;nginx&quot;: 为容器指定一个名称-h &quot;localhost&quot;: 指定容器的hostname-e spring.profiles.active&#x3D;&quot;dev&quot;: 设置环境变量--env-file&#x3D;[]: 从指定文件读环境变量-m :设置容器使用内存最大值--volume &#x2F;home&#x2F;data:&#x2F;etc&#x2F;data : 绑定一个卷and so on​如：docker run -d -t -p 80:80 -v &#x2F;home&#x2F;data:&#x2F;usr&#x2F;data --name nginx nginx:latest​docker create: 创建一个新的容器但不启动它​docker stop: 停止一个运行的容器docker stop containerName​docker restart: 重启一个容器docker restart containerName​docker start: 启动一个被停止的容器docker start containerName​docker ps [OPTIONS]: 列出容器OPTIONS说明:-a: 显示所有的容器，包括未运行的-f: 根据条件过滤显示的内容--format: 指定返回值的模板文件-l: 显示最近创建的容器-n: 列出最近创建的n个容器--no-trunc: 不截断输出-q: 静默模式，只显示容器编号-s: 显示总的文件大小​docker ps -a: 查看所有容器​docker ps: 查看正在运行的容器​docker exec: 进入一个运行中的容器执行命令如：docker exec -it 容器id sh or bash or &#x2F;bin&#x2F;bash表示在容器中开启一个交互模式的终端​docker rm: 删除一个容器，可加-f 表示强制 -v：并删除挂载卷删除所有停止的容器：docker rm $(docker ps -a -q)​docker rmi: 删除一个镜像，可加-f 表示强制​docker inspect : 获取容器&#x2F;镜像的元数据如：docker inspect [OPTIONS] NAME|ID [NAME|ID...]OPTIONS说明:-f: 指定返回值的模板文件-s: 显示总文件大小-type: 为指定类型返回json数据​获取正在运行的容器 nginx 的 IP:docker inspect --format&#x3D;&#39;&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;&#39; nginx​docker kill: 杀死一个运行中的容器如： docker kill -s killyou nginx​docker logs: 获取容器的日志如：docker logs -f -t 容器id or docker logs -f -t --tail&#x3D;100 容器id​docker build: 命令用于使用 Dockerfile 创建镜像docker build [OPTIONS] PATH | URL | -OPTIONS说明:-f: 指定要使用的Dockerfile路径-m: 设置内存最大值--memory-swap: 设置Swap的最大值为内存+swap，&quot;-1&quot;表示不限swap--no-cache: 创建镜像的过程不使用缓存--pull: 尝试去更新镜像的新版本-q: 安静模式，成功后只输出镜像 ID--rm: 设置镜像成功后删除中间容器--shm-size: 设置&#x2F;dev&#x2F;shm的大小，默认值是64M--tag: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签--network: 默认 default。在构建期间设置RUN指令的网络模式​docker build -t 镜像标签名 .: docker build -t nginx:latest .docker build -f &#x2F;path&#x2F;to&#x2F;a&#x2F;Dockerfile .​docker tag: 标记本地镜像，将其归入某一仓库​docker tag nginx nginx:old​docker save: 将指定镜像保存成 tar 归档文件docker save -o nginx.tar nginx:latest​docker load: 导入使用 docker save 命令导出的镜像docker load -i tar文件名​docker info: 查看docker环境信息​docker version: 查看docker版本信息​docker login: 登录一个Docker镜像仓库docker login -u 用户名 -p 密码​docker logout: 退出登录​docker pull: 拉取或者更新指定镜像 -a 拉取所有的tag的镜像​docker push: 将本地的镜像上传到镜像仓库 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"消息中间件那点事儿","date":"2020-07-27T02:26:55.000Z","path":"2020/07/27/mq-01/","text":"背景系统的稳定性一直以来是所有企业研发人员所追求的，当出现系统问题时，这时候可以通过日志、系统监控、性能指标等来进行排查，但系统的复杂性、分布式、高并发等导致了很多信息的堆积，这个时候，可能对于工作人员来说，是一件非常头痛的事情。消息中间件，英文简称 MQ，这个名词的出现，可以解决很多事情：比如：复杂的业务中通过 MQ 来减少频繁的业务交互。高并发下，不必及时处理的事务都可以交给 MQ。交易过程中，更是存在很多的事务处理，如果长时间保持长链接以及锁的状态，很有可能造成锁表、锁库，形成死锁。这时候也需要 MQ 来进行消息的缓冲、异步进行，保证系统的稳定、持续的进行下去。不会因为死锁等长时间的循环而导致 cpu、内存的增耗，从而避免出现服务挂掉、宕机等问题。 形形色色的 MQ事实上，消息中间件的种类越来越多：RabbitMQ、RocketMQ、Kafka 等等。下面给出了一张表展示各种 MQ 特点： 说到这，大家可能要说，还有一种中间件：Redis，的确，Redis 也作为一种中间件，一般用来作为缓存中间件。缓存一些信息，以便数据信息共享、也可以利用其来实现分布式锁，例如实现秒杀、抢单等功能。还会被用作一些订单信息的缓存，防止大量的订单信息被积压而导致服务器的负载很高。总之，Redis 常被用来作为一种缓冲剂使用。 除了上面说的，消息中间件还可以用来抢红包，交易系统的账单记录、流程推送、通知等等。Redis 作为缓存处理器，它的使用，大大的提升了应用的性能与效率，特别是在查询数据的层面上，大大降低了查询数据库的频率。但这也带来了一些问题，其中比较典型的，比如：缓存穿透、缓存击穿、缓存雪崩。 什么是缓存穿透呢？ 我们先来看看缓存的查询流程：前端发来请求查询数据时，后端首先会在缓存 Redis 中查询，如果查询到数据，直接返回给前端，流程结束；如果在缓存中未查到数据，则前往数据库查找，此时查询数据后返回给前端，同时会塞进缓存中。还有一种可能就是：查询数据库未查到数据时，会直接返回 NULL。 这种情况下，如果用户不停滴发起请求时，恶意提供数据库中不存在的信息，则在数据库中查到的数据永远都是 NULL。这种数据是不会被塞进缓存的，这种的数据永远会被从数据库中访问，即为恶意攻击式，则很有可能对数据库造成极大的压力，搞哭数据库。这个过程被称为：缓存穿透。缓存永远被直接穿透而直接访问数据库。 解决方案 目前对于缓存穿透，比较典型的解决方案是：当在数据库查询未找到时，将 NULL 返给前端，同时，会将 NULL 塞入缓存，并对对应的 Key 设置一定的过期时间。 这种处理方式在电商的话，用到的较多。 什么是缓存击穿呢？ 缓存击穿，是指缓存中某个 Key 在不停的、频繁的被请求，当这个 Key 在某个时刻失效时，持续的高并发请求就会击穿缓存，直接请求数据库，导致数据库的压力在那一时刻猛增。就像水滴石穿。 解决方案既然这种 key 会被不停的访问、请求，那么可以将其有效期设为一万年，这样，不停的高并发请求，就永不会落在数据库层。 什么是缓存雪崩呢？ 缓存雪崩，是指在某个时刻，缓存的 key 集体发生失效，这样导致大量的查询请求落在了数据库层，导致数据库负载过高，甚至会压垮数据库。 解决方案雪崩的现象，主要在于大量的 key 在同一时刻处于失效状态，所以为了避免这种情况：一般会为 key 设置不同的、随机的失效时间，错开缓存中 key 的失效时间点，从而最终减少数据库压力。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"浅谈微服务安全架构设计","date":"2020-07-24T05:57:33.000Z","path":"2020/07/24/micro-service01/","text":"1、 回顾微服务设计理念 在 浅入微服务架构 一文中，我们了解到什么是微服务，微服务的划分依据，其实，说到底，微服务的设计，有其独到的好处：使得各个模块之间解耦合，让每一个模块有自己独立的灵魂，其他服务即使出现任何问题，自己不会受到任何的影响。这是微服务的核心宗旨。那么今天要讲的微服务安全性问题，其实也是反映微服务的一个核心：高内聚。所谓高内聚，简单的理解就是，对外暴露的最小限度，降低其依赖关系，大部分都作为一个黑盒子封装起来，不直接对外，这样，即使内部发生变更、翻云覆雨，对外的接口没发生改变，这才是好的微服务设计理念，做到完美的对外兼容，一个好的架构设计，首先，这一点可能需要 get 到位，不知道大家咋认为呢？所以今天说的微服务安全性，就跟这个高内聚有一点点相关了。或者说，体现了微服务设计的核心理念。 2、微服务下的各种安全性保证 2.1 常见的几种安全性措施在微服务中，我们常见的，有如下几种安全性设计的举措：网关设计、服务端口的对外暴露的限度、token 鉴权、OAuth2 的统一认证、微信中的 openId 设计等。这些都是在为服务的安全性作考虑的一些举措。 2.2 OAuth2 的概念何为 OAuth2 呢？我们先了解 OAuth，Oauth 是一个开放标准，假设有这样一种场景：一个 QQ 应用，希望让一个第三方的（慕课网）应用，能够得到关于自身的一些信息（唯一用户标识，比如说 QQ 号，用户个人信息、一些基础资料，昵称和头像等）。但是在获得这些资料的同时，却又不能提供用户名和密码之类的信息。如下图： 而 OAuth 就是实现上述目标的一种规范。OAuth2 是 OAuth 协议的延续版本，但不兼容 OAuth1.0，即完全废弃了 OAuth1.0。 OAuth2.0 有这么几个术语：客户凭证、令牌、作用域。 客户凭证：客户的 clientId 和密码用于认证客户。 令牌：授权服务器在接收到客户请求后颁发的令牌。 作用域：客户请求访问令牌时,由资源拥有者额外指定的细分权限。 2.3 OAuth2 的原理在 OAuth2 的授权机制中有 4 个核心对象： Resource Owner：资源拥有者，即用户。 Client：第三方接入平台、应用，请求者。 Resource Server：资源服务器，存储用户信息、用户的资源信息等资源。 Authorization Server：授权认证服务器。 实现机制： 用户在第三方应用上点击登录，应用向认证服务器发送请求，说有用户希望进行授权操作，同时说明自己是谁、用户授权完成后的回调 url，例如：上面的截图，通过慕课网访问 QQ 获取授权。 认证服务器展示给用户自己的授权界面。 用户进行授权操作，认证服务器验证成功后，生成一个授权编码 code，并跳转到第三方的回调 url。 第三方应用拿到 code 后，连同自己在平台上的身份信息（ID 密码）发送给认证服务器，再一次进行验证请求，说明自己的身份正确，并且用户也已经授权我了，来换取访问用户资源的权限。 认证服务器对请求信息进行验证，如果没问题，就生成访问资源服务器的令牌 access_token，交给第三方应用。 第三方应用使用 access_token 向资源服务器请求资源。 资源服务器验证 access_token 成功后返回响应资源。 2.4 OAuth2 的几种授权模式OAuth2.0 有这么几个授权模式：授权码模式、简化模式、密码模式、客户端凭证模式。 授权码模式：（authorization_code）是功能最完整、流程最严密的授权模式，code 保证了 token 的安全性，即使 code 被拦截，由于没有 client_secret，也是无法通过 code 获得 token 的。 简化模式：和授权码模式类似，只不过少了获取 code 的步骤，是直接获取令牌 token 的，适用于公开的浏览器单页应用，令牌直接从授权服务器返回，不支持刷新令牌，且没有 code 安全保证，令牌容易因为被拦截窃听而泄露。 密码模式：使用用户名/密码作为授权方式从授权服务器上获取令牌，一般不支持刷新令牌。 客户端凭证模式：一般用于资源服务器是应用的一个后端模块，客户端向认证服务器验证身份来获取令牌。 2.5 实战 OAuth2 的密码模式本次结合 Spring Cloud Alibaba 组件，实现微服务的安全系统体系，本文主要讲解 OAuth2 的部分。 先来看鉴权中心，鉴权中心需要做到提供单点服务，为所有的客户端微服务的安全保驾护航。下面首先看依赖: 12345678910111213141516171819&lt;dependency&gt;&lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt;&lt;artifactId&gt;spring-cloud-starter-oauth2&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;!-- 对redis支持,引入的话项目缓存就支持redis了,所以必须加上redis的相关配置,否则操作相关缓存会报异常 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt; 如果需要使用 redis 来存储 token，则可以加入 reids 依赖，如果使用 jwt，则使用: 12345&lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;&#x2F;groupId&gt; &lt;artifactId&gt;jjwt&lt;&#x2F;artifactId&gt; &lt;version&gt;0.9.0&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt; 当然，本次的项目模块引入的是比较新的 Spring Boot: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;&#x2F;artifactId&gt; &lt;version&gt;2.1.13.RELEASE&lt;&#x2F;version&gt; &lt;relativePath&#x2F;&gt;&lt;&#x2F;parent&gt;&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;&#x2F;project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;&#x2F;java.version&gt; &lt;swagger.version&gt;2.6.1&lt;&#x2F;swagger.version&gt; &lt;xstream.version&gt;1.4.7&lt;&#x2F;xstream.version&gt; &lt;pageHelper.version&gt;4.1.6&lt;&#x2F;pageHelper.version&gt; &lt;fastjson.version&gt;1.2.51&lt;&#x2F;fastjson.version&gt; &lt;springcloud.version&gt;Greenwich.SR3&lt;&#x2F;springcloud.version&gt; &lt;mysql.version&gt;5.1.46&lt;&#x2F;mysql.version&gt; &lt;alibaba-cloud.version&gt;2.1.1.RELEASE&lt;&#x2F;alibaba-cloud.version&gt; &lt;springcloud.alibaba.version&gt;0.9.0.RELEASE&lt;&#x2F;springcloud.alibaba.version&gt; &lt;&#x2F;properties&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;alibaba-cloud.version&#125;&lt;&#x2F;version&gt; &lt;type&gt;pom&lt;&#x2F;type&gt; &lt;scope&gt;import&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;!-- &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;springcloud.alibaba.version&#125;&lt;&#x2F;version&gt; &lt;type&gt;pom&lt;&#x2F;type&gt; &lt;scope&gt;import&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;springcloud.version&#125;&lt;&#x2F;version&gt; &lt;type&gt;pom&lt;&#x2F;type&gt; &lt;scope&gt;import&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt;&lt;&#x2F;dependencyManagement&gt; 剩下的，像数据库、持久化等，其他的可以根据需要添加。 配置完成后，我们需要写一个认证服务器的配置: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163package com.damon.config;import java.util.ArrayList;import java.util.List;import javax.sql.DataSource;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.context.annotation.Configuration;import org.springframework.core.env.Environment;import org.springframework.security.authentication.AuthenticationManager;import org.springframework.security.crypto.password.PasswordEncoder;import org.springframework.security.oauth2.config.annotation.configurers.ClientDetailsServiceConfigurer;import org.springframework.security.oauth2.config.annotation.web.configuration.AuthorizationServerConfigurerAdapter;import org.springframework.security.oauth2.config.annotation.web.configuration.EnableAuthorizationServer;import org.springframework.security.oauth2.config.annotation.web.configurers.AuthorizationServerEndpointsConfigurer;import org.springframework.security.oauth2.config.annotation.web.configurers.AuthorizationServerSecurityConfigurer;import org.springframework.security.oauth2.provider.error.WebResponseExceptionTranslator;import org.springframework.security.oauth2.provider.token.TokenEnhancer;import org.springframework.security.oauth2.provider.token.TokenEnhancerChain;import org.springframework.security.oauth2.provider.token.TokenStore;import org.springframework.security.oauth2.provider.token.store.JwtAccessTokenConverter;import com.damon.component.JwtTokenEnhancer;import com.damon.login.service.LoginService;@Configuration@EnableAuthorizationServerpublic class AuthorizationServerConfig extends AuthorizationServerConfigurerAdapter &#123; @Autowired private PasswordEncoder passwordEncoder; @Autowired private AuthenticationManager authenticationManager; @Autowired private LoginService loginService; @Autowired &#x2F;&#x2F;@Qualifier(&quot;jwtTokenStore&quot;) @Qualifier(&quot;redisTokenStore&quot;) private TokenStore tokenStore; &#x2F;*@Autowired private JwtAccessTokenConverter jwtAccessTokenConverter; @Autowired private JwtTokenEnhancer jwtTokenEnhancer;*&#x2F; @Autowired private Environment env; @Autowired private DataSource dataSource; @Autowired private WebResponseExceptionTranslator userOAuth2WebResponseExceptionTranslator; &#x2F;** * redis token 方式 *&#x2F; @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception &#123; &#x2F;&#x2F;验证时发生的情况处理 endpoints.authenticationManager(authenticationManager) &#x2F;&#x2F;支持 password 模式 .exceptionTranslator(userOAuth2WebResponseExceptionTranslator)&#x2F;&#x2F;自定义异常处理类添加到认证服务器配置 .userDetailsService(loginService) .tokenStore(tokenStore); &#125; &#x2F;** * 客户端配置（给谁发令牌） * 不同客户端配置不同 * * authorizedGrantTypes 可以包括如下几种设置中的一种或多种： authorization_code：授权码类型。需要redirect_uri implicit：隐式授权类型。需要redirect_uri password：资源所有者（即用户）密码类型。 client_credentials：客户端凭据（客户端ID以及Key）类型。 refresh_token：通过以上授权获得的刷新令牌来获取新的令牌。 accessTokenValiditySeconds：token 的有效期 scopes：用来限制客户端访问的权限，在换取的 token 的时候会带上 scope 参数，只有在 scopes 定义内的，才可以正常换取 token。 * @param clients * @throws Exception * @author Damon * *&#x2F; @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123; clients.inMemory() .withClient(&quot;provider-service&quot;) .secret(passwordEncoder.encode(&quot;provider-service-123&quot;)) .accessTokenValiditySeconds(3600) .refreshTokenValiditySeconds(864000)&#x2F;&#x2F;配置刷新token的有效期 .autoApprove(true) &#x2F;&#x2F;自动授权配置 .scopes(&quot;all&quot;)&#x2F;&#x2F;配置申请的权限范围 .authorizedGrantTypes(&quot;password&quot;, &quot;authorization_code&quot;, &quot;client_credentials&quot;, &quot;refresh_token&quot;)&#x2F;&#x2F;配置授权模式 .redirectUris(&quot;http:&#x2F;&#x2F;localhost:2001&#x2F;login&quot;)&#x2F;&#x2F;授权码模式开启后必须指定 .and() .withClient(&quot;consumer-service&quot;) .secret(passwordEncoder.encode(&quot;consumer-service-123&quot;)) .accessTokenValiditySeconds(3600) .refreshTokenValiditySeconds(864000)&#x2F;&#x2F;配置刷新token的有效期 .autoApprove(true) &#x2F;&#x2F;自动授权配置 .scopes(&quot;all&quot;)&#x2F;&#x2F;配置申请的权限范围 .authorizedGrantTypes(&quot;password&quot;, &quot;authorization_code&quot;, &quot;client_credentials&quot;, &quot;refresh_token&quot;)&#x2F;&#x2F;配置授权模式 .redirectUris(&quot;http:&#x2F;&#x2F;localhost:2005&#x2F;login&quot;)&#x2F;&#x2F;授权码模式开启后必须指定 .and() .withClient(&quot;resource-service&quot;) .secret(passwordEncoder.encode(&quot;resource-service-123&quot;)) .accessTokenValiditySeconds(3600) .refreshTokenValiditySeconds(864000)&#x2F;&#x2F;配置刷新token的有效期 .autoApprove(true) &#x2F;&#x2F;自动授权配置 .scopes(&quot;all&quot;)&#x2F;&#x2F;配置申请的权限范围 .authorizedGrantTypes(&quot;password&quot;, &quot;authorization_code&quot;, &quot;client_credentials&quot;, &quot;refresh_token&quot;)&#x2F;&#x2F;配置授权模式 .redirectUris(&quot;http:&#x2F;&#x2F;localhost:2006&#x2F;login&quot;)&#x2F;&#x2F;授权码模式开启后必须指定 .and() .withClient(&quot;test-sentinel&quot;) .secret(passwordEncoder.encode(&quot;test-sentinel-123&quot;)) .accessTokenValiditySeconds(3600) .refreshTokenValiditySeconds(864000)&#x2F;&#x2F;配置刷新token的有效期 .autoApprove(true) &#x2F;&#x2F;自动授权配置 .scopes(&quot;all&quot;)&#x2F;&#x2F;配置申请的权限范围 .authorizedGrantTypes(&quot;password&quot;, &quot;authorization_code&quot;, &quot;client_credentials&quot;, &quot;refresh_token&quot;)&#x2F;&#x2F;配置授权模式 .redirectUris(&quot;http:&#x2F;&#x2F;localhost:2008&#x2F;login&quot;)&#x2F;&#x2F;授权码模式开启后必须指定 .and() .withClient(&quot;test-sentinel-feign&quot;) .secret(passwordEncoder.encode(&quot;test-sentinel-feign-123&quot;)) .accessTokenValiditySeconds(3600) .refreshTokenValiditySeconds(864000)&#x2F;&#x2F;配置刷新token的有效期 .autoApprove(true) &#x2F;&#x2F;自动授权配置 .scopes(&quot;all&quot;)&#x2F;&#x2F;配置申请的权限范围 .authorizedGrantTypes(&quot;password&quot;, &quot;authorization_code&quot;, &quot;client_credentials&quot;, &quot;refresh_token&quot;)&#x2F;&#x2F;配置授权模式 .redirectUris(&quot;http:&#x2F;&#x2F;localhost:2010&#x2F;login&quot;)&#x2F;&#x2F;授权码模式开启后必须指定 .and() .withClient(&quot;customer-service&quot;) .secret(passwordEncoder.encode(&quot;customer-service-123&quot;)) .accessTokenValiditySeconds(3600) .refreshTokenValiditySeconds(864000)&#x2F;&#x2F;配置刷新token的有效期 .autoApprove(true) &#x2F;&#x2F;自动授权配置 .scopes(&quot;all&quot;) .authorizedGrantTypes(&quot;password&quot;, &quot;authorization_code&quot;, &quot;client_credentials&quot;, &quot;refresh_token&quot;)&#x2F;&#x2F;配置授权模式 .redirectUris(&quot;http:&#x2F;&#x2F;localhost:2012&#x2F;login&quot;)&#x2F;&#x2F;授权码模式开启后必须指定 ; &#125; @Override public void configure(AuthorizationServerSecurityConfigurer security) &#123; security.allowFormAuthenticationForClients();&#x2F;&#x2F;是允许客户端访问 OAuth2 授权接口，否则请求 token 会返回 401 security.checkTokenAccess(&quot;isAuthenticated()&quot;);&#x2F;&#x2F;是允许已授权用户访问 checkToken 接口 security.tokenKeyAccess(&quot;isAuthenticated()&quot;); &#x2F;&#x2F; security.tokenKeyAccess(&quot;permitAll()&quot;);获取密钥需要身份认证，使用单点登录时必须配置，是允许已授权用户获取 token 接口 &#125;&#125; Redis 配置: 123456789101112131415161718192021package com.damon.config;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.security.oauth2.provider.token.TokenStore;import org.springframework.security.oauth2.provider.token.store.redis.RedisTokenStore;@Configurationpublic class RedisTokenStoreConfig &#123; @Autowired private RedisConnectionFactory redisConnectionFactory; @Bean public TokenStore redisTokenStore ()&#123; &#x2F;&#x2F;return new RedisTokenStore(redisConnectionFactory); return new MyRedisTokenStore(redisConnectionFactory); &#125;&#125; 后面接下来需要配置安全访问的拦截，这时候需要 SpringSecurity: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.damon.config;import javax.servlet.http.HttpServletResponse;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.security.authentication.AuthenticationManager;import org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.annotation.web.builders.WebSecurity;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;import org.springframework.security.crypto.password.PasswordEncoder;@Configuration@EnableWebSecuritypublic class SecurityConfig extends WebSecurityConfigurerAdapter &#123; @Bean public PasswordEncoder passwordEncoder() &#123; return new BCryptPasswordEncoder(); &#125; @Bean @Override public AuthenticationManager authenticationManagerBean() throws Exception &#123; return super.authenticationManagerBean(); &#125; @Override public void configure(HttpSecurity http) throws Exception &#123; http.csrf() .disable() .exceptionHandling() .authenticationEntryPoint(new AuthenticationEntryPointHandle()) &#x2F;&#x2F;.authenticationEntryPoint((request, response, authException) -&gt; response.sendError(HttpServletResponse.SC_UNAUTHORIZED)) .and() .authorizeRequests() .antMatchers(&quot;&#x2F;oauth&#x2F;**&quot;, &quot;&#x2F;login&#x2F;**&quot;)&#x2F;&#x2F;&quot;&#x2F;logout&#x2F;**&quot; .permitAll() .anyRequest() .authenticated() .and() .formLogin() .permitAll(); &#125; &#x2F;*@Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth.userDetailsService(userDetailsService) .passwordEncoder(passwordEncoder()); &#125;*&#x2F; @Override public void configure(WebSecurity web) throws Exception &#123; web.ignoring().antMatchers(&quot;&#x2F;css&#x2F;**&quot;, &quot;&#x2F;js&#x2F;**&quot;, &quot;&#x2F;plugins&#x2F;**&quot;, &quot;&#x2F;favicon.ico&quot;); &#125;&#125; 再者，就是需要配置资源拦截: 12345678910111213141516171819202122232425262728293031package com.damon.config;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.oauth2.config.annotation.web.configuration.EnableResourceServer;import org.springframework.security.oauth2.config.annotation.web.configuration.ResourceServerConfigurerAdapter;@Configuration@EnableResourceServerpublic class ResourceServerConfig extends ResourceServerConfigurerAdapter &#123; @Override public void configure(HttpSecurity http) throws Exception &#123; http.csrf().disable() .exceptionHandling() .authenticationEntryPoint(new AuthenticationEntryPointHandle()) &#x2F;&#x2F;.authenticationEntryPoint((request, response, authException) -&gt; response.sendError(HttpServletResponse.SC_UNAUTHORIZED)) .and() .requestMatchers().antMatchers(&quot;&#x2F;api&#x2F;**&quot;) .and() .authorizeRequests() .antMatchers(&quot;&#x2F;api&#x2F;**&quot;).authenticated() .and() .httpBasic(); &#125;&#125; 其中，在上面我们配置了资源拦截、权限拦截的统一处理配置: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.damon.config;import java.io.IOException;import javax.servlet.ServletException;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import org.springframework.http.HttpStatus;import org.springframework.security.core.AuthenticationException;import org.springframework.security.web.AuthenticationEntryPoint;import com.alibaba.fastjson.JSON;import com.damon.commons.Response;&#x2F;** * * 统一结果处理 * * @author Damon * *&#x2F;public class AuthenticationEntryPointHandle implements AuthenticationEntryPoint &#123; &#x2F;** * * @author Damon * *&#x2F; @Override public void commence(HttpServletRequest request, HttpServletResponse response, AuthenticationException authException) throws IOException, ServletException &#123; &#x2F;&#x2F;response.setStatus(HttpServletResponse.SC_FORBIDDEN); &#x2F;&#x2F;response.setStatus(HttpStatus.OK.value()); &#x2F;&#x2F;response.setHeader(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;); &#x2F;&#x2F;gateway已加，无需再加 &#x2F;&#x2F;response.setHeader(&quot;Access-Control-Allow-Headers&quot;, &quot;token&quot;); &#x2F;&#x2F;解决低危漏洞点击劫持 X-Frame-Options Header未配置 response.setHeader(&quot;X-Frame-Options&quot;, &quot;SAMEORIGIN&quot;); response.setCharacterEncoding(&quot;UTF-8&quot;); response.setContentType(&quot;application&#x2F;json; charset&#x3D;utf-8&quot;); response.getWriter() .write(JSON.toJSONString(Response.ok(response.getStatus(), -2, authException.getMessage(), null))); &#x2F;*response.getWriter() .write(JSON.toJSONString(Response.ok(200, -2, &quot;Internal Server Error&quot;, authException.getMessage())));*&#x2F; &#125;&#125; 最后，自定义异常处理类添加到认证服务器配置: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126package com.damon.config;import java.io.IOException;import org.springframework.http.HttpHeaders;import org.springframework.http.HttpStatus;import org.springframework.http.ResponseEntity;import org.springframework.security.access.AccessDeniedException;import org.springframework.security.core.AuthenticationException;import org.springframework.security.oauth2.common.DefaultThrowableAnalyzer;import org.springframework.security.oauth2.common.exceptions.InsufficientScopeException;import org.springframework.security.oauth2.common.exceptions.OAuth2Exception;import org.springframework.security.oauth2.provider.error.WebResponseExceptionTranslator;import org.springframework.security.web.util.ThrowableAnalyzer;import org.springframework.stereotype.Component;import org.springframework.web.HttpRequestMethodNotSupportedException;import com.damon.exception.UserOAuth2Exception;&#x2F;** * * 自定义异常转换类 * @author Damon * *&#x2F;@Component(&quot;userOAuth2WebResponseExceptionTranslator&quot;)public class UserOAuth2WebResponseExceptionTranslator implements WebResponseExceptionTranslator &#123; private ThrowableAnalyzer throwableAnalyzer &#x3D; new DefaultThrowableAnalyzer(); @Override public ResponseEntity&lt;OAuth2Exception&gt; translate(Exception e) throws Exception &#123; Throwable[] causeChain &#x3D; this.throwableAnalyzer.determineCauseChain(e); Exception ase &#x3D; (OAuth2Exception)this.throwableAnalyzer.getFirstThrowableOfType(OAuth2Exception.class, causeChain); &#x2F;&#x2F;异常链中有OAuth2Exception异常 if (ase !&#x3D; null) &#123; return this.handleOAuth2Exception((OAuth2Exception)ase); &#125; &#x2F;&#x2F;身份验证相关异常 ase &#x3D; (AuthenticationException)this.throwableAnalyzer.getFirstThrowableOfType(AuthenticationException.class, causeChain); if (ase !&#x3D; null) &#123; return this.handleOAuth2Exception(new UserOAuth2WebResponseExceptionTranslator.UnauthorizedException(e.getMessage(), e)); &#125; &#x2F;&#x2F;异常链中包含拒绝访问异常 ase &#x3D; (AccessDeniedException)this.throwableAnalyzer.getFirstThrowableOfType(AccessDeniedException.class, causeChain); if (ase instanceof AccessDeniedException) &#123; return this.handleOAuth2Exception(new UserOAuth2WebResponseExceptionTranslator.ForbiddenException(ase.getMessage(), ase)); &#125; &#x2F;&#x2F;异常链中包含Http方法请求异常 ase &#x3D; (HttpRequestMethodNotSupportedException)this.throwableAnalyzer.getFirstThrowableOfType(HttpRequestMethodNotSupportedException.class, causeChain); if(ase instanceof HttpRequestMethodNotSupportedException)&#123; return this.handleOAuth2Exception(new UserOAuth2WebResponseExceptionTranslator.MethodNotAllowed(ase.getMessage(), ase)); &#125; return this.handleOAuth2Exception(new UserOAuth2WebResponseExceptionTranslator.ServerErrorException(HttpStatus.INTERNAL_SERVER_ERROR.getReasonPhrase(), e)); &#125; private ResponseEntity&lt;OAuth2Exception&gt; handleOAuth2Exception(OAuth2Exception e) throws IOException &#123; int status &#x3D; e.getHttpErrorCode(); HttpHeaders headers &#x3D; new HttpHeaders(); headers.set(&quot;Cache-Control&quot;, &quot;no-store&quot;); headers.set(&quot;Pragma&quot;, &quot;no-cache&quot;); if (status &#x3D;&#x3D; HttpStatus.UNAUTHORIZED.value() || e instanceof InsufficientScopeException) &#123; headers.set(&quot;WWW-Authenticate&quot;, String.format(&quot;%s %s&quot;, &quot;Bearer&quot;, e.getSummary())); &#125; UserOAuth2Exception exception &#x3D; new UserOAuth2Exception(e.getMessage(),e); ResponseEntity&lt;OAuth2Exception&gt; response &#x3D; new ResponseEntity(exception, headers, HttpStatus.valueOf(status)); return response; &#125; private static class MethodNotAllowed extends OAuth2Exception &#123; public MethodNotAllowed(String msg, Throwable t) &#123; super(msg, t); &#125; @Override public String getOAuth2ErrorCode() &#123; return &quot;method_not_allowed&quot;; &#125; @Override public int getHttpErrorCode() &#123; return 405; &#125; &#125; private static class UnauthorizedException extends OAuth2Exception &#123; public UnauthorizedException(String msg, Throwable t) &#123; super(msg, t); &#125; @Override public String getOAuth2ErrorCode() &#123; return &quot;unauthorized&quot;; &#125; @Override public int getHttpErrorCode() &#123; return 401; &#125; &#125; private static class ServerErrorException extends OAuth2Exception &#123; public ServerErrorException(String msg, Throwable t) &#123; super(msg, t); &#125; @Override public String getOAuth2ErrorCode() &#123; return &quot;server_error&quot;; &#125; @Override public int getHttpErrorCode() &#123; return 500; &#125; &#125; private static class ForbiddenException extends OAuth2Exception &#123; public ForbiddenException(String msg, Throwable t) &#123; super(msg, t); &#125; @Override public String getOAuth2ErrorCode() &#123; return &quot;access_denied&quot;; &#125; @Override public int getHttpErrorCode() &#123; return 403; &#125; &#125;&#125; 最后，我们可能需要配置一些请求客户端的配置，以及变量配置: 123456789101112131415@Configurationpublic class BeansConfig &#123; @Resource private Environment env; @Bean public RestTemplate restTemplate() &#123; SimpleClientHttpRequestFactory requestFactory &#x3D; new SimpleClientHttpRequestFactory(); requestFactory.setReadTimeout(env.getProperty(&quot;client.http.request.readTimeout&quot;, Integer.class, 15000)); requestFactory.setConnectTimeout(env.getProperty(&quot;client.http.request.connectTimeout&quot;, Integer.class, 3000)); RestTemplate rt &#x3D; new RestTemplate(requestFactory); return rt; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package com.damon.config;import org.springframework.beans.factory.annotation.Value;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.cloud.context.config.annotation.RefreshScope;import org.springframework.context.annotation.Configuration;import org.springframework.stereotype.Component;&#x2F;** * 配置信息 * @author Damon * *&#x2F;@Component@RefreshScopepublic class EnvConfig &#123; @Value(&quot;$&#123;jdbc.driverClassName:&#125;&quot;) private String jdbc_driverClassName; @Value(&quot;$&#123;jdbc.url:&#125;&quot;) private String jdbc_url; @Value(&quot;$&#123;jdbc.username:&#125;&quot;) private String jdbc_username; @Value(&quot;$&#123;jdbc.password:&#125;&quot;) private String jdbc_password; public String getJdbc_driverClassName() &#123; return jdbc_driverClassName; &#125; public void setJdbc_driverClassName(String jdbc_driverClassName) &#123; this.jdbc_driverClassName &#x3D; jdbc_driverClassName; &#125; public String getJdbc_url() &#123; return jdbc_url; &#125; public void setJdbc_url(String jdbc_url) &#123; this.jdbc_url &#x3D; jdbc_url; &#125; public String getJdbc_username() &#123; return jdbc_username; &#125; public void setJdbc_username(String jdbc_username) &#123; this.jdbc_username &#x3D; jdbc_username; &#125; public String getJdbc_password() &#123; return jdbc_password; &#125; public void setJdbc_password(String jdbc_password) &#123; this.jdbc_password &#x3D; jdbc_password; &#125;&#125; 最后需要配置一些环境配置: 12345678910111213141516171819202122spring: application: name: oauth-cas cloud: nacos: discovery: server-addr: 127.0.0.1:8848 config: server-addr: 127.0.0.1:8848 refreshable-dataids: actuator.properties,log.properties redis: #redis相关配置 database: 8 host: 127.0.0.1 #localhost port: 6379 password: aaa #有密码时设置 jedis: pool: max-active: 8 max-idle: 8 min-idle: 0 timeout: 10000ms 记住：上面这个启动配置需要在 bootstrap 文件中添加，否则，可能会失败，大家可以尝试下。 1234567891011121314151617181920212223242526272829server: port: 2000 undertow: uri-encoding: UTF-8 accesslog: enabled: false pattern: combined #这里我们使用了SpringBoot2.x，注意session与1.x不同 servlet: session: timeout: PT120M cookie: name: OAUTH-CAS-SESSIONID #防止Cookie冲突，冲突会导致登录验证不通过client: http: request: connectTimeout: 8000 readTimeout: 30000mybatis: mapperLocations: classpath:mapper&#x2F;*.xml typeAliasesPackage: com.damon.*.modelspring: profiles: active: dev 最后，我们添加启动类: 123456789@Configuration@EnableAutoConfiguration@ComponentScan(basePackages &#x3D; &#123;&quot;com.damon&quot;&#125;)@EnableDiscoveryClientpublic class CasApp &#123; public static void main(String[] args) &#123; SpringApplication.run(CasApp.class, args); &#125;&#125; 以上，一个认证中心的代码实战逻辑就完成了。 接下来，我们看一个客户端如何去认证，首先还是依赖: 123456789101112131415161718192021222324&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-oauth2&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt; 在客户端，我们也需要配置一个资源配置与权限配置: 12345678910111213141516171819202122232425262728293031323334package com.damon.config;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.oauth2.config.annotation.web.configuration.EnableResourceServer;import org.springframework.security.oauth2.config.annotation.web.configuration.ResourceServerConfigurerAdapter;&#x2F;** * * * @author Damon * *&#x2F;@Configuration@EnableResourceServerpublic class ResourceServerConfig extends ResourceServerConfigurerAdapter &#123; @Override public void configure(HttpSecurity http) throws Exception &#123; http.csrf().disable() .exceptionHandling() .authenticationEntryPoint(new AuthenticationEntryPointHandle()) &#x2F;&#x2F;.authenticationEntryPoint((request, response, authException) -&gt; response.sendError(HttpServletResponse.SC_UNAUTHORIZED)) .and() .requestMatchers().antMatchers(&quot;&#x2F;api&#x2F;**&quot;) .and() .authorizeRequests() .antMatchers(&quot;&#x2F;api&#x2F;**&quot;).authenticated() .and() .httpBasic(); &#125;&#125; 当然，权限拦截可能就相对简单了: 123456789101112131415161718package com.damon.config;import org.springframework.context.annotation.Configuration;import org.springframework.core.annotation.Order;import org.springframework.security.config.annotation.method.configuration.EnableGlobalMethodSecurity;import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;&#x2F;** * * 在接口上配置权限时使用 * @author Damon * *&#x2F;@Configuration@EnableGlobalMethodSecurity(prePostEnabled &#x3D; true)@Order(101)public class SecurityConfig extends WebSecurityConfigurerAdapter &#123;&#125; 同样，这里也需要一个统一结果处理类，这里就不展示了。 接下来，我们主要看配置: 123456789101112131415161718192021cas-server-url: http:&#x2F;&#x2F;oauth-cas #http:&#x2F;&#x2F;localhost:2000#设置可以访问的地址security: oauth2: #与cas对应的配置 client: client-id: provider-service client-secret: provider-service-123 user-authorization-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;authorize #是授权码认证方式需要的 access-token-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;token #是密码模式需要用到的获取 token 的接口 resource: loadBalanced: true #jwt: #jwt存储token时开启 #key-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;token_key #key-value: test_jwt_sign_key id: provider-service #指定用户信息地址 user-info-uri: $&#123;cas-server-url&#125;&#x2F;api&#x2F;user #指定user info的URI，原生地址后缀为&#x2F;auth&#x2F;user prefer-token-info: false #token-info-uri: authorization: check-token-access: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;check_token #当此web服务端接收到来自UI客户端的请求后，需要拿着请求中的 token 到认证服务端做 token 验证，就是请求的这个接口 在上面的配置里，我们看到了各种注释了，讲得很仔细，但是我要强调下：为了高可用，我们的认证中心可能多个，所以需要域名来作 LB。同时，开启了 loadBalanced=true。最后，如果是授权码认证模式，则需要 “user-authorization-uri”，如果是密码模式，需要 “access-token-uri” 来获取 token。我们通过它 “user-info-uri” 来获取认证中心的用户信息，从而判断该用户的权限，从而访问相应的资源。另外，上面的配置需要在 bootstrap 文件中，否则可能失败，大家可以试试。 接下来，我们添加一般配置: 12345678910111213141516171819202122232425262728293031323334353637server: port: 2001 undertow: uri-encoding: UTF-8 accesslog: enabled: false pattern: combined servlet: session: timeout: PT120M cookie: name: PROVIDER-SERVICE-SESSIONID #防止Cookie冲突，冲突会导致登录验证不通过backend: ribbon: client: enabled: true ServerListRefreshInterval: 5000ribbon: ConnectTimeout: 3000 # 设置全局默认的ribbon的读超时 ReadTimeout: 1000 eager-load: enabled: true clients: oauth-cas,consumer-service MaxAutoRetries: 1 #对第一次请求的服务的重试次数 MaxAutoRetriesNextServer: 1 #要重试的下一个服务的最大数量（不包括第一个服务） #listOfServers: localhost:5556,localhost:5557 #ServerListRefreshInterval: 2000 OkToRetryOnAllOperations: true NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RoundRobinRulehystrix.command.BackendCall.execution.isolation.thread.timeoutInMilliseconds: 5000hystrix.threadpool.BackendCallThread.coreSize: 5 这里，我们使用了 Ribbon 来做 LB，hystrix 来作熔断，最后需要注意的是：加上了 cookie name，防止 Cookie 冲突，冲突会导致登录验证不通过。 配置启动类: 123456789101112@Configuration@EnableAutoConfiguration@ComponentScan(basePackages &#x3D; &#123;&quot;com.damon&quot;&#125;)@EnableDiscoveryClient@EnableOAuth2Ssopublic class ProviderApp &#123; public static void main(String[] args) &#123; SpringApplication.run(ProviderApp.class, args); &#125;&#125; 我们在上面配置了所有带有 “/api/**“ 的路径请求，都会加以拦截，根据用户的信息来判断其是否有权限访问。 写一个简单的测试类: 12345678910111213141516@RestController@RequestMapping(&quot;&#x2F;api&#x2F;user&quot;)public class UserController &#123; private static final Logger logger &#x3D; LoggerFactory.getLogger(UserController.class); @Autowired private UserService userService; @PreAuthorize(&quot;hasAuthority(&#39;admin&#39;)&quot;) @GetMapping(&quot;&#x2F;auth&#x2F;admin&quot;) public Object adminAuth() &#123; logger.info(&quot;test password mode&quot;); return &quot;Has admin auth!&quot;; &#125;&#125; 上面的代码表示：如果用户具有 “admin” 的权限，则能够访问该接口，否则会被拒绝。 本文用的是 alibaba 的组件来作 LB，具体可以看前面的文章，用域名来找到服务。同时也加上了网关 Gateway。 最后，我们先来通过密码模式来进行认证吧: 1curl -i -X POST -d &quot;username&#x3D;admin&amp;password&#x3D;123456&amp;grant_type&#x3D;password&amp;client_id&#x3D;provider-service&amp;client_secret&#x3D;provider-service-123&quot; http:&#x2F;&#x2F;localhost:5555&#x2F;oauth-cas&#x2F;oauth&#x2F;token 认证成功后，会返回如下结果: 1&#123;&quot;access_token&quot;:&quot;d2066f68-665b-4038-9dbe-5dd1035e75a0&quot;,&quot;token_type&quot;:&quot;bearer&quot;,&quot;refresh_token&quot;:&quot;44009836-731c-4e6a-9cc3-274ce3af8c6b&quot;,&quot;expires_in&quot;:3599,&quot;scope&quot;:&quot;all&quot;&#125; 接下来，我们通过 token 来访问接口: 1curl -i -H &quot;Accept: application&#x2F;json&quot; -H &quot;Authorization:bearer d2066f68-665b-4038-9dbe-5dd1035e75a0&quot; -X GET http:&#x2F;&#x2F;localhost:5555&#x2F;provider-service&#x2F;api&#x2F;user&#x2F;auth&#x2F;admin 成功会返回结果: 1Has admin auth! token 如果失效，会返回: 1&#123;&quot;error&quot;:&quot;invalid_token&quot;,&quot;error_description&quot;:&quot;d2066f68-665b-4038-9dbe-5dd1035e75a01&quot;&#125; 3、GitHub 的授权应用案例 如果你的应用想要接入 GitHub，则可以通过如下办法来实现。 首先注册一个 GitHub 账号，登陆后，找到设置，打开页面，最下面有一个开发者设置： 找到后，点击，可以看到三个，可以选择第二个方式来接入： 可以新增你的应用 app，新建时，应用名、回调地址必填项： 最后，完成后会生成一个 Client ID、Client Secret。 然后利用 Github 官方给的文档来进行认证、接入，授权逻辑： 1.在注册完信息后生成了 Client ID、Client Secret，首先，用户点击 github 登录本地应用引导用户跳转到第三方授权页跳转地址: 1https:&#x2F;&#x2F;github.com&#x2F;login&#x2F;oauth&#x2F;authorize?client_id&#x3D;&#123;client_id&#125;&amp;redirect_uri&#x3D;&#123;redirect_uri&#125;&amp;state&#x3D;&#123;state&#125; 其中，client_id，client_secret 是注册好 Oauth APP 后 github 提供的，需要写在本地代码或者配置文件中，state 也是在本地生成的。redirect_uri 就是在 GitHub 官网填的 Authorization callback URL。此时带着 state 等参数去申请授权，但此时尚未登陆，未能通过 authorize，GitHub 返回 code 参数。 2.授权成功后会重定向带参数访问上面的 redirect_uri，并多了一个 code 参数后台接收 code 这个参数,我们带着这个 code 再次访问 github 地址: 1https:&#x2F;&#x2F;github.com&#x2F;login&#x2F;oauth&#x2F;access_token?client_id&#x3D;xxx&amp;client_secret&#x3D;xxx&amp;code&#x3D;xxx&amp;redirect_uri&#x3D;http:&#x2F;&#x2F;localhost:3001&#x2F;authCallback 注意：上面的 redirect_uri 要与之前在新建 app 时填写的保持一直，否则会报错。 3.通过 state 参数和 code 参数，成功获取 access_token有了 access_token，只需要把 access_token 参数放在 URL 后面即可，就可以换取用户信息了。访问地址: 1https:&#x2F;&#x2F;api.github.com&#x2F;user?access_token&#x3D;xxx 4.得到 GitHub 授权用户的个人信息，就表明授权成功。 4、微服务安全架构设计 在微服务中，安全性是一个很重要的问题。我们经常比较多的场景是：服务 A 需要调用服务 B，但是问题来了，到底是走外网调用呢？还是走局域网调用呢？这当然看 A、B 是否在同一个网段，如果在同一个局域网段，那肯定走局域网好。为什么呢？因为局域网快呀，如果说还有理由吗？当然有：除了网络快，降低网络开销，还可以保证安全性，不至于被黑客黑掉。这是安全的一个保证。 那么除了上面说的安全性，还有其他的吗？比如：在一个局域网下，有 N 个微服务模块，但是这些微服务并不想完全直接暴露给外部，这时候，就需要一个网关 Gateway 来处理。网关把所有的服务给路由了，就像在所有的服务上面一层，加了一个保护光环，突出高内聚的含义。同时还可以加上一些拦截，安全的拦截，鉴权、认证等。存在通过 token 的鉴权，也可以通过 jwt 的，等等。有时候，可以借助 redis 通过 session 共享。也可以通过 OAuth2 的鉴权模式来实现安全拦截。 最后安全性的考虑是在每个服务的接口设计上，比如：幂等的存在，让很多恶意攻击成为无用之功。更多的介绍可以看下面: 1https:&#x2F;&#x2F;mp.weixin.qq.com&#x2F;s&#x2F;G3yhwvLVTu_T5uPxgZD00w 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"k8s master机器文件系统故障的一次恢复过程","date":"2020-07-23T07:23:59.000Z","path":"2020/07/23/k8s-01/","text":"研发反馈他们那边一套集群有台master文件系统损坏无法开机，他们是三台openstack上的虚机，是虚拟化宿主机故障导致的虚机文件系统损坏。三台机器是master+node，指导他修复后开机，修复过程和我之前文章opensuse的一次救援步骤一样 起来后我上去看，因为做了 HA 的，所以只有这个node有问题，集群没影响 12345[root@k8s-m1 ~]# kubectl get node -o wideNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME10.252.146.104 NotReady &lt;none&gt; 30d v1.16.9 10.252.146.104 &lt;none&gt; CentOS Linux 8 (Core) 4.18.0-193.6.3.el8_2.x86_64 docker:&#x2F;&#x2F;19.3.1110.252.146.105 Ready &lt;none&gt; 30d v1.16.9 10.252.146.105 &lt;none&gt; CentOS Linux 8 (Core) 4.18.0-193.6.3.el8_2.x86_64 docker:&#x2F;&#x2F;19.3.1110.252.146.106 Ready &lt;none&gt; 30d v1.16.9 10.252.146.106 &lt;none&gt; CentOS Linux 8 (Core) 4.18.0-193.6.3.el8_2.x86_64 docker:&#x2F;&#x2F;19.3.11 启动docker试试 12[root@k8s-m1 ~]# systemctl start dockerJob for docker.service canceled. 无法启动，查看下启动失败的服务 123[root@k8s-m1 ~]# systemctl --failed UNIT LOAD ACTIVE SUB DESCRIPTION● containerd.service loaded failed failed containerd container runtime 查看下containerd的日志 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@k8s-m1 ~]# journalctl -xe -u containerdJul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.481459735+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.service.v1.snapshots-service&quot;...&quot; type&#x3D;io.containerd.service.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.481472223+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.runtime.v1.linux&quot;...&quot; type&#x3D;io.containerd.runtime.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.481517630+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.runtime.v2.task&quot;...&quot; type&#x3D;io.containerd.runtime.v2Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.481562176+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.monitor.v1.cgroups&quot;...&quot; type&#x3D;io.containerd.monitor.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.481964349+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.service.v1.tasks-service&quot;...&quot; type&#x3D;io.containerd.service.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.481996158+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.internal.v1.restart&quot;...&quot; type&#x3D;io.containerd.internal.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482048208+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.containers&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482081110+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.content&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482096598+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.diff&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482112263+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.events&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482123307+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.healthcheck&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482133477+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.images&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482142943+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.leases&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482151644+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.namespaces&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482160741+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.internal.v1.opt&quot;...&quot; type&#x3D;io.containerd.internal.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482184201+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.snapshots&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482194643+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.tasks&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482206871+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.version&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482215454+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.introspection&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482365838+08:00&quot; level&#x3D;info msg&#x3D;serving... address&#x3D;&quot;&#x2F;run&#x2F;containerd&#x2F;containerd.sock&quot;Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482404139+08:00&quot; level&#x3D;info msg&#x3D;&quot;containerd successfully booted in 0.003611s&quot;Jul 23 11:20:11 k8s-m1 containerd[9186]: panic: runtime error: invalid memory address or nil pointer dereferenceJul 23 11:20:11 k8s-m1 containerd[9186]: [signal SIGSEGV: segmentation violation code&#x3D;0x1 addr&#x3D;0x8 pc&#x3D;0x5626b983c259]Jul 23 11:20:11 k8s-m1 containerd[9186]: goroutine 55 [running]:Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*Bucket).Cursor(...)Jul 23 11:20:11 k8s-m1 containerd[9186]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;bucket.go:84Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*Bucket).Get(0x0, 0x5626bb7e3f10, 0xb, 0xb, 0x0, 0x2, 0x4)Jul 23 11:20:11 k8s-m1 containerd[9186]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;bucket.go:260 +0x39Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata.scanRoots.func6(0x7fe557c63020, 0x2, 0x2, 0x0, 0x0, 0x0, 0x0, 0x5626b95eec72)Jul 23 11:20:11 k8s-m1 containerd[9186]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata&#x2F;gc.go:222 +0xcbJul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*Bucket).ForEach(0xc0003d1780, 0xc00057b640, 0xa, 0xa)Jul 23 11:20:11 k8s-m1 containerd[9186]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;bucket.go:388 +0x100Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata.scanRoots(0x5626bacedde0, 0xc0003d1680, 0xc0002ee2a0, 0xc00031a3c0, 0xc000527a60, 0x7fe586a43fff)Jul 23 11:20:11 k8s-m1 containerd[9186]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata&#x2F;gc.go:216 +0x4dfJul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata.(*DB).getMarked.func1(0xc0002ee2a0, 0x0, 0x0)Jul 23 11:20:11 k8s-m1 containerd[9186]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata&#x2F;db.go:359 +0x165Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*DB).View(0xc00000c1e0, 0xc00008b860, 0x0, 0x0)Jul 23 11:20:11 k8s-m1 containerd[9186]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;db.go:701 +0x92Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata.(*DB).getMarked(0xc0000a0a80, 0x5626bacede20, 0xc0000d6010, 0x203000, 0x203000, 0x400)Jul 23 11:20:11 k8s-m1 containerd[9186]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata&#x2F;db.go:342 +0x7eJul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata.(*DB).GarbageCollect(0xc0000a0a80, 0x5626bacede20, 0xc0000d6010, 0x0, 0x1, 0x0, 0x0)Jul 23 11:20:11 k8s-m1 containerd[9186]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata&#x2F;db.go:257 +0xa3Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;gc&#x2F;scheduler.(*gcScheduler).run(0xc0000a0b40, 0x5626bacede20, 0xc0000d6010)Jul 23 11:20:11 k8s-m1 containerd[9186]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;gc&#x2F;scheduler&#x2F;scheduler.go:310 +0x511Jul 23 11:20:11 k8s-m1 containerd[9186]: created by github.com&#x2F;containerd&#x2F;containerd&#x2F;gc&#x2F;scheduler.init.0.func1Jul 23 11:20:11 k8s-m1 containerd[9186]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;gc&#x2F;scheduler&#x2F;scheduler.go:132 +0x462Jul 23 11:20:11 k8s-m1 systemd[1]: containerd.service: Main process exited, code&#x3D;exited, status&#x3D;2&#x2F;INVALIDARGUMENTJul 23 11:20:11 k8s-m1 systemd[1]: containerd.service: Failed with result &#39;exit-code&#39;. 这个问题从panic抛出的堆栈信息看和我之前文章docker启动panic很类似，都是 boltdb 文件出错，找下 git 信息去看看代码路径在哪 123456[root@k8s-m1 ~]# systemctl cat containerd | grep ExecStartExecStartPre&#x3D;-&#x2F;sbin&#x2F;modprobe overlayExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;containerd[root@k8s-m1 ~]# &#x2F;usr&#x2F;bin&#x2F;containerd --versioncontainerd containerd.io 1.2.13 7ad184331fa3e55e52b890ea95e65ba581ae3429 按照这个blob去用github的url访问是404，只有去按照tag版本查看了，根据相关代码找到了 boltdb 的文件名是meta.db 123https:&#x2F;&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;blob&#x2F;v1.2.13&#x2F;metadata&#x2F;db.go#L257https:&#x2F;&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;blob&#x2F;v1.2.13&#x2F;metadata&#x2F;db.go#L79https:&#x2F;&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;blob&#x2F;v1.2.13&#x2F;services&#x2F;server&#x2F;server.go#L261-L268 查找下ic.Root路径是多少 12345678[root@k8s-m1 ~]# &#x2F;usr&#x2F;bin&#x2F;containerd --help | grep config config information on the containerd config --config value, -c value path to the configuration file (default: &quot;&#x2F;etc&#x2F;containerd&#x2F;config.toml&quot;)[root@k8s-m1 ~]# grep root &#x2F;etc&#x2F;containerd&#x2F;config.toml#root &#x3D; &quot;&#x2F;var&#x2F;lib&#x2F;containerd&quot;[root@k8s-m1 ~]]# find &#x2F;var&#x2F;lib&#x2F;containerd -type f -name meta.db&#x2F;var&#x2F;lib&#x2F;containerd&#x2F;io.containerd.metadata.v1.bolt&#x2F;meta.db 找到boltdb文件，改名启动 12345678910111213141516171819202122232425262728293031323334353637383940414243[root@k8s-m1 ~]]# mv &#x2F;var&#x2F;lib&#x2F;containerd&#x2F;io.containerd.metadata.v1.bolt&#x2F;meta.db&#123;,.bak&#125;[root@k8s-m1 ~]# systemctl status containerd.service● containerd.service - containerd container runtime Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;containerd.service; disabled; vendor preset: disabled) Active: failed (Result: exit-code) since Thu 2020-07-23 11:20:11 CST; 17min ago Docs: https:&#x2F;&#x2F;containerd.io Process: 9186 ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;containerd (code&#x3D;exited, status&#x3D;2) Process: 9182 ExecStartPre&#x3D;&#x2F;sbin&#x2F;modprobe overlay (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS) Main PID: 9186 (code&#x3D;exited, status&#x3D;2)Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata.(*DB).getMarked(0xc0000a0a80, 0x5626bacede20, 0xc0000d6010, 0x203000, 0x203000, 0x400)Jul 23 11:20:11 k8s-m1 containerd[9186]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata&#x2F;db.go:342 +0x7eJul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata.(*DB).GarbageCollect(0xc0000a0a80, 0x5626bacede20, 0xc0000d6010, 0x0, 0x1, 0x0, 0x0)Jul 23 11:20:11 k8s-m1 containerd[9186]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata&#x2F;db.go:257 +0xa3Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;gc&#x2F;scheduler.(*gcScheduler).run(0xc0000a0b40, 0x5626bacede20, 0xc0000d6010)Jul 23 11:20:11 k8s-m1 containerd[9186]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;gc&#x2F;scheduler&#x2F;scheduler.go:310 +0x511Jul 23 11:20:11 k8s-m1 containerd[9186]: created by github.com&#x2F;containerd&#x2F;containerd&#x2F;gc&#x2F;scheduler.init.0.func1Jul 23 11:20:11 k8s-m1 containerd[9186]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;gc&#x2F;scheduler&#x2F;scheduler.go:132 +0x462Jul 23 11:20:11 k8s-m1 systemd[1]: containerd.service: Main process exited, code&#x3D;exited, status&#x3D;2&#x2F;INVALIDARGUMENTJul 23 11:20:11 k8s-m1 systemd[1]: containerd.service: Failed with result &#39;exit-code&#39;.[root@k8s-m1 ~]# systemctl restart containerd.service[root@k8s-m1 ~]# systemctl status containerd.service● containerd.service - containerd container runtime Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;containerd.service; disabled; vendor preset: disabled) Active: active (running) since Thu 2020-07-23 11:25:37 CST; 1s ago Docs: https:&#x2F;&#x2F;containerd.io Process: 15661 ExecStartPre&#x3D;&#x2F;sbin&#x2F;modprobe overlay (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS) Main PID: 15663 (containerd) Tasks: 16 Memory: 28.6M CGroup: &#x2F;system.slice&#x2F;containerd.service └─15663 &#x2F;usr&#x2F;bin&#x2F;containerdJul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496725460+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.images&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496734129+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.leases&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496742793+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.namespaces&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496751740+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.internal.v1.opt&quot;...&quot; type&#x3D;io.containerd.internal.v1Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496775185+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.snapshots&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496785498+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.tasks&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496794873+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.version&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496803178+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.introspection&quot;...&quot; type&#x3D;io.containerd.grpc.v1Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496944458+08:00&quot; level&#x3D;info msg&#x3D;serving... address&#x3D;&quot;&#x2F;run&#x2F;containerd&#x2F;containerd.sock&quot;Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496958031+08:00&quot; level&#x3D;info msg&#x3D;&quot;containerd successfully booted in 0.003994s&quot; containerd 起来后，启动下 docker 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@k8s-m1 ~]# systemctl status docker● docker.service - Docker Application Container Engine Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service; enabled; vendor preset: disabled) Drop-In: &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d └─10-docker.conf Active: inactive (dead) since Thu 2020-07-23 11:20:13 CST; 18min ago Docs: https:&#x2F;&#x2F;docs.docker.com Process: 9398 ExecStopPost&#x3D;&#x2F;bin&#x2F;bash -c &#x2F;sbin&#x2F;iptables -D FORWARD -s 0.0.0.0&#x2F;0 -j ACCEPT &amp;&gt; &#x2F;dev&#x2F;null || : (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS) Process: 9187 ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;dockerd -H fd:&#x2F;&#x2F; --containerd&#x3D;&#x2F;run&#x2F;containerd&#x2F;containerd.sock (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS) Main PID: 9187 (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS)Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.956503485+08:00&quot; level&#x3D;error msg&#x3D;&quot;Stop container error: Failed to stop container 68860c8d16b9ce7e74e8efd9db00e70a57eef1b752c2e6c703073c0bce5517d3 with error: Cannot kill c&gt;Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.954347116+08:00&quot; level&#x3D;error msg&#x3D;&quot;Stop container error: Failed to stop container 5ec9922beed1276989f1866c3fd911f37cc26aae4e4b27c7ce78183a9a4725cc with error: Cannot kill c&gt;Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.953615411+08:00&quot; level&#x3D;info msg&#x3D;&quot;Container failed to stop after sending signal 15 to the process, force killing&quot;Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.956557179+08:00&quot; level&#x3D;error msg&#x3D;&quot;Stop container error: Failed to stop container 6d0096fbcd4055f8bafb6b38f502a0186cd1dfca34219e9dd6050f512971aef5 with error: Cannot kill c&gt;Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.954601191+08:00&quot; level&#x3D;info msg&#x3D;&quot;Container failed to stop after sending signal 15 to the process, force killing&quot;Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.956600790+08:00&quot; level&#x3D;error msg&#x3D;&quot;Stop container error: Failed to stop container 6d1175ba6c55cb05ad89f4134ba8e9d3495c5acb5f07938dc16339b7cca013bf with error: Cannot kill c&gt;Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.957188989+08:00&quot; level&#x3D;info msg&#x3D;&quot;Daemon shutdown complete&quot;Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.957212655+08:00&quot; level&#x3D;info msg&#x3D;&quot;stopping event stream following graceful shutdown&quot; error&#x3D;&quot;context canceled&quot; module&#x3D;libcontainerd namespace&#x3D;plugins.mobyJul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.957209679+08:00&quot; level&#x3D;info msg&#x3D;&quot;stopping event stream following graceful shutdown&quot; error&#x3D;&quot;context canceled&quot; module&#x3D;libcontainerd namespace&#x3D;mobyJul 23 11:20:13 k8s-m1 systemd[1]: Stopped Docker Application Container Engine.[root@k8s-m1 ~]# systemctl start docker[root@k8s-m1 ~]# systemctl status docker● docker.service - Docker Application Container Engine Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service; enabled; vendor preset: disabled) Drop-In: &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d └─10-docker.conf Active: active (running) since Thu 2020-07-23 11:26:11 CST; 1s ago Docs: https:&#x2F;&#x2F;docs.docker.com Process: 9398 ExecStopPost&#x3D;&#x2F;bin&#x2F;bash -c &#x2F;sbin&#x2F;iptables -D FORWARD -s 0.0.0.0&#x2F;0 -j ACCEPT &amp;&gt; &#x2F;dev&#x2F;null || : (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS) Process: 16156 ExecStartPost&#x3D;&#x2F;sbin&#x2F;iptables -I FORWARD -s 0.0.0.0&#x2F;0 -j ACCEPT (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS) Main PID: 15974 (dockerd) Tasks: 62 Memory: 89.1M CGroup: &#x2F;system.slice&#x2F;docker.service └─15974 &#x2F;usr&#x2F;bin&#x2F;dockerd -H fd:&#x2F;&#x2F; --containerd&#x3D;&#x2F;run&#x2F;containerd&#x2F;containerd.sockJul 23 11:26:10 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:10.851106564+08:00&quot; level&#x3D;error msg&#x3D;&quot;cb4e16249cd8eac48ed734c71237195f04d63c56c55c0199b3cdf3d49461903d cleanup: failed to delete container from containerd: no such container&quot;Jul 23 11:26:10 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:10.860456898+08:00&quot; level&#x3D;error msg&#x3D;&quot;d9bbcab186ccb59f96c95fc886ec1b66a52aa96e45b117cf7d12e3ff9b95db9f cleanup: failed to delete container from containerd: no such container&quot;Jul 23 11:26:10 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:10.872405757+08:00&quot; level&#x3D;error msg&#x3D;&quot;07eb7a09bc8589abcb4d79af4b46798327bfb00624a7b9ceea457de392ad8f3d cleanup: failed to delete container from containerd: no such container&quot;Jul 23 11:26:10 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:10.877896618+08:00&quot; level&#x3D;error msg&#x3D;&quot;f5867657025bd7c3951cbd3e08ad97338cf69df2a97967a419e0e78eda869b73 cleanup: failed to delete container from containerd: no such container&quot;Jul 23 11:26:11 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:11.143661583+08:00&quot; level&#x3D;info msg&#x3D;&quot;Default bridge (docker0) is assigned with an IP address 172.17.0.0&#x2F;16. Daemon option --bip can be used to set a preferred IP address&quot;Jul 23 11:26:11 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:11.198200760+08:00&quot; level&#x3D;info msg&#x3D;&quot;Loading containers: done.&quot;Jul 23 11:26:11 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:11.219959208+08:00&quot; level&#x3D;info msg&#x3D;&quot;Docker daemon&quot; commit&#x3D;42e35e61f3 graphdriver(s)&#x3D;overlay2 version&#x3D;19.03.11Jul 23 11:26:11 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:11.220049865+08:00&quot; level&#x3D;info msg&#x3D;&quot;Daemon has completed initialization&quot;Jul 23 11:26:11 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:11.232373131+08:00&quot; level&#x3D;info msg&#x3D;&quot;API listen on &#x2F;var&#x2F;run&#x2F;docker.sock&quot;Jul 23 11:26:11 k8s-m1 systemd[1]: Started Docker Application Container Engine. etcd启动也失败，journal 查看下 etcd 状态 123456789101112131415161718192021222324252627282930[root@k8s-m1 ~]# journalctl -xe -u etcdJul 23 11:26:15 k8s-m1 etcd[18129]: Loading server configuration from &quot;&#x2F;etc&#x2F;etcd&#x2F;etcd.config.yml&quot;Jul 23 11:26:15 k8s-m1 etcd[18129]: etcd Version: 3.3.20Jul 23 11:26:15 k8s-m1 etcd[18129]: Git SHA: 9fd7e2b80Jul 23 11:26:15 k8s-m1 etcd[18129]: Go Version: go1.12.17Jul 23 11:26:15 k8s-m1 etcd[18129]: Go OS&#x2F;Arch: linux&#x2F;amd64Jul 23 11:26:15 k8s-m1 etcd[18129]: setting maximum number of CPUs to 16, total number of available CPUs is 16Jul 23 11:26:15 k8s-m1 etcd[18129]: found invalid file&#x2F;dir wal under data dir &#x2F;var&#x2F;lib&#x2F;etcd (Ignore this if you are upgrading etcd)Jul 23 11:26:15 k8s-m1 etcd[18129]: the server is already initialized as member before, starting as etcd member...Jul 23 11:26:15 k8s-m1 etcd[18129]: ignoring peer auto TLS since certs givenJul 23 11:26:15 k8s-m1 etcd[18129]: peerTLS: cert &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.crt, key &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.key, ca &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt, trusted-ca &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt, client-cert-auth &#x3D; fals&gt;Jul 23 11:26:15 k8s-m1 etcd[18129]: listening for peers on https:&#x2F;&#x2F;10.252.146.104:2380Jul 23 11:26:15 k8s-m1 etcd[18129]: ignoring client auto TLS since certs givenJul 23 11:26:15 k8s-m1 etcd[18129]: pprof is enabled under &#x2F;debug&#x2F;pprofJul 23 11:26:15 k8s-m1 etcd[18129]: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while peer key&#x2F;cert files are presented. Ignored key&#x2F;cert files.Jul 23 11:26:15 k8s-m1 etcd[18129]: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while client cert auth (--client-cert-auth) is enabled. Ignored client cert auth for this url.Jul 23 11:26:15 k8s-m1 etcd[18129]: listening for client requests on 127.0.0.1:2379Jul 23 11:26:15 k8s-m1 etcd[18129]: listening for client requests on 10.252.146.104:2379Jul 23 11:26:15 k8s-m1 etcd[18129]: skipped unexpected non snapshot file 000000000000002e-000000000052f2be.snap.brokenJul 23 11:26:15 k8s-m1 etcd[18129]: recovered store from snapshot at index 5426092Jul 23 11:26:15 k8s-m1 etcd[18129]: restore compact to 3967425Jul 23 11:26:15 k8s-m1 etcd[18129]: cannot unmarshal event: proto: KeyValue: illegal tag 0 (wire type 0)Jul 23 11:26:15 k8s-m1 systemd[1]: etcd.service: Main process exited, code&#x3D;exited, status&#x3D;1&#x2F;FAILUREJul 23 11:26:15 k8s-m1 systemd[1]: etcd.service: Failed with result &#39;exit-code&#39;.Jul 23 11:26:15 k8s-m1 systemd[1]: Failed to start Etcd Service.[root@k8s-m1 ~]# ll &#x2F;var&#x2F;lib&#x2F;etcd&#x2F;member&#x2F;snap&#x2F;total 8560-rw-r--r-- 1 root root 13499 Jul 20 13:36 000000000000002e-000000000052cbac.snap-rw-r--r-- 2 root root 128360 Jul 20 13:01 000000000000002e-000000000052f2be.snap.broken-rw------- 1 root root 8617984 Jul 23 11:26 db 这套集群是使用我的ansible部署，求star的，自带了备份脚本，但是是三天前坏的 123456[root@k8s-m1 ~]# ll &#x2F;opt&#x2F;etcd_bak&#x2F;total 41524-rw-r--r-- 1 root root 8618016 Jul 17 02:00 etcd-2020-07-17-02:00:01.db-rw-r--r-- 1 root root 8618016 Jul 18 02:00 etcd-2020-07-18-02:00:01.db-rw-r--r-- 1 root root 8323104 Jul 19 02:00 etcd-2020-07-19-02:00:01.db-rw-r--r-- 1 root root 8618016 Jul 20 02:00 etcd-2020-07-20-02:00:01.db 有恢复剧本，但是前提是etcd的v2和v3不能共存，否则无法恢复备份，我们线上都是把v2的存储关闭了的。主要是这个tasks里的26到42行步骤，这里复制了其他机器master上的 07/23 号的etcd备份文件，然后改了下host跑了下 12345678910111213141516171819202122232425262728293031323334353637383940[root@k8s-m1 ~]# cd Kubernetes-ansible[root@k8s-m1 Kubernetes-ansible]# ansible-playbook restoreETCD.yml -e &#39;db&#x3D;&#x2F;opt&#x2F;etcd_bak&#x2F;etcd-bak.db&#39;PLAY [10.252.146.104] **********************************************************************************************************************************************************************************************************************TASK [Gathering Facts] *********************************************************************************************************************************************************************************************************************ok: [10.252.146.104]TASK [restoreETCD : fail] ******************************************************************************************************************************************************************************************************************skipping: [10.252.146.104]TASK [restoreETCD : 检测备份文件存在否] *************************************************************************************************************************************************************************************************************ok: [10.252.146.104]TASK [restoreETCD : fail] ******************************************************************************************************************************************************************************************************************skipping: [10.252.146.104]TASK [restoreETCD : set_fact] **************************************************************************************************************************************************************************************************************skipping: [10.252.146.104]TASK [restoreETCD : set_fact] **************************************************************************************************************************************************************************************************************ok: [10.252.146.104]TASK [restoreETCD : 停止etcd] ****************************************************************************************************************************************************************************************************************ok: [10.252.146.104]TASK [restoreETCD : 删除etcd数据目录] ************************************************************************************************************************************************************************************************************ok: [10.252.146.104] &#x3D;&gt; (item&#x3D;&#x2F;var&#x2F;lib&#x2F;etcd)TASK [restoreETCD : 分发备份文件] ****************************************************************************************************************************************************************************************************************ok: [10.252.146.104]TASK [restoreETCD : 恢复备份] ******************************************************************************************************************************************************************************************************************changed: [10.252.146.104]TASK [restoreETCD : 启动etcd] ****************************************************************************************************************************************************************************************************************fatal: [10.252.146.104]: FAILED! &#x3D;&gt; &#123;&quot;changed&quot;: false, &quot;msg&quot;: &quot;Unable to start service etcd: Job for etcd.service failed because the control process exited with error code.\\nSee \\&quot;systemctl status etcd.service\\&quot; and \\&quot;journalctl -xe\\&quot; for details.\\n&quot;&#125;PLAY RECAP *********************************************************************************************************************************************************************************************************************************10.252.146.104 : ok&#x3D;7 changed&#x3D;1 unreachable&#x3D;0 failed&#x3D;1 skipped&#x3D;3 rescued&#x3D;0 ignored&#x3D;0 查看下日志 123456789101112131415161718192021[root@k8s-m1 Kubernetes-ansible]# journalctl -xe -u etcdJul 23 11:27:46 k8s-m1 etcd[58954]: Loading server configuration from &quot;&#x2F;etc&#x2F;etcd&#x2F;etcd.config.yml&quot;Jul 23 11:27:46 k8s-m1 etcd[58954]: etcd Version: 3.3.20Jul 23 11:27:46 k8s-m1 etcd[58954]: Git SHA: 9fd7e2b80Jul 23 11:27:46 k8s-m1 etcd[58954]: Go Version: go1.12.17Jul 23 11:27:46 k8s-m1 etcd[58954]: Go OS&#x2F;Arch: linux&#x2F;amd64Jul 23 11:27:46 k8s-m1 etcd[58954]: setting maximum number of CPUs to 16, total number of available CPUs is 16Jul 23 11:27:46 k8s-m1 etcd[58954]: the server is already initialized as member before, starting as etcd member...Jul 23 11:27:46 k8s-m1 etcd[58954]: ignoring peer auto TLS since certs givenJul 23 11:27:46 k8s-m1 etcd[58954]: peerTLS: cert &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.crt, key &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.key, ca &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt, trusted-ca &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt, client-cert-auth &#x3D; fals&gt;Jul 23 11:27:46 k8s-m1 etcd[58954]: listening for peers on https:&#x2F;&#x2F;10.252.146.104:2380Jul 23 11:27:46 k8s-m1 etcd[58954]: ignoring client auto TLS since certs givenJul 23 11:27:46 k8s-m1 etcd[58954]: pprof is enabled under &#x2F;debug&#x2F;pprofJul 23 11:27:46 k8s-m1 etcd[58954]: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while peer key&#x2F;cert files are presented. Ignored key&#x2F;cert files.Jul 23 11:27:46 k8s-m1 etcd[58954]: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while client cert auth (--client-cert-auth) is enabled. Ignored client cert auth for this url.Jul 23 11:27:46 k8s-m1 etcd[58954]: listening for client requests on 127.0.0.1:2379Jul 23 11:27:46 k8s-m1 etcd[58954]: listening for client requests on 10.252.146.104:2379Jul 23 11:27:47 k8s-m1 etcd[58954]: member ac2dcf6aed12e8f1 has already been bootstrappedJul 23 11:27:47 k8s-m1 systemd[1]: etcd.service: Main process exited, code&#x3D;exited, status&#x3D;1&#x2F;FAILUREJul 23 11:27:47 k8s-m1 systemd[1]: etcd.service: Failed with result &#39;exit-code&#39;.Jul 23 11:27:47 k8s-m1 systemd[1]: Failed to start Etcd Service. 这个member xxxx has already been bootstrapped解决办法就是把配置文件的下面修改，后面启动完记得改回来 1initial-cluster-state: &#39;new&#39; 改成 initial-cluster-state: &#39;existing&#39; 然后成功启动 123456789101112131415161718192021222324252627282930313233343536[root@k8s-m1 Kubernetes-ansible]# systemctl start etcd[root@k8s-m1 Kubernetes-ansible]# journalctl -xe -u etcdJul 23 11:27:55 k8s-m1 etcd[59889]: Loading server configuration from &quot;&#x2F;etc&#x2F;etcd&#x2F;etcd.config.yml&quot;Jul 23 11:27:55 k8s-m1 etcd[59889]: etcd Version: 3.3.20Jul 23 11:27:55 k8s-m1 etcd[59889]: Git SHA: 9fd7e2b80Jul 23 11:27:55 k8s-m1 etcd[59889]: Go Version: go1.12.17Jul 23 11:27:55 k8s-m1 etcd[59889]: Go OS&#x2F;Arch: linux&#x2F;amd64Jul 23 11:27:55 k8s-m1 etcd[59889]: setting maximum number of CPUs to 16, total number of available CPUs is 16Jul 23 11:27:55 k8s-m1 etcd[59889]: found invalid file&#x2F;dir wal under data dir &#x2F;var&#x2F;lib&#x2F;etcd (Ignore this if you are upgrading etcd)Jul 23 11:27:55 k8s-m1 etcd[59889]: the server is already initialized as member before, starting as etcd member...Jul 23 11:27:55 k8s-m1 etcd[59889]: ignoring peer auto TLS since certs givenJul 23 11:27:55 k8s-m1 etcd[59889]: peerTLS: cert &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.crt, key &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.key, ca &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt, trusted-ca &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt, client-cert-auth &#x3D; fals&gt;Jul 23 11:27:55 k8s-m1 etcd[59889]: listening for peers on https:&#x2F;&#x2F;10.252.146.104:2380Jul 23 11:27:55 k8s-m1 etcd[59889]: ignoring client auto TLS since certs givenJul 23 11:27:55 k8s-m1 etcd[59889]: pprof is enabled under &#x2F;debug&#x2F;pprofJul 23 11:27:55 k8s-m1 etcd[59889]: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while peer key&#x2F;cert files are presented. Ignored key&#x2F;cert files.Jul 23 11:27:55 k8s-m1 etcd[59889]: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while client cert auth (--client-cert-auth) is enabled. Ignored client cert auth for this url.Jul 23 11:27:55 k8s-m1 etcd[59889]: listening for client requests on 127.0.0.1:2379Jul 23 11:27:55 k8s-m1 etcd[59889]: listening for client requests on 10.252.146.104:2379Jul 23 11:27:55 k8s-m1 etcd[59889]: recovered store from snapshot at index 5952463Jul 23 11:27:55 k8s-m1 etcd[59889]: restore compact to 4369703Jul 23 11:27:55 k8s-m1 etcd[59889]: name &#x3D; etcd-001Jul 23 11:27:55 k8s-m1 etcd[59889]: data dir &#x3D; &#x2F;var&#x2F;lib&#x2F;etcdJul 23 11:27:55 k8s-m1 etcd[59889]: member dir &#x3D; &#x2F;var&#x2F;lib&#x2F;etcd&#x2F;memberJul 23 11:27:55 k8s-m1 etcd[59889]: dedicated WAL dir &#x3D; &#x2F;var&#x2F;lib&#x2F;etcd&#x2F;walJul 23 11:27:55 k8s-m1 etcd[59889]: heartbeat &#x3D; 100msJul 23 11:27:55 k8s-m1 etcd[59889]: election &#x3D; 1000msJul 23 11:27:55 k8s-m1 etcd[59889]: snapshot count &#x3D; 5000Jul 23 11:27:55 k8s-m1 etcd[59889]: advertise client URLs &#x3D; https:&#x2F;&#x2F;10.252.146.104:2379Jul 23 11:27:55 k8s-m1 etcd[59889]: restarting member ac2dcf6aed12e8f1 in cluster 367e2aebc6430cbe at commit index 5952491Jul 23 11:27:55 k8s-m1 etcd[59889]: ac2dcf6aed12e8f1 became follower at term 47Jul 23 11:27:55 k8s-m1 etcd[59889]: newRaft ac2dcf6aed12e8f1 [peers: [1e713be314744d53,8b1621b475555fd9,ac2dcf6aed12e8f1], term: 47, commit: 5952491, applied: 5952463, lastindex: 5952491, lastterm: 47]Jul 23 11:27:55 k8s-m1 etcd[59889]: enabled capabilities for version 3.3Jul 23 11:27:55 k8s-m1 etcd[59889]: added member 1e713be314744d53 [https:&#x2F;&#x2F;10.252.146.105:2380] to cluster 367e2aebc6430cbe from storeJul 23 11:27:55 k8s-m1 etcd[59889]: added member 8b1621b475555fd9 [https:&#x2F;&#x2F;10.252.146.106:2380] to cluster 367e2aebc6430cbe from storeJul 23 11:27:55 k8s-m1 etcd[59889]: added member ac2dcf6aed12e8f1 [https:&#x2F;&#x2F;10.252.146.104:2380] to cluster 367e2aebc6430cbe from store 查看集群状态 12345678[root@k8s-m1 Kubernetes-ansible]# etcd-ha+-----------------------------+------------------+---------+---------+-----------+-----------+------------+| ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |+-----------------------------+------------------+---------+---------+-----------+-----------+------------+| https:&#x2F;&#x2F;10.252.146.104:2379 | ac2dcf6aed12e8f1 | 3.3.20 | 8.3 MB | false | 47 | 5953557 || https:&#x2F;&#x2F;10.252.146.105:2379 | 1e713be314744d53 | 3.3.20 | 8.6 MB | false | 47 | 5953557 || https:&#x2F;&#x2F;10.252.146.106:2379 | 8b1621b475555fd9 | 3.3.20 | 8.3 MB | true | 47 | 5953557 |+-----------------------------+------------------+---------+---------+-----------+-----------+------------+ 然后给kube-apiserver三个组件和kubelet起来后 12345[root@k8s-m1 Kubernetes-ansible]# kubectl get node -o wideNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME10.252.146.104 Ready &lt;none&gt; 30d v1.16.9 10.252.146.104 &lt;none&gt; CentOS Linux 8 (Core) 4.18.0-193.6.3.el8_2.x86_64 docker:&#x2F;&#x2F;19.3.1110.252.146.105 Ready &lt;none&gt; 30d v1.16.9 10.252.146.105 &lt;none&gt; CentOS Linux 8 (Core) 4.18.0-193.6.3.el8_2.x86_64 docker:&#x2F;&#x2F;19.3.1110.252.146.106 Ready &lt;none&gt; 30d v1.16.9 10.252.146.106 &lt;none&gt; CentOS Linux 8 (Core) 4.18.0-193.6.3.el8_2.x86_64 docker:&#x2F;&#x2F;19.3.11 pod也在慢慢自愈了 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 特别声明 原文作者：Zhangguanzhang 本文原链：http://zhangguanzhang.github.io/2020/07/23/fs-error-fix-k8s-master/ 本文转载如有侵权，请联系站长删除，谢谢 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"Spring Cloud Kubernetes之实战服务注册与发现","date":"2020-07-23T01:38:22.000Z","path":"2020/07/23/spring-cloud-k8s-discovery/","text":"好久没写文章了，本文主讲利用 k8s 来实现服务的注册与发现，甚至负载均衡，简称 LB，完美无坑版！ 环境： ubuntu16.04 docker18.04 k8s1.13.x + maven3.5.3 java1.8 + springboot 2.1.1 spring-cloud-kubernetes：1.0.1.RELEASERelax 前提 Ubuntu下安装docker18.04 or 其它较高版本，k8s1.13.x及以上，jvm环境等。 创建项目 我们都知道，涉及到微服务，那必体现六个字，”高内聚，低耦合”，所以针对不同业务或应用场景，服务模块化很重要，这个不再赘述了。咱们先来创建服务提供方，同样，利用eclipse或IDEA创建一个项目，此处略了。 创建好项目之后，首先引入依赖： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes-config&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;&#x2F;artifactId&gt; &lt;scope&gt;test&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;&lt;artifactId&gt;spring-boot-starter-aop&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt; 其他数据库，中间件等，可根据项目自行添加。 同样，我们需要配置初始化bean，这就涉及到配置文件bootstrap.yaml： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# we enable some of the management endpoints to make it possible to restart the applicationmanagement: endpoint: restart: enabled: true health: enabled: true info: enabled: truespring: application: name: edge-cas cloud: kubernetes: reload: #自动更新配置的开关设置为打开 enabled: true #更新配置信息的模式：polling是主动拉取，event是事件通知 mode: event #主动拉取的间隔时间是500毫秒 #period: 500 config: sources: - name: $&#123;spring.application.name&#125; namespace: default discovery: all-namespaces: true http: encoding: charset: UTF-8 enabled: true force: true mvc: throw-exception-if-no-handler-found: true main: allow-bean-definition-overriding: true # 当遇到同样名称时，是否允许覆盖注册 接下来就是application.yaml： 123456789101112131415161718192021222324252627282930313233343536373839server: port: 1000 undertow: accesslog: enabled: false pattern: combined servlet: session: timeout: PT120Mlogging: path: &#x2F;data&#x2F;$&#123;spring.application.name&#125;&#x2F;logsclient: http: request: connectTimeout: 8000 readTimeout: 30000 mybatis: mapperLocations: classpath:mapper&#x2F;*.xml typeAliasesPackage: com.gemantic.*.model 到这，基本的配置即完成，同样，我们也引入了k8s的configmap功能，可以新建configmap的yaml文件来创建其configmap。 然后最重要的一点，就是我们需要创建service： 1234567891011121314151617181920212223apiVersion: v1kind: Servicemetadata: name: demo-cas-service namespace: defaultspec: ports: - name: cas01 port: 1000 targetPort: cas01 selector: app: demo-cas 这一点很关键，即实现了服务的注册。 然后服务提供者的项目架子搭建好了，自己可以添加一些内容，比如我把它作为微服务架构的统一鉴权中心CAS。 接下来创建服务消费者的项目，同样引入依赖，但这一次不同： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes-config&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;&#x2F;artifactId&gt; &lt;scope&gt;test&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;&lt;artifactId&gt;spring-boot-starter-aop&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt;&lt;groupId&gt;io.kubernetes&lt;&#x2F;groupId&gt;&lt;artifactId&gt;client-java&lt;&#x2F;artifactId&gt;&lt;version&gt;$&#123;kubernetes-client-version&#125;&lt;&#x2F;version&gt;&lt;exclusions&gt;&lt;exclusion&gt;&lt;groupId&gt;com.squareup.okio&lt;&#x2F;groupId&gt; &lt;artifactId&gt;okio&lt;&#x2F;artifactId&gt;&lt;&#x2F;exclusion&gt;&lt;&#x2F;exclusions&gt;&lt;&#x2F;dependency&gt;&lt;!-- springcloud-k8s-discovery --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-commons&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-kubernetes-core&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-kubernetes-discovery&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes-ribbon&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; 以上是服务消费者的必须依赖，其他的可根据项目自行添加，比如：在线文档swagger，数据库，json解析，权限管理shiro等。 同样，我们也需要配置初始化bean，这就涉及到配置文件bootstrap.yaml：如上 接下来需要配置服务消费者的消费逻辑以及实现负载均衡的策略(application.yaml)： 1234567891011121314151617181920212223242526272829303132333435363738394041server: port: 1002 undertow: accesslog: enabled: false pattern: combined servlet: session: timeout: PT120M logging: path: &#x2F;data&#x2F;$&#123;spring.application.name&#125;&#x2F;logsclient: http: request: connectTimeout: 8000 readTimeout: 30000 mybatis: mapperLocations: classpath:mapper&#x2F;*.xml typeAliasesPackage: com.gemantic.*.model #这是针对所有的提供者服务的消费策略： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869backend: ribbon: eureka: enabled: false client: enabled: true ServerListRefreshInterval: 5000 ribbon: ConnectTimeout: 3000 # 设置全局默认的ribbon的读超时 ReadTimeout: 1000 eager-load: enabled: true clients: demo-cas-service,cloud-admin-service MaxAutoRetries: 1 #对第一次请求的服务的重试次数 MaxAutoRetriesNextServer: 1 #要重试的下一个服务的最大数量（不包括第一个服务） #listOfServers: localhost:5556,localhost:5557 #ServerListRefreshInterval: 2000 OkToRetryOnAllOperations: true NFLoadBalancerRuleClassName:com.netflix.loadbalancer.RoundRobinRule#这个是针对某个指定服务来进行配置负载均衡的策略#demo-cas-service:# ribbon:# ConnectTimeout: 3000# ReadTimeout: 60000# MaxAutoRetries: 1 #对第一次请求的服务的重试次数# MaxAutoRetriesNextServer: 1 #要重试的下一个服务的最大数量（不包括第一个服务）# listOfServers: localhost:5556,localhost:5557# ServerListRefreshInterval: 2000# OkToRetryOnAllOperations: true#NFLoadBalancerRuleClassName:com.netflix.loadbalancer.RoundRobinRulehystrix.command.BackendCall.execution.isolation.thread.timeoutInMilliseconds: 5000hystrix.threadpool.BackendCallThread.coreSize: 5 这样，服务提供者与服务消费者就都新建成功了，接下来就需要丰满自己的业务应用逻辑了，同样，消费者也可以创建configmap来配置管理自己的配置。 接下来就是亲测： 这里，消费者调用提供者，提供者是个cas服务，则： 1234567891011121314151617181920212223242526272829MultiValueMap&lt;String, String&gt; map &#x3D; new LinkedMultiValueMap&lt;&gt;();map.add(&quot;username&quot;, username);map.add(&quot;password&quot;, password);logger.info(&quot;CAS URL: &#123;&#125;&quot;, envConfig.getCas_url());String respBody &#x3D; HttpRequestUtil.doPostForm(restTemplate, envConfig.getCas_url(), map);if (StringUtils.isNotBlank(respBody)) &#123;JSONObject pobj &#x3D; JSON.parseObject(respBody);Object object &#x3D; pobj.get(&quot;message&quot;);Integer code &#x3D; JSON.parseObject(object.toString()).getInteger(&quot;code&quot;);if (code &#x3D;&#x3D; LoginEnum.LOGIN_SUCCESS.getSeq()) &#123;Object data &#x3D; pobj.get(&quot;data&quot;);SysUserDto sysUser &#x3D; JSON.parseObject(data.toString(), SysUserDto.class);return sysUser;&#125;&#125; 这里的环境变量即使configmap提供，值：cas_url: http://demo-cas-service/login，这样我们就完成了调用的逻辑。 亲测有效： 接下来我们如果需要测试LB，需要添加一条脚本： 增加pod： 1kubectl scale --replicas&#x3D;2 deployment demo-cas-deployment 这样，我们既看到两个demo-cas-deployment的pod： 同样测试，根据策略轮询调用的方式，这次会请求到该pod上，这里不贴截图了，大家可以试试。 以上，即是分享了k8s带来的第二大优点： 通过service的方式提供了服务的注册与发现，而且单机的k8s本身也不重，所以操作起来也非常之简单。避免了springboot原生提供的eureka、阿里的nacos、zk来作分布式的服务注册与发现要简单的多。减轻系统的繁重，以及避免了系统的冗余。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"k8s部署手册","date":"2020-07-23T01:06:27.000Z","path":"2020/07/23/k8s/","text":"1、 K8S 的由来K8S 是 kubernetes 的英文缩写，是用 8 代替 8 个字符 “ubernete” 而成的缩写。 2、 K8S 单机版实战环境： ubuntu 16.04 gpu 驱动 418.56 docker 18.06 k8s 1.13.5 一、设置环境首先备份一下源配置: 1cp &#x2F;etc&#x2F;apt&#x2F;sources.list &#x2F;etc&#x2F;apt&#x2F;sources.list.cp 编辑，替换为阿里源: 12345678910111213141516171819vim &#x2F;etc&#x2F;apt&#x2F;sources.listdeb-src http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial main restricteddeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial main restricteddeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial main restricted multiverse universedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-updates main restricteddeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-updates main restricted multiverse universedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial universedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-updates universedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial multiversedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-updates multiversedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-backports main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F;xenial-backports main restricted universe multiversedeb http:&#x2F;&#x2F;archive.canonical.com&#x2F;ubuntu xenial partnerdeb-src http:&#x2F;&#x2F;archive.canonical.com&#x2F;ubuntu xenial partnerdeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-security main restricteddeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-security main restricted multiverse universedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-security universedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-security multiverse 更新源: 1apt-get update 自动修复安装出现 broken 的 package: 1apt --fix-broken install 升级，对于 gpu 机器可不执行，否则可能升级 gpu 驱动导致问题: 1apt-get upgrade 关闭防火墙: 1ufw disable 安装 selinux: 1apt install selinux-utils selinux 防火墙配置: 12345setenforce 0vim&#x2F;etc&#x2F;selinux&#x2F;conifgSELINUX&#x3D;disabled 设置网络: 1234567tee &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf &lt;&lt;-&#39;EOF&#39;net.bridge.bridge-nf-call-ip6tables &#x3D; 1net.bridge.bridge-nf-call-iptables &#x3D; 1net.ipv4.ip_forward &#x3D; 1EOF​modprobe br_netfilter 查看 ipv4 与 v6 配置是否生效: 1sysctl --system 配置 iptables: 1234iptables -P FORWARD ACCEPTvim &#x2F;etc&#x2F;rc.local&#x2F;usr&#x2F;sbin&#x2F;iptables -P FORWARD ACCEPT 永久关闭 swap 分区: 1sed -i &#39;s&#x2F;.*swap.*&#x2F;#&amp;&#x2F;&#39; &#x2F;etc&#x2F;fstab 二、安装 docker执行下面的命令: 1234567891011apt-get install apt-transport-https ca-certificates curl software-properties-common​curl -fsSL https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;ubuntu&#x2F;gpg | apt-key add -​add-apt-repository &quot;deb [arch&#x3D;amd64] https:&#x2F;&#x2F;download.docker.com&#x2F;linux&#x2F;ubuntu $(lsb_release -cs) stable&quot; apt-get update​apt-get purge docker-ce docker docker-engine docker.io &amp;&amp; rm -rf &#x2F;var&#x2F;lib&#x2F;dockerapt-get autoremove docker-ce docker docker-engine docker.io​apt-get install -y docker-ce&#x3D;18.06.3~ce~3-0~ubuntu 启动 docker 并设置开机自重启: 1systemctl enable docker &amp;&amp; systemctl start docker Docker 配置: 1234567891011121314151617vim &#x2F;etc&#x2F;docker&#x2F;daemon.json&#123; &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: &#123; &quot;max-size&quot;: &quot;100m&quot;, &quot;max-file&quot;: &quot;10&quot; &#125;, &quot;insecure-registries&quot;: [&quot;http:&#x2F;&#x2F;k8s.gcr.io&quot;], &quot;data-root&quot;: &quot;&quot;, &quot;default-runtime&quot;: &quot;nvidia&quot;, &quot;runtimes&quot;: &#123; &quot;nvidia&quot;: &#123; &quot;path&quot;: &quot;&#x2F;usr&#x2F;bin&#x2F;nvidia-container-runtime&quot;, &quot;runtimeArgs&quot;: [] &#125; &#125;&#125; 上面是含 GPU 的配置，不含 GPU 的配置: 123456789101112131415&#123;&quot;registry-mirrors&quot;:[&quot;https:&#x2F;&#x2F;registry.docker-cn.com&quot;],&quot;storage-driver&quot;:&quot;overlay2&quot;,&quot;log-driver&quot;:&quot;json-file&quot;,&quot;log-opts&quot;:&#123;&quot;max-size&quot;:&quot;100m&quot;&#125;,&quot;exec-opts&quot;:[&quot;native.cgroupdriver&#x3D;systemd&quot;],&quot;insecure-registries&quot;:[&quot;http:&#x2F;&#x2F;k8s.gcr.io&quot;],&quot;live-restore&quot;:true&#125; 重启服务并设置开机自动重启: 1systemctl daemon-reload &amp;&amp; systemctl restart docker &amp;&amp; docker info 三、安装 k8s拉取镜像前的设置: 1234567apt-get update &amp;&amp; apt-get install -y apt-transport-https curl​curl -s https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;apt&#x2F;doc&#x2F;apt-key.gpg | apt-key add -​tee &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;kubernetes.list &lt;&lt;-&#39;EOF&#39;deb https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;apt kubernetes-xenial mainEOF 更新: 12345apt-get updateapt-get purge kubelet&#x3D;1.13.5-00 kubeadm&#x3D;1.13.5-00 kubectl&#x3D;1.13.5-00apt-get autoremove kubelet&#x3D;1.13.5-00 kubeadm&#x3D;1.13.5-00 kubectl&#x3D;1.13.5-00apt-get install -y kubelet&#x3D;1.13.5-00 kubeadm&#x3D;1.13.5-00 kubectl&#x3D;1.13.5-00apt-mark hold kubelet&#x3D;1.13.5-00 kubeadm&#x3D;1.13.5-00 kubectl&#x3D;1.13.5-00 启动服务并设置开机自动重启: 1systemctl enable kubelet &amp;&amp; sudo systemctl start kubelet 安装 k8s 相关镜像，由于 gcr.io 网络访问不了，从 registry.cn-hangzhou.aliyuncs.com 镜像地址下载: 12345678910111213docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;gg-gcr-io&#x2F;kube-apiserver:v1.13.5docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;gg-gcr-io&#x2F;kube-controller-manager:v1.13.5docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;gg-gcr-io&#x2F;kube-scheduler:v1.13.5docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;gg-gcr-io&#x2F;kube-proxy:v1.13.5docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;kuberimages&#x2F;pause:3.1docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;kuberimages&#x2F;etcd:3.2.24docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;kuberimages&#x2F;coredns:1.2.6 打标签: 12345678910111213docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;gg-gcr-io&#x2F;kube-apiserver:v1.13.5 k8s.gcr.io&#x2F;kube-apiserver:v1.13.5docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;gg-gcr-io&#x2F;kube-controller-manager:v1.13.5 k8s.gcr.io&#x2F;kube-controller-manager:v1.13.5docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;gg-gcr-io&#x2F;kube-scheduler:v1.13.5 k8s.gcr.io&#x2F;kube-scheduler:v1.13.5docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;gg-gcr-io&#x2F;kube-proxy:v1.13.5 k8s.gcr.io&#x2F;kube-proxy:v1.13.5docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;kuberimages&#x2F;pause:3.1 k8s.gcr.io&#x2F;pause:3.1docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;kuberimages&#x2F;etcd:3.2.24 k8s.gcr.io&#x2F;etcd:3.2.24docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;kuberimages&#x2F;coredns:1.2.6 k8s.gcr.io&#x2F;coredns:1.2.6 四、kubeadm 初始化利用 kubeadm 初始化 k8s，其中主机 IP 根据自己的实际情况输入: 1kubeadm init --kubernetes-version&#x3D;v1.13.5 --pod-network-cidr&#x3D;10.244.0.0&#x2F;16 --service-cidr&#x3D;10.16.0.0&#x2F;16 --apiserver-advertise-address&#x3D;$&#123;masterIp&#125; | tee kubeadm-init.log 此时，如果未知主机 IP，也可利用 yaml 文件动态初始化: 12345678910111213141516vi &#x2F;etc&#x2F;hosts10.10.5.100 k8s.api.servervi kube-init.yamlapiVersion: kubeadm.k8s.io&#x2F;v1beta1kind: ClusterConfigurationkubernetesVersion: v1.13.5imageRepository: registry.aliyuncs.com&#x2F;google_containersapiServer: certSANs: - &quot;k8s.api.server&quot;controlPlaneEndpoint: &quot;k8s.api.server:6443&quot;networking: serviceSubnet: &quot;10.1.0.0&#x2F;16&quot; podSubnet: &quot;10.244.0.0&#x2F;16&quot; HA 版本: 1234567891011121314151617apiVersion: kubeadm.k8s.io&#x2F;v1beta1kind: ClusterConfigurationkubernetesVersion: v1.13.5imageRepository: registry.aliyuncs.com&#x2F;google_containersapiServer: certSANs: - &quot;api.k8s.com&quot;controlPlaneEndpoint: &quot;api.k8s.com:6443&quot;etcd: external: endpoints: - https:&#x2F;&#x2F;ETCD_0_IP:2379 - https:&#x2F;&#x2F;ETCD_1_IP:2379 - https:&#x2F;&#x2F;ETCD_2_IP:2379networking: serviceSubnet: 10.1.0.0&#x2F;16 podSubnet: 10.244.0.0&#x2F;16 注意: apiVersion 中用 kubeadm，因为需要用 kubeadm 来初始化，最后执行下面来初始化: 1kubeadm init --config&#x3D;kube-init.yaml 出现问题，解决后，reset 后再执行，如果需要更多，执行: 1kubeadm --help 五、部署出现问题先删除 node 节点(集群版) 123kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsets​kubectl delete node &lt;node name&gt; 清空 init 配置在需要删除的节点上执行（注意，当执行 init 或者 join 后出现任何错误，都可以使用此命令返回）: 1kubeadm reset 六、查问题初始化后出现问题，可以通过以下命令先查看其容器状态以及网络情况: 12345678910111213141516171819sudo docker ps -a | grep kube | grep -v pause​sudo docker logs CONTAINERID​sudo docker images &amp;&amp; systemctl status -l kubelet​netstat -nlpt​kubectl describe ep kuberneteskubectl describe svc kubernetes​kubectl get svc kubernetes​kubectl get ep​netstat -nlpt | grep apiservi &#x2F;var&#x2F;log&#x2F;syslog 七、给当前用户配置 k8s apiserver 访问公钥12345sudo mkdir -p $HOME&#x2F;.kube​sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config​sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config 八、网络插件123456789101112kubectl apply -f https:&#x2F;&#x2F;docs.projectcalico.org&#x2F;v3.3&#x2F;getting-started&#x2F;kubernetes&#x2F;installation&#x2F;hosted&#x2F;rbac-kdd.yaml​wget https:&#x2F;&#x2F;docs.projectcalico.org&#x2F;v3.3&#x2F;getting-started&#x2F;kubernetes&#x2F;installation&#x2F;hosted&#x2F;kubernetes-datastore&#x2F;calico-networking&#x2F;1.7&#x2F;calico.yamlvi calico.yaml- name: CALICO_IPV4POOL_IPIP value:&quot;off&quot;- name: CALICO_IPV4POOL_CIDR value: &quot;10.244.0.0&#x2F;16​kubectl apply -f calico.yaml 单机下允许 master 节点部署 pod 命令如下: 1kubectl taint nodes --all node-role.kubernetes.io&#x2F;master- 禁止 master 部署 pod: 1kubectl taint nodes k8s node-role.kubernetes.io&#x2F;master&#x3D;true:NoSchedule 以上单机版部署结束，如果你的项目中，交付的是软硬件结合的一体机，那么到此就结束了。记得单机下要允许 master 节点部署哟！ 接下来，集群版本上线咯！以上面部署的机器为例，作为 master 节点，继续执行: 123456789scp &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $nodeUser@$nodeIp:&#x2F;home&#x2F;$nodeUser​scp &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;* $nodeUser@$nodeIp:&#x2F;home&#x2F;$nodeUser&#x2F;etcd​kubeadm token generate​kubeadm token create $token_name --print-join-command --ttl&#x3D;0​kubeadm join $masterIP:6443 --token $token_name --discovery-token-ca-cert-hash $hash Node 机器执行时，如果需要 cuda ，可以参考以下资料: 123https:&#x2F;&#x2F;docs.nvidia.com&#x2F;cuda&#x2F;cuda-installation-guide-linux&#x2F;index.html#ubuntu-installation​https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012235003&#x2F;article&#x2F;details&#x2F;54575758https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_39670011&#x2F;article&#x2F;details&#x2F;90404111 正式执行: 1234vim &#x2F;etc&#x2F;modprobe.d&#x2F;blacklist-nouveau.confblacklist nouveauoptions nouveau modeset&#x3D;0update-initramfs -u 重启 ubuntu 查看是否禁用成功: 1234567lsmod | grep nouveauapt-get remove --purge nvidia*https:&#x2F;&#x2F;developer.nvidia.com&#x2F;cuda-downloads​sudo apt-get install freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev 安装 cuda: 123456789​acceptselect &quot;Install&quot; &#x2F; Enterselect &quot;Yes&quot;sh cuda_10.1.168_418.67_linux.run​echo &#39;export PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;bin:$PATH&#39; &gt;&gt; ~&#x2F;.bashrcecho &#39;export PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;NsightCompute-2019.3:$PATH&#39; &gt;&gt; ~&#x2F;.bashrcecho &#39;export LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;lib64:$LD_LIBRARY_PATH&#39; &gt;&gt; ~&#x2F;.bashrcsource ~&#x2F;.bashrc 重启机器，检查 cuda 是否安装成功。 查看是否有“nvidia*”的设备: 1cd &#x2F;dev &amp;&amp; ls -al 如果没有，创建一个 nv.sh: 123456789101112131415161718192021222324252627282930313233343536vi nv.sh#!&#x2F;bin&#x2F;bash &#x2F;sbin&#x2F;modprobe nvidiaif [ &quot;$?&quot; -eq 0 ];thenNVDEVS&#x3D;&#96;lspci | grep -i NVIDIA&#96;N3D&#x3D;&#96;echo&quot;$NVDEVS&quot;| grep &quot;3D controller&quot; | wc -l&#96;NVGA&#x3D;&#96;echo&quot;$NVDEVS&quot;| grep &quot;VGA compatible controller&quot; | wc -l&#96;N&#x3D;&#96;expr $N3D + $NVGA -1&#96;for i in &#96;seq0 $N&#96;; do mknod -m 666 &#x2F;dev&#x2F;nvidia$i c 195 $idone mknod -m 666 &#x2F;dev&#x2F;nvidiactl c 195 255else exit 1fi​chmod +x nv.sh &amp;&amp; bash nv.sh 再次重启机器查看 cuda 版本: 1nvcc -V 编译: 123cd &#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;samples &amp;&amp; makecd &#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;samples&#x2F;bin&#x2F;x86_64&#x2F;linux&#x2F;release .&#x2F;deviceQuery 以上如果输出：“Result = PASS” 代表 cuda 安装成功。 安装 nvdocker: 12345678910111213141516171819vim &#x2F;etc&#x2F;docker&#x2F;daemon.json&#123;&quot;runtimes&quot;:&#123; &quot;nvidia&quot;:&#123; &quot;path&quot;:&quot;nvidia-container-runtime&quot;, &quot;runtimeArgs&quot;:[] &#125;&#125;,&quot;registry-mirrors&quot;:[&quot;https:&#x2F;&#x2F;registry.docker-cn.com&quot;],&quot;storage-driver&quot;:&quot;overlay2&quot;,&quot;default-runtime&quot;:&quot;nvidia&quot;,&quot;log-driver&quot;:&quot;json-file&quot;,&quot;log-opts&quot;:&#123; &quot;max-size&quot;:&quot;100m&quot;&#125;,&quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;],&quot;insecure-registries&quot;: [$harborRgistry],&quot;live-restore&quot;: true&#125; 重启 docker: 1sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker &amp;&amp; docker info 检查 nvidia-docker 安装是否成功: 1docker run --runtime&#x3D;nvidia --rm nvidia&#x2F;cuda:9.0-base nvidia-smi 在节点机器进入 su 模式: 1su $nodeUser 给当前节点用户配置 k8s apiserver 访问公钥: 123456789101112131415mkdir -p $HOME&#x2F;.kube​cp -i admin.conf $HOME&#x2F;.kube&#x2F;config​chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config​mkdir -p $HOME&#x2F;etcd​sudo rm -rf &#x2F;etc&#x2F;kubernetes​sudo mkdir -p &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd​sudo cp &#x2F;home&#x2F;$nodeUser&#x2F;etcd&#x2F;* &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd​sudo kubeadm join $masterIP:6443 --token $token_name --discovery-token-ca-cert-hash $hash 如: 1sudo kubeadm join 192.168.8.116:6443 --token vyi4ga.foyxqr2iz9i391q3 --discovery-token-ca-cert-hash sha256:929143bcdaa3e23c6faf20bc51ef6a57df02edf9df86cedf200320a9b4d3220a 检查 node 是否加入 master: 1kubectl get node 以上介绍了单机的 k8s 部署，以及 HA 的 master 节点的部署安装。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"浅入Spring Cloud架构","date":"2020-07-23T01:00:09.000Z","path":"2020/07/23/micro-service03/","text":"1、 微服务简介 1.1 什么是微服务&emsp;&emsp;所谓微服务，就是把一个比较大的单个应用程序或服务拆分为若干个独立的、粒度很小的服务或组件。 1.2 为什么使用微服务&emsp;&emsp;微服务的拆解业务，这一策略，可扩展单个组件，而不需要整个的应用程序堆栈做修改，从而满足服务等级协议。微服务带来的好处是，它们更快且更容易更新。当开发者对一个传统的单体应用程序进行变更时，他们必须做详细、完整的 QA 测试，以确保变更不会影响其他特性或功能。但有了微服务，开发者可以更新应用程序的单个组件，而不会影响其他的部分。测试微服务应用程序仍然是必需的，但使得其更容易被识别和隔离，从而加快开发速度并支持 DevOps 和持续应用程序开发。 1.3 微服务的架构组成&emsp;&emsp;这几年的快速发展，微服务已经变得越来越流行。其中，Spring Cloud 一直在更新，并被大部分公司所使用。代表性的有 Alibaba，2018 年 11 月左右，Spring Cloud 联合创始人 Spencer Gibb 在 Spring 官网的博客页面宣布：阿里巴巴开源 Spring Cloud Alibaba，并发布了首个预览版本。随后，Spring Cloud 官方 Twitter 也发布了此消息。Spring Cloud 的版本也很多: Spring Cloud Spring Cloud Alibaba Spring Boot Spring Cloud Hoxton 2.2.0.RELEASE 2.2.X.RELEASE Spring Cloud Greenwich 2.1.1.RELEASE 2.1.X.RELEASE Spring Cloud Finchley 2.0.1.RELEASE 2.0.X.RELEASE Spring Cloud Edgware 1.5.1.RELEASE 1.5.X.RELEASE 以 Spring Boot1.x 为例，主要包括 Eureka、Zuul、Config、Ribbon、Hystrix 等。而在 Spring Boot2.x 中，网关采用了自己的 Gateway。当然在 Alibaba 版本中，其组件更是丰富：使用 Alibaba 的 Nacos 作为注册中心和配置中心。使用自带组件 Sentinel 作为限流、熔断神器。 2、 微服务之网关 2.1 常见的几种网关&emsp;&emsp;目前，在 Spring Boot1.x 中，用到的比较多的网关就是 Zuul。Zuul 是 Netflix 公司开源的一个网关服务，而 Spring Boot2.x 中，采用了自家推出的 Spring Cloud Gateway。 2.2 API 网关的作用&emsp;&emsp;API 网关的主要作用是反向路由、安全认证、负载均衡、限流熔断、日志监控。在 Zuul 中，我们可以通过注入 Bean 的方式来配置路由，也可以在直接通过配置文件来配置: 123zuul.routes.api-d.sensitiveHeaders&#x3D;&quot;*&quot;zuul.routes.api-d.path&#x3D;&#x2F;business&#x2F;api&#x2F;**zuul.routes.api-d.serviceId&#x3D;business-web 我们可以通过网关来做一些安全的认证：如统一鉴权。在 Zuul 中： Zuul 的工作原理 过滤器机制 &emsp;&emsp;zuul 的核心是一系列的 filters, 其作用可以类比 Servlet 框架的 Filter，或者 AOP。zuul 把 Request route 到用户处理逻辑的过程中，这些 filter 参与一些过滤处理，比如 Authentication，Load Shedding 等。几种标准的过滤器类型： &emsp;&emsp;(1) PRE：这种过滤器在请求被路由之前调用。我们可利用这种过滤器实现身份验证、在集群中选择请求的微服务、记录调试信息等。 &emsp;&emsp;(2) ROUTING：这种过滤器用于构建发送给微服务的请求，并使用 Apache HttpClient 或 Netfilx Ribbon 请求微服务。 &emsp;&emsp;(3) POST：这种过滤器在路由到微服务以后执行。这种过滤器可用来为响应添加标准的 HTTP Header、收集统计信息和指标、将响应从微服务发送给客户端等。 &emsp;&emsp;(4) ERROR：在其他阶段发生错误时执行该过滤器。 过滤器的生命周期 &emsp;&emsp;filterOrder：通过 int 值来定义过滤器的执行顺序，越小优先级越高。 &emsp;&emsp;shouldFilter：返回一个 boolean 类型来判断该过滤器是否要执行，所以通过此函数可实现过滤器的开关。在上例中，我们直接返回 true，所以该过滤器总是生效。 &emsp;&emsp;run：过滤器的具体逻辑。需要注意，这里我们通过 ctx.setSendZuulResponse(false) 令 zuul 过滤该请求，不对其进行路由，然后通过 ctx.setResponseStatusCode(401) 设置了其返回的错误码。 代码示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384@Componentpublic class AccessFilter extends ZuulFilter &#123; private static Logger logger = LoggerFactory.getLogger(AccessFilter.class); @Autowired RedisCacheConfiguration redisCacheConfiguration; @Autowired EnvironmentConfig env; private static final String[] PASS_PATH_ARRAY = &#123; \"/login\", \"openProject\" &#125;; @Override public String filterType() &#123; return \"pre\"; &#125; @Override public int filterOrder() &#123; return 0; &#125; @Override public boolean shouldFilter() &#123; return true; &#125; @Override public Object run() &#123; RequestContext ctx = RequestContext.getCurrentContext(); HttpServletRequest request = ctx.getRequest(); HttpServletResponse response = ctx.getResponse(); response.setCharacterEncoding(\"UTF-8\"); response.setHeader(\"content-type\", \"text/html;charset=UTF-8\"); logger.info(\"&#123;&#125; request to &#123;&#125;\", request.getMethod(), request.getRequestURL()); for (String path : PASS_PATH_ARRAY) &#123; if (StringUtils.contains(request.getRequestURL().toString(), path)) &#123; logger.debug(\"request path: &#123;&#125; is pass\", path); return null; &#125; &#125; String token = request.getHeader(\"token\"); if (StringUtils.isEmpty(token)) &#123; logger.warn(\"access token is empty\"); ctx.setSendZuulResponse(false); ctx.setResponseStatusCode(404); ctx.setResponseBody(JSONObject.toJSONString( Response.error(200, -3, \"header param error\", null))); return ctx; &#125; Jedis jedis = null; try &#123; JedisPool jedisPool = redisCacheConfiguration.getJedisPool(); jedis = jedisPool.getResource(); logger.debug(\"zuul gateway service get redisResource success\"); String key = env.getPrefix() + token; String value = jedis.get(key); if (StringUtils.isBlank(value)) &#123; ctx.setSendZuulResponse(false); ctx.setResponseStatusCode(401); ctx.setResponseBody(JSONObject.toJSONString(Response.error(200, -1, \"login timeout\",null))); return ctx; &#125; else &#123; logger.debug(\"access token ok\"); return null; &#125; &#125; catch (Exception e) &#123; logger.error(\"get redisResource failed\"); logger.error(e.getMessage(), e); ctx.setSendZuulResponse(false); ctx.setResponseStatusCode(500); ctx.setResponseBody(JSONObject.toJSONString( Response.error(200, -8, \"redis connect failed\", null))); return ctx; &#125; finally &#123; if (jedis != null) &#123; jedis.close(); &#125; &#125; &#125;&#125; 3、 微服务之服务注册与发现 3.1 常见的几种注册中心&emsp;&emsp;目前常见的几种注册中心有：Eureka、Consul、Nacos，但其实 Kubernetes 也可以实现服务的注册与发现功能，且听下面讲解。 Eureka 的高可用 &emsp;&emsp;在注册中心部署时，有可能出现节点问题，我们先看看 Eureka 集群如何实现高可用，首先配置基础的 Eureka 配置： 1234567891011121314151617181920212223242526spring.application.name&#x3D;eureka-serverserver.port&#x3D;1111spring.profiles.active&#x3D;deveureka.instance.hostname&#x3D;localhosteureka.client.serviceUrl.defaultZone&#x3D;http:&#x2F;&#x2F;$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;&#x2F;eureka&#x2F;logging.path&#x3D;&#x2F;data&#x2F;$&#123;spring.application.name&#125;&#x2F;logseureka.server.enable-self-preservation&#x3D;falseeureka.client.register-with-eureka&#x3D;falseeureka.client.fetch-registry&#x3D;falseeureka.server.eviction-interval-timer-in-ms&#x3D;5000eureka.server.responseCacheUpdateInvervalMs&#x3D;60000eureka.instance.lease-expiration-duration-in-seconds&#x3D;10eureka.instance.lease-renewal-interval-in-seconds&#x3D;3eureka.server.responseCacheAutoExpirationInSeconds&#x3D;180server.undertow.accesslog.enabled&#x3D;falseserver.undertow.accesslog.pattern&#x3D;combined 配置好后，新建一个 application-peer1.properties 文件： 1234spring.application.name&#x3D;eureka-serverserver.port&#x3D;1111eureka.instance.hostname&#x3D;peer1eureka.client.serviceUrl.defaultZone&#x3D;http:&#x2F;&#x2F;peer2:1112&#x2F;eureka&#x2F; application-peer2.properties 文件： 1234spring.application.name&#x3D;eureka-serverserver.port&#x3D;1112eureka.instance.hostname&#x3D;peer2eureka.client.serviceUrl.defaultZone&#x3D;http:&#x2F;&#x2F;peer1:1111&#x2F;eureka&#x2F; 这样通过域名 peer1、peer2 的形式来实现高可用，那么如何配置域名呢？有几种方式： 通过 hosts 来配置域名，vi /etc/hosts: 1210.12.3.2 peer110.12.3.5 peer2 通过 kubernetes 部署服务时来配置域名： 1234567hostAliases:- ip: &quot;10.12.3.2&quot; hostnames: - &quot;peer1&quot;- ip: &quot;10.12.3.5&quot; hostnames: - &quot;peer2&quot; Nacos 实现服务注册、发现 &emsp;&emsp;Nacos 是 Alibaba 推出来的，目前最新版本是 v1.2.1。其功能可以实现服务的注册、发现，也可以作为配置管理来提供配置服务。可以手动去官网下载安装，Nacos 地址：https://github.com/alibaba/nacos/releases。 执行，Linux/Unix/Mac： 1sh startup.sh -m standalone Windows： 1cmd startup.cmd -m standalone 当我们引入 Nacos 相关配置时，即可使用它： 123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; 注意：下面这个配置文件需要是 bootstrap，否则可能失败，至于为什么，大家可以自己试试。12345678910spring: application: name: oauth-cas cloud: nacos: discovery: server-addr: 127.0.0.1:8848 config: server-addr: 127.0.0.1:8848 refreshable-dataids: actuator.properties,log.properties 配置完成后，完成 main： 1234567891011121314151617package com.damon;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;@Configuration@EnableAutoConfiguration@ComponentScan(basePackages &#x3D; &#123;&quot;com.damon&quot;&#125;)@EnableDiscoveryClientpublic class CasApp &#123; public static void main(String[] args) &#123; SpringApplication.run(CasApp.class, args); &#125;&#125; 完成以上，我们运行启动类，我们打开 Nacos 登录后，打开服务列表，即可看到： Kubernetes 服务注册与发现 &emsp;&emsp;接下来，请允许我为大家引入 Kubernetes 的服务注册与发现功能，spring-cloud-kubernetes 的 DiscoveryClient 服务将 Kubernetes 中的 \"Service\" 资源与 Spring Cloud 中的服务对应起来了，有了这个 DiscoveryClient，我们在 Kubernetes 环境下就不需要 Eureka 等来做注册发现了，而是直接使用 Kubernetes 的服务机制。 在 pom.xml 中，有对 spring-cloud-kubernetes 框架的依赖配置： 123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-kubernetes-core&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-kubernetes-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 为何 spring-cloud-kubernetes 可以完成服务注册发现呢？首先，创建一个 Spring Boot 项目的启动类，且引入服务发现注解 @EnableDiscoveryClient，同时需要开启服务发现： 1234567spring: application: name: edge-admin cloud: kubernetes: discovery: all-namespaces: true 开启后，我们打开spring-cloud-kubernetes-discovery的源码，地址是：https://github.com/spring-cloud/spring-cloud-kubernetes/tree/master/spring-cloud-kubernetes-discovery，看到内容： 为什么要看这个文件呢？因为 spring 容器启动时，会寻找 classpath 下所有 spring.factories 文件(包括 jar 文件中的)，spring.factories 中配置的所有类都会实例化，我们在开发 springboot 时常用到的***-starter.jar 就用到了这个技术，效果是一旦依赖了某个 starter.jar 很多功能就在 spring 初始化时候自动执行。 spring.factories 文件中有两个类：KubernetesDiscoveryClientAutoConfiguration 和 KubernetesDiscoveryClientConfigClientBootstrapConfiguration 都会被实例化。先看 KubernetesDiscoveryClientConfigClientBootstrapConfiguration，KubernetesAutoConfiguration 和 KubernetesDiscoveryClientAutoConfiguration 这两个类会被实例化： 12345678910111213141516 * Copyright 2013-2019 the original author or authors.package org.springframework.cloud.kubernetes.discovery;import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;import org.springframework.cloud.kubernetes.KubernetesAutoConfiguration;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Import;@Configuration@ConditionalOnProperty(\"spring.cloud.config.discovery.enabled\")@Import(&#123; KubernetesAutoConfiguration.class, KubernetesDiscoveryClientAutoConfiguration.class &#125;)public class KubernetesDiscoveryClientConfigClientBootstrapConfiguration &#123;&#125; 再看 KubernetesAutoConfiguration 的源码，会实例化一个重要的类 DefaultKubernetesClient，如下： 12345@Bean@ConditionalOnMissingBeanpublic KubernetesClient kubernetesClient(Config config) &#123; return new DefaultKubernetesClient(config);&#125; 最后我们再看 KubernetesDiscoveryClientAutoConfiguration 源码，注意 kubernetesDiscoveryClient 方法，这里面是接口实现的重点，还要重点关注的地方是 KubernetesClient 参数的值，是上面提到的 DefaultKubernetesClient 对象： 123456789@Bean@ConditionalOnMissingBean@ConditionalOnProperty(name = \"spring.cloud.kubernetes.discovery.enabled\", matchIfMissing = true)public KubernetesDiscoveryClient kubernetesDiscoveryClient(KubernetesClient client, KubernetesDiscoveryProperties properties, KubernetesClientServicesFunction kubernetesClientServicesFunction, DefaultIsServicePortSecureResolver isServicePortSecureResolver) &#123; return new KubernetesDiscoveryClient(client, properties, kubernetesClientServicesFunction, isServicePortSecureResolver);&#125; 接下来，我们看 spring-cloud-kubernetes 中的 KubernetesDiscoveryClient.java，看方法： 12345public List&lt;String&gt; getServices(Predicate&lt;Service&gt; filter) &#123; return this.kubernetesClientServicesFunction.apply(this.client).list().getItems() .stream().filter(filter).map(s -&gt; s.getMetadata().getName()) .collect(Collectors.toList());&#125; 在 apply(this.client).list()，可以看到数据源其实就是 this.client，并且 KubernetesClientServicesFunction 实例化时： 123456789@Beanpublic KubernetesClientServicesFunction servicesFunction( KubernetesDiscoveryProperties properties) &#123; if (properties.getServiceLabels().isEmpty()) &#123; return KubernetesClient::services; &#125; return (client) -&gt; client.services().withLabels(properties.getServiceLabels());&#125; 调用其 services 方法的返回结果，KubernetesDiscoveryClient.getServices 方法中的 this.client 是什么呢？在前面的分析时已经提到了，就是 DefaultKubernetesClient 类的实例，所以，此时要去去看 DefaultKubernetesClient.services 方法，发现 client 是 ServiceOperationsImpl： 1234@Override public MixedOperation&lt;Service, ServiceList, DoneableService, ServiceResource&lt;Service, DoneableService&gt;&gt; services() &#123; return new ServiceOperationsImpl(httpClient, getConfiguration(), getNamespace()); &#125; 接着我们在实例 ServiceOperationsImpl 中看其 list 函数： 12345678910111213141516171819202122public L list() throws KubernetesClientException &#123; try &#123; HttpUrl.Builder requestUrlBuilder &#x3D; HttpUrl.get(getNamespacedUrl()).newBuilder(); String labelQueryParam &#x3D; getLabelQueryParam(); if (Utils.isNotNullOrEmpty(labelQueryParam)) &#123; requestUrlBuilder.addQueryParameter(&quot;labelSelector&quot;, labelQueryParam); &#125; String fieldQueryString &#x3D; getFieldQueryParam(); if (Utils.isNotNullOrEmpty(fieldQueryString)) &#123; requestUrlBuilder.addQueryParameter(&quot;fieldSelector&quot;, fieldQueryString); &#125; Request.Builder requestBuilder &#x3D; new Request.Builder().get().url(requestUrlBuilder.build()); L answer &#x3D; handleResponse(requestBuilder, listType); updateApiVersion(answer); return answer; &#125; catch (InterruptedException | ExecutionException | IOException e) &#123; throw KubernetesClientException.launderThrowable(forOperationType(&quot;list&quot;), e); &#125; &#125; 接着展开上面代码的 handleResponse 函数，可见里面是一次 http 请求，至于请求的地址，可以展开 getNamespacedUrl() 方法，里面调用的 getRootUrl 方法如下： 12345678910public URL getRootUrl() &#123; try &#123; if (apiGroup !&#x3D; null) &#123; return new URL(URLUtils.join(config.getMasterUrl().toString(), &quot;apis&quot;, apiGroup, apiVersion)); &#125; return new URL(URLUtils.join(config.getMasterUrl().toString(), &quot;api&quot;, apiVersion)); &#125; catch (MalformedURLException e) &#123; throw KubernetesClientException.launderThrowable(e); &#125; &#125; 我们看到逻辑中，貌似了解到其结果是这样的格式： 1xxx&#x2F;api&#x2F;version 或 xxx&#x2F;apis&#x2F;xxx&#x2F;version 看到这样的结果，感觉比较像访问 kubernetes 的 API Server 时用的 URL 标准格式，有关 API Server 服务的详情请参考官方文档，地址是: https://kubernetes.io/docs/reference/using-api/api-concepts/。 弄清楚以上，我们发现了其实最终是向 kubernetes 的 API Server 发起 http 请求，获取 Service 资源的数据列表。因此，我们在最后还得在 k8s 底层新建 Service 资源来让其获取： 123456789101112apiVersion: v1kind: Servicemetadata: name: admin-web-service namespace: defaultspec: ports: - name: admin-web01 port: 2001 targetPort: admin-web01 selector: app: admin-web 当然，在部署时，不管是以 Deployment 形式，还是以 DaemonSet 来部署，其最后还是 pod，如果要实现单个服务的多节点部署，可以用： 1kubectl scale --replicas&#x3D;2 deployment admin-web-deployment 总结： spring-cloud-kubernetes 这个组件的服务发现目的就是获取 Kubernetes 中一个或者多个 Namespace 下的所有服务列表，且在过滤列表时候设置过滤的端口号 ，这样获取到服务列表后就能让依赖它们的 Spring Boot 或其它框架的应用完成服务发现工作，让服务能够通过 http://serviceName 这种方式进行访问。 4、 微服务之配置管理 4.1 常见的配置中心 &emsp;&emsp;目前常见的几种配置中心有：Spring Cloud Config、Apollo、Nacos，但其实 Kubernetes 组件 configMap 就可以实现服务的配置管理。并且，在 Spring Boot2.x 中，就已经引入使用了。 Nacos 配置中心 &emsp;&emsp;在上面注册中心中，我们讲到 Nacos，作为注册中心，其实也可以作为配置来管理服务的环境变量。 同样，引入其以依赖： 123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; 同样，注意：下面这个配置文件需要是 bootstrap，否则可能失败。 12345678910spring: application: name: oauth-cas cloud: nacos: discovery: server-addr: 127.0.0.1:8848 config: server-addr: 127.0.0.1:8848 refreshable-dataids: actuator.properties,log.properties 启动类在上面的注册中心已经讲过了，现在看其配置类: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.damon.config;import org.springframework.beans.factory.annotation.Value;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.cloud.context.config.annotation.RefreshScope;import org.springframework.context.annotation.Configuration;import org.springframework.stereotype.Component;@Component@RefreshScopepublic class EnvConfig &#123; @Value(&quot;$&#123;jdbc.driverClassName:&#125;&quot;) private String jdbc_driverClassName; @Value(&quot;$&#123;jdbc.url:&#125;&quot;) private String jdbc_url; @Value(&quot;$&#123;jdbc.username:&#125;&quot;) private String jdbc_username; @Value(&quot;$&#123;jdbc.password:&#125;&quot;) private String jdbc_password; public String getJdbc_driverClassName() &#123; return jdbc_driverClassName; &#125; public void setJdbc_driverClassName(String jdbc_driverClassName) &#123; this.jdbc_driverClassName &#x3D; jdbc_driverClassName; &#125; public String getJdbc_url() &#123; return jdbc_url; &#125; public void setJdbc_url(String jdbc_url) &#123; this.jdbc_url &#x3D; jdbc_url; &#125; public String getJdbc_username() &#123; return jdbc_username; &#125; public void setJdbc_username(String jdbc_username) &#123; this.jdbc_username &#x3D; jdbc_username; &#125; public String getJdbc_password() &#123; return jdbc_password; &#125; public void setJdbc_password(String jdbc_password) &#123; this.jdbc_password &#x3D; jdbc_password; &#125;&#125; 我们通过注解 @Component、@RefreshScope，来实现其配置可被获取。注意 @Value(&quot;${jdbc.username:}&quot;)最后需要冒号的，否则启动后会报错的。 接下来可以配置属性值来，点击配置管理，查看配置： 如果首次打开没有配置，可以新建配置： 编辑配置： 新建完之后，可以编辑，也可以删除，这里就不操作了。 ConfigMap 作为配置管理 &emsp;&emsp;spring-cloud-kubernetes 在上面提供了服务发现的功能，其实它还很强大，也提供了服务的配置管理： 123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes-config&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; 在初始化时，引入注解来自动注入： 123456789101112131415161718192021222324package com.damon;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.boot.autoconfigure.security.oauth2.client.EnableOAuth2Sso;import org.springframework.boot.context.properties.EnableConfigurationProperties;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;import com.damon.config.EnvConfig;@Configuration@EnableAutoConfiguration@ComponentScan(basePackages &#x3D; &#123;&quot;com.damon&quot;&#125;)@EnableConfigurationProperties(EnvConfig.class)@EnableDiscoveryClientpublic class AdminApp &#123; public static void main(String[] args) &#123; SpringApplication.run(AdminApp.class, args); &#125;&#125; 其中，EnvConfig 类来配置环境变量配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150package com.damon.config;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.context.annotation.Configuration;@Configuration@ConfigurationProperties(prefix &#x3D; &quot;damon&quot;)public class EnvConfig &#123; private String message &#x3D; &quot;This is a dummy message&quot;; private String spring_mq_host; private String spring_mq_port; private String spring_mq_user; private String spring_mq_pwd; private String jdbc_driverClassName &#x3D; &quot;com.mysql.jdbc.Driver&quot;; private String jdbc_url &#x3D; &quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;data_test?zeroDateTimeBehavior&#x3D;convertToNull&amp;useUnicode&#x3D;true&amp;characterEncoding&#x3D;utf-8&amp;useSSL&#x3D;false&quot;; private String jdbc_username &#x3D; &quot;root&quot;; private String jdbc_password &#x3D; &quot;wwww&quot;; private String spring_redis_host; private String spring_redis_port; private String spring_redis_pwd; private String base_path; private String chunk_size; private Long expire_time&#x3D; 600000L; public String getMessage() &#123; return this.message; &#125; public void setMessage(String message) &#123; this.message &#x3D; message; &#125; public String getSpring_mq_host() &#123; return spring_mq_host; &#125; public void setSpring_mq_host(String spring_mq_host) &#123; this.spring_mq_host &#x3D; spring_mq_host; &#125; public String getSpring_mq_port() &#123; return spring_mq_port; &#125; public void setSpring_mq_port(String spring_mq_port) &#123; this.spring_mq_port &#x3D; spring_mq_port; &#125; public String getSpring_mq_user() &#123; return spring_mq_user; &#125; public void setSpring_mq_user(String spring_mq_user) &#123; this.spring_mq_user &#x3D; spring_mq_user; &#125; public String getSpring_mq_pwd() &#123; return spring_mq_pwd; &#125; public void setSpring_mq_pwd(String spring_mq_pwd) &#123; this.spring_mq_pwd &#x3D; spring_mq_pwd; &#125; public String getJdbc_driverClassName() &#123; return jdbc_driverClassName; &#125; public void setJdbc_driverClassName(String jdbc_driverClassName) &#123; this.jdbc_driverClassName &#x3D; jdbc_driverClassName; &#125; public String getJdbc_url() &#123; return jdbc_url; &#125; public void setJdbc_url(String jdbc_url) &#123; this.jdbc_url &#x3D; jdbc_url; &#125; public String getJdbc_username() &#123; return jdbc_username; &#125; public void setJdbc_username(String jdbc_username) &#123; this.jdbc_username &#x3D; jdbc_username; &#125; public String getJdbc_password() &#123; return jdbc_password; &#125; public void setJdbc_password(String jdbc_password) &#123; this.jdbc_password &#x3D; jdbc_password; &#125; public String getSpring_redis_host() &#123; return spring_redis_host; &#125; public void setSpring_redis_host(String spring_redis_host) &#123; this.spring_redis_host &#x3D; spring_redis_host; &#125; public String getSpring_redis_port() &#123; return spring_redis_port; &#125; public void setSpring_redis_port(String spring_redis_port) &#123; this.spring_redis_port &#x3D; spring_redis_port; &#125; public String getSpring_redis_pwd() &#123; return spring_redis_pwd; &#125; public void setSpring_redis_pwd(String spring_redis_pwd) &#123; this.spring_redis_pwd &#x3D; spring_redis_pwd; &#125; public String getBase_path() &#123; return base_path; &#125; public void setBase_path(String base_path) &#123; this.base_path &#x3D; base_path; &#125; public String getChunk_size() &#123; return chunk_size; &#125; public void setChunk_size(String chunk_size) &#123; this.chunk_size &#x3D; chunk_size; &#125; public Long getExpire_time() &#123; return expire_time; &#125; public void setExpire_time(Long expire_time) &#123; this.expire_time &#x3D; expire_time; &#125;&#125; 这样，在部署时，我们新建 ConfigMap 类型的资源，同时，会配置其属性值： 1234567891011121314151617181920212223kind: ConfigMapapiVersion: v1metadata: name: admin-webdata: application.yaml: |- damon: message: Say Hello to the World --- spring: profiles: dev damon: message: Say Hello to the Developers --- spring: profiles: test damon: message: Say Hello to the Test --- spring: profiles: prod damon: message: Say Hello to the Prod 并且结合配置，来实现动态更新： 123456789101112131415spring: application: name: admin-web cloud: kubernetes: discovery: all-namespaces: true reload: enabled: true mode: polling period: 500 config: sources: - name: $&#123;spring.application.name&#125; namespace: default 这里是实现自动 500ms 拉取配置，也可以通过事件触发的形式来动态获取最新配置： 123456789101112131415spring: application: name: admin-web cloud: kubernetes: config: sources: - name: $&#123;spring.application.name&#125; namespace: default discovery: all-namespaces: true reload: enabled: true mode: event period: 500 5、 微服务模块划分 5.1 如何划分微服务 &emsp;&emsp;微服务架构设计中，服务拆分的问题很突出，第一种，按照纵向的业务拆分，第二种，横向的功能拆分。 &emsp;&emsp;以电商业务为例，首先按照业务领域的纵向拆分，分为用户微服务、商品微服务、交易微服务、订单微服务等等。 &emsp;&emsp;思考一下： 在纵向拆分仅仅按照业务领域进行拆分是否满足所有的业务场景？结果肯定是否定的。例如用户服务分为用户注册（写）和登录（读）等。写请求的重要性总是大于读请求的，在高并发下，读写比例 10:1，甚至更高的情况下，从而导致了大量的读请求往往会直接影响写请求。为了避免大量的读对写的请求干扰，需要对服务进行读写分离，即用户注册为一个微服务，登录为另一个微服务。此时按照 API 的细粒度继续进行纵向的业务拆分。 &emsp;&emsp;在横向上，按照所请求的功能进行拆分，即对一个请求的生命周期继续进行拆分。请求从用户端发出，首先接受到请求的是网关服务（这里不考虑 nginx 代理网关分发过程），网关服务对请求进行鉴权、参数合法性检查、路由转发等。接下来业务逻辑服务对请求进行业务逻辑的编排处理。对业务数据进行存储和查询就需要数据访问服务，数据访问服务提供了基本的 CRUD 原子操作，并负责海量数据的分库分表，以及屏蔽底层存储的差异性等功能。最后是数据持久化和缓存服务，比如可以采用 MQ、Kafka、Redis Cluster 等。 &emsp;&emsp;微服务架构通过业务的纵向拆分以及功能的横向拆分，服务演化成更小的颗粒度，各服务之间相互解耦，每个服务都可以快速迭代和持续交付（CI/CD），从而在公司层面能够达到降本增效的终极目标。但是服务粒度越细，服务之间的交互就会越来越多，更多的交互会使得服务之间的治理更复杂。服务之间的治理包括服务间的发现、通信、路由、负载均衡、重试机制、限流降级、熔断、链路跟踪等。 5.2 微服务划分的粒度 &emsp;&emsp;微服务划分粒度，其最核心的六个字可能就是：“高内聚、低耦合”。高内聚：就是说每个服务处于同一个网络或网域下，而且相对于外部，整个的是一个封闭的、安全的盒子，宛如一朵玫瑰花。盒子对外的接口是不变的，盒子内部各模块之间的接口也是不变的，但是各模块内部的内容可以更改。模块只对外暴露最小限度的接口，避免强依赖关系。增删一个模块，应该只会影响有依赖关系的相关模块，无关的不应该受影响。 &emsp;&emsp;那么低耦合，这就涉及到我们业务系统的设计了。所谓低耦合：就是要每个业务模块之间的关系降低，减少冗余、重复、交叉的复杂度，模块功能划分也尽可能单一。这样，才能达到低耦合的目的。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"Spring Cloud Alibaba 实战","date":"2020-07-23T00:50:27.000Z","path":"2020/07/23/springcloud-alibaba01/","text":"2018年11月左右，Springcloud 联合创始人Spencer Gibb在Spring官网的博客页面宣布：阿里巴巴开源 Spring Cloud Alibaba，并发布了首个预览版本。随后，Spring Cloud 官方Twitter也发布了此消息。 一、环境准备 Spring Boot: 2.1.8 Spring Cloud: Greenwich.SR3 Spring Cloud Alibaba: 0.9.0.RELEASE Maven: 3.5.4 Java 1.8 + Oauth2 (Spring Security 5.1.6 +) 二、实战 项目模块 主要分为：鉴权中心、服务提供者、服务消费者、网关 实战代码 鉴权中心，依赖pom.xml： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt; &lt;groupId&gt;com.damon&lt;&#x2F;groupId&gt; &lt;artifactId&gt;oauth-cas&lt;&#x2F;artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;&#x2F;version&gt; &lt;packaging&gt;jar&lt;&#x2F;packaging&gt; &lt;name&gt;oauth-cas&lt;&#x2F;name&gt; &lt;url&gt;http:&#x2F;&#x2F;maven.apache.org&lt;&#x2F;url&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;&#x2F;artifactId&gt; &lt;version&gt;2.1.8.RELEASE&lt;&#x2F;version&gt; &lt;relativePath&#x2F;&gt; &lt;&#x2F;parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;&#x2F;project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;&#x2F;java.version&gt; &lt;swagger.version&gt;2.6.1&lt;&#x2F;swagger.version&gt; &lt;xstream.version&gt;1.4.7&lt;&#x2F;xstream.version&gt; &lt;pageHelper.version&gt;4.1.6&lt;&#x2F;pageHelper.version&gt; &lt;fastjson.version&gt;1.2.51&lt;&#x2F;fastjson.version&gt; &lt;!-- &lt;springcloud.version&gt;2.1.8.RELEASE&lt;&#x2F;springcloud.version&gt; --&gt; &lt;springcloud.version&gt;Greenwich.SR3&lt;&#x2F;springcloud.version&gt; &lt;springcloud.kubernetes.version&gt;1.1.1.RELEASE&lt;&#x2F;springcloud.kubernetes.version&gt; &lt;mysql.version&gt;5.1.46&lt;&#x2F;mysql.version&gt; &lt;alibaba-cloud.version&gt;2.1.1.RELEASE&lt;&#x2F;alibaba-cloud.version&gt; &lt;springcloud.alibaba.version&gt;0.9.0.RELEASE&lt;&#x2F;springcloud.alibaba.version&gt; &lt;&#x2F;properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;alibaba-cloud.version&#125;&lt;&#x2F;version&gt; &lt;type&gt;pom&lt;&#x2F;type&gt; &lt;scope&gt;import&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;springcloud.alibaba.version&#125;&lt;&#x2F;version&gt; &lt;type&gt;pom&lt;&#x2F;type&gt; &lt;scope&gt;import&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;springcloud.version&#125;&lt;&#x2F;version&gt; &lt;type&gt;pom&lt;&#x2F;type&gt; &lt;scope&gt;import&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; &lt;&#x2F;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;&#x2F;artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;&#x2F;artifactId&gt; &lt;&#x2F;exclusion&gt; &lt;&#x2F;exclusions&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-undertow&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-oauth2&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;&#x2F;artifactId&gt; &lt;scope&gt;test&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;&#x2F;groupId&gt; &lt;artifactId&gt;jjwt&lt;&#x2F;artifactId&gt; &lt;version&gt;0.9.0&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;cn.hutool&lt;&#x2F;groupId&gt; &lt;artifactId&gt;hutool-all&lt;&#x2F;artifactId&gt; &lt;version&gt;4.6.3&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;&#x2F;groupId&gt; &lt;artifactId&gt;guava&lt;&#x2F;artifactId&gt; &lt;version&gt;19.0&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;&#x2F;groupId&gt; &lt;artifactId&gt;commons-lang3&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-collections&lt;&#x2F;groupId&gt; &lt;artifactId&gt;commons-collections&lt;&#x2F;artifactId&gt; &lt;version&gt;3.2.2&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!-- swagger --&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;&#x2F;groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;swagger.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;&#x2F;groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;swagger.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!--分页插件--&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;&#x2F;groupId&gt; &lt;artifactId&gt;pagehelper&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;pageHelper.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!-- mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;&#x2F;artifactId&gt; &lt;version&gt;1.1.1&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!-- datasource pool--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt; &lt;artifactId&gt;druid&lt;&#x2F;artifactId&gt; &lt;version&gt;1.1.3&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!-- 对redis支持,引入的话项目缓存就支持redis了,所以必须加上redis的相关配置,否则操作相关缓存会报异常 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt; &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;fastjson.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jsoup&lt;&#x2F;groupId&gt; &lt;artifactId&gt;jsoup&lt;&#x2F;artifactId&gt; &lt;version&gt;1.11.3&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; &lt;build&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;&lt;&#x2F;finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;&#x2F;artifactId&gt; &lt;configuration&gt; &lt;jvmArguments&gt;-Dfile.encoding&#x3D;UTF-8&lt;&#x2F;jvmArguments&gt; &lt;fork&gt;true&lt;&#x2F;fork&gt; &lt;&#x2F;configuration&gt; &lt;&#x2F;plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.jacoco&lt;&#x2F;groupId&gt; &lt;artifactId&gt;jacoco-maven-plugin&lt;&#x2F;artifactId&gt; &lt;version&gt;0.7.8&lt;&#x2F;version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;prepare-agent&lt;&#x2F;goal&gt; &lt;goal&gt;report&lt;&#x2F;goal&gt; &lt;&#x2F;goals&gt; &lt;&#x2F;execution&gt; &lt;&#x2F;executions&gt; &lt;&#x2F;plugin&gt; &lt;&#x2F;plugins&gt; &lt;&#x2F;build&gt;&lt;&#x2F;project&gt; 本例中，用到了 Nacos 作为注册中心、配置中心，估需要引入其依赖： 123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; Oauth2 的依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-oauth2&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt; 同时利用 redis 来处理鉴权的信息存储： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt; 接下来需要准备配置文件 yaml： 123456789101112131415161718192021222324252627282930313233343536373839404142434445management: endpoint: restart: enabled: true health: enabled: true info: enabled: truespring: application: name: oauth-cas cloud: nacos: discovery: server-addr: 127.0.0.1:8848 config: server-addr: 127.0.0.1:8848 refreshable-dataids: actuator.properties,log.properties redis: #redis相关配置 database: 8 host: 127.0.0.1 port: 6379 password: qwqwsq jedis: pool: max-active: 8 max-idle: 8 min-idle: 0 timeout: 10000ms http: encoding: charset: UTF-8 enabled: true force: true mvc: throw-exception-if-no-handler-found: true main: allow-bean-definition-overriding: true # 当遇到同样名称时，是否允许覆盖注册logging: path: &#x2F;data&#x2F;$&#123;spring.application.name&#125;&#x2F;logs 注意，这个配置文件需要是 bootstrap，否则可能失败，至于为什么，大家可以自己试试。 接下来就是 application： 1234567891011121314151617181920server: port: 2000 undertow: accesslog: enabled: false pattern: combined servlet: session: timeout: PT120Mclient: http: request: connectTimeout: 8000 readTimeout: 30000mybatis: mapperLocations: classpath:mapper&#x2F;*.xml typeAliasesPackage: com.damon.*.model 配置完成后，完成 main： 123456789101112131415161718192021222324252627package com.damon;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;&#x2F;** * * 配置最多的就是认证服务端，验证账号、密码，存储 token，检查 token ,刷新 token 等都是认证服务端的工作 * @author Damon * @date 2020年1月13日 下午2:29:42 * *&#x2F;@Configuration@EnableAutoConfiguration@ComponentScan(basePackages &#x3D; &#123;&quot;com.damon&quot;&#125;)@EnableDiscoveryClientpublic class CasApp &#123; public static void main(String[] args) &#123; SpringApplication.run(CasApp.class, args); &#125;&#125; 接下来就是配置几个 Oauth2 服务端的几个配置类：AuthorizationServerConfig、ResourceServerConfig、SecurityConfig、RedisTokenStoreConfig、MyRedisTokenStore、UserOAuth2WebResponseExceptionTranslator、AuthenticationEntryPointHandle 等。在 Springcloud Oauth2 进阶篇、Springcloud Oauth2 HA篇 等几篇中已经讲过了。对于相关代码可以关注我的公众号和我互动。 其中最重要的就是登录时的函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899package com.damon.login.service.impl;import java.util.List;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import org.apache.commons.lang3.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.security.core.authority.SimpleGrantedAuthority;import org.springframework.security.core.userdetails.User;import org.springframework.security.core.userdetails.UserDetails;import org.springframework.security.core.userdetails.UsernameNotFoundException;import org.springframework.security.crypto.password.PasswordEncoder;import org.springframework.security.oauth2.common.OAuth2AccessToken;import org.springframework.security.oauth2.provider.token.ConsumerTokenServices;import org.springframework.stereotype.Service;import com.damon.commons.Response;import com.damon.constant.Constant;import com.damon.constant.LoginEnum;import com.damon.exception.InnerErrorException;import com.damon.login.dao.UserMapper;import com.damon.login.model.SysUser;import com.damon.login.service.LoginService;import com.damon.utils.IpUtil;import com.google.common.collect.Lists;&#x2F;** * @author wangshoufa * @date 2018年11月15日 下午12:01:53 * *&#x2F;@Servicepublic class LoginServiceImpl implements LoginService &#123; Logger logger &#x3D; LoggerFactory.getLogger(LoginServiceImpl.class); &#x2F;&#x2F;private List&lt;User&gt; userList; @Autowired private PasswordEncoder passwordEncoder; @Autowired private UserMapper userMapper; @Autowired private HttpServletRequest req; &#x2F;** * Auth * 登录认证 * 实际中从数据库获取信息 * 这里为了做演示，把用户名、密码和所属角色都写在代码里了，正式环境中，这里应该是从数据库或者其他地方根据用户名将加密后的密码及所属角色查出来的。账号 damon ， * 密码123456，稍后在换取 token 的时候会用到。并且给这个用户设置 &quot;ROLE_ADMIN&quot; 角色。 * *&#x2F; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; logger.info(&quot;clientIp is: &#123;&#125; ,username: &#123;&#125;&quot;, IpUtil.getClientIp(req), username); logger.info(&quot;serverIp is: &#123;&#125;&quot;, IpUtil.getCurrentIp()); &#x2F;&#x2F; 查询数据库操作 try &#123; SysUser user &#x3D; userMapper.getUserByUsername(username); if (user &#x3D;&#x3D; null) &#123; logger.error(&quot;user not exist&quot;); throw new UsernameNotFoundException(&quot;username is not exist&quot;); &#x2F;&#x2F;throw new UsernameNotFoundException(&quot;the user is not found&quot;); &#125; else &#123; &#x2F;&#x2F; 用户角色也应在数据库中获取，这里简化 String role &#x3D; &quot;&quot;; if(user.getIsAdmin() &#x3D;&#x3D; 1) &#123; role &#x3D; &quot;admin&quot;; &#125; List&lt;SimpleGrantedAuthority&gt; authorities &#x3D; Lists.newArrayList(); authorities.add(new SimpleGrantedAuthority(role)); &#x2F;&#x2F;String password &#x3D; passwordEncoder.encode(&quot;123456&quot;);&#x2F;&#x2F; 123456是密码 &#x2F;&#x2F;return new User(username, password, authorities); &#x2F;&#x2F; 线上环境应该通过用户名查询数据库获取加密后的密码 return new User(username, user.getPassword(), authorities); &#125; &#125; catch (Exception e) &#123; logger.error(&quot;database collect failed&quot;); logger.error(e.getMessage(), e); throw new UsernameNotFoundException(e.getMessage()); &#125; &#125; &#125; 函数 loadUserByUsername 需要验证数据库的密码，并且给用户授权角色。 到此，鉴权中心服务端完成。上面说的利用了 Nacos 来作为注册中心被客户端服务发现，并提供配置管理。 下载 Nacos 地址：https://github.com/alibaba/nacos/releases 版本：v1.2.1 执行： Linux/Unix/Mac：sh startup.sh -m standalone Windows：cmd startup.cmd -m standalone 启动完成之后，访问：http://127.0.0.1:8848/nacos/，可以进入Nacos的服务管理页面，具体如下： 默认用户名与密码都是nacos。 登陆后打开服务管理，可以看到注册到 Nacos 的服务列表： 可以点击配置管理，查看配置： 如果没有配置任何服务的配置，可以新建： 上面讲述了Nacos 如何作为注册中心与配置中心的，很简单吧。 接下来我们讲解服务提供者代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt; &lt;groupId&gt;com.damon&lt;&#x2F;groupId&gt; &lt;artifactId&gt;provider-service&lt;&#x2F;artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;&#x2F;version&gt; &lt;packaging&gt;jar&lt;&#x2F;packaging&gt; &lt;name&gt;provider-service&lt;&#x2F;name&gt; &lt;url&gt;http:&#x2F;&#x2F;maven.apache.org&lt;&#x2F;url&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;&#x2F;artifactId&gt; &lt;version&gt;2.1.8.RELEASE&lt;&#x2F;version&gt; &lt;relativePath&#x2F;&gt; &lt;&#x2F;parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;&#x2F;project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;&#x2F;java.version&gt; &lt;swagger.version&gt;2.6.1&lt;&#x2F;swagger.version&gt; &lt;xstream.version&gt;1.4.7&lt;&#x2F;xstream.version&gt; &lt;pageHelper.version&gt;4.1.6&lt;&#x2F;pageHelper.version&gt; &lt;fastjson.version&gt;1.2.51&lt;&#x2F;fastjson.version&gt; &lt;!-- &lt;springcloud.version&gt;2.1.8.RELEASE&lt;&#x2F;springcloud.version&gt; --&gt; &lt;springcloud.version&gt;Greenwich.SR3&lt;&#x2F;springcloud.version&gt; &lt;springcloud.kubernetes.version&gt;1.1.1.RELEASE&lt;&#x2F;springcloud.kubernetes.version&gt; &lt;mysql.version&gt;5.1.46&lt;&#x2F;mysql.version&gt; &lt;alibaba-cloud.version&gt;2.1.1.RELEASE&lt;&#x2F;alibaba-cloud.version&gt; &lt;springcloud.alibaba.version&gt;0.9.0.RELEASE&lt;&#x2F;springcloud.alibaba.version&gt; &lt;&#x2F;properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;alibaba-cloud.version&#125;&lt;&#x2F;version&gt; &lt;type&gt;pom&lt;&#x2F;type&gt; &lt;scope&gt;import&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;springcloud.alibaba.version&#125;&lt;&#x2F;version&gt; &lt;type&gt;pom&lt;&#x2F;type&gt; &lt;scope&gt;import&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;springcloud.version&#125;&lt;&#x2F;version&gt; &lt;type&gt;pom&lt;&#x2F;type&gt; &lt;scope&gt;import&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; &lt;&#x2F;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;&#x2F;artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;&#x2F;artifactId&gt; &lt;&#x2F;exclusion&gt; &lt;&#x2F;exclusions&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-undertow&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-oauth2&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;&#x2F;artifactId&gt; &lt;scope&gt;test&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt; &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;fastjson.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!-- swagger --&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;&#x2F;groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;swagger.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;&#x2F;groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;swagger.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;&#x2F;groupId&gt; &lt;artifactId&gt;commons-lang3&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-collections&lt;&#x2F;groupId&gt; &lt;artifactId&gt;commons-collections&lt;&#x2F;artifactId&gt; &lt;version&gt;3.2.2&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!--分页插件--&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;&#x2F;groupId&gt; &lt;artifactId&gt;pagehelper&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;pageHelper.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!-- mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;&#x2F;artifactId&gt; &lt;version&gt;1.1.1&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!-- datasource pool--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt; &lt;artifactId&gt;druid&lt;&#x2F;artifactId&gt; &lt;version&gt;1.1.3&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!-- 对redis支持,引入的话项目缓存就支持redis了,所以必须加上redis的相关配置,否则操作相关缓存会报异常 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;&#x2F;groupId&gt; &lt;artifactId&gt;guava&lt;&#x2F;artifactId&gt; &lt;version&gt;19.0&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;&#x2F;groupId&gt; &lt;artifactId&gt;jjwt&lt;&#x2F;artifactId&gt; &lt;version&gt;0.9.0&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; &lt;build&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;&lt;&#x2F;finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;&#x2F;artifactId&gt; &lt;configuration&gt; &lt;jvmArguments&gt;-Dfile.encoding&#x3D;UTF-8&lt;&#x2F;jvmArguments&gt; &lt;fork&gt;true&lt;&#x2F;fork&gt; &lt;&#x2F;configuration&gt; &lt;&#x2F;plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.jacoco&lt;&#x2F;groupId&gt; &lt;artifactId&gt;jacoco-maven-plugin&lt;&#x2F;artifactId&gt; &lt;version&gt;0.7.8&lt;&#x2F;version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;prepare-agent&lt;&#x2F;goal&gt; &lt;goal&gt;report&lt;&#x2F;goal&gt; &lt;&#x2F;goals&gt; &lt;&#x2F;execution&gt; &lt;&#x2F;executions&gt; &lt;&#x2F;plugin&gt; &lt;!-- 自动生成代码 插件 begin --&gt; &lt;!-- &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;&#x2F;artifactId&gt; &lt;version&gt;1.3.2&lt;&#x2F;version&gt; &lt;configuration&gt; &lt;configurationFile&gt;src&#x2F;main&#x2F;resources&#x2F;generatorConfig.xml&lt;&#x2F;configurationFile&gt; &lt;verbose&gt;true&lt;&#x2F;verbose&gt; &lt;overwrite&gt;true&lt;&#x2F;overwrite&gt; &lt;&#x2F;configuration&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;&#x2F;artifactId&gt; &lt;version&gt;1.3.2&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; &lt;&#x2F;plugin&gt; --&gt; &lt;&#x2F;plugins&gt; &lt;&#x2F;build&gt;&lt;&#x2F;project&gt; 一如既往的引入依赖。 配置 bootstrap 文件； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113management: endpoint: restart: enabled: true health: enabled: true info: enabled: truespring: application: name: provider-service cloud: nacos: discovery: server-addr: 127.0.0.1:8848 config: server-addr: 127.0.0.1:8848 refreshable-dataids: actuator.properties,log.properties http: encoding: charset: UTF-8 enabled: true force: true mvc: throw-exception-if-no-handler-found: true main: allow-bean-definition-overriding: true #当遇到同样名称时，是否允许覆盖注册logging: path: &#x2F;data&#x2F;$&#123;spring.application.name&#125;&#x2F;logscas-server-url: http:&#x2F;&#x2F;oauth-cas #http:&#x2F;&#x2F;localhost:2000#设置可以访问的地址security: oauth2: #与cas对应的配置 client: client-id: provider-service client-secret: provider-service-123 user-authorization-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;authorize #是授权码认证方式需要的 access-token-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;token #是密码模式需要用到的获取 token 的接口 resource: loadBalanced: true #jwt: #jwt存储token时开启 #key-uri: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;token_key #key-value: test_jwt_sign_key id: provider-service #指定用户信息地址 user-info-uri: $&#123;cas-server-url&#125;&#x2F;api&#x2F;user #指定user info的URI，原生地址后缀为&#x2F;auth&#x2F;user prefer-token-info: false #token-info-uri: authorization: check-token-access: $&#123;cas-server-url&#125;&#x2F;oauth&#x2F;check_token #当此web服务端接收到来自UI客户端的请求后，需要拿着请求中的 token 到认证服务端做 token 验证，就是请求的这个接口application 文件；server: port: 2001 undertow: accesslog: enabled: false pattern: combined servlet: session: timeout: PT120M cookie: name: PROVIDER-SERVICE-SESSIONID #防止Cookie冲突，冲突会导致登录验证不通过client: http: request: connectTimeout: 8000 readTimeout: 30000mybatis: mapperLocations: classpath:mapper&#x2F;*.xml typeAliasesPackage: com.damon.*.modelbackend: ribbon: client: enabled: true ServerListRefreshInterval: 5000ribbon: ConnectTimeout: 3000 # 设置全局默认的ribbon的读超时 ReadTimeout: 1000 eager-load: enabled: true clients: oauth-cas,consumer-service MaxAutoRetries: 1 #对第一次请求的服务的重试次数 MaxAutoRetriesNextServer: 1 #要重试的下一个服务的最大数量（不包括第一个服务） #listOfServers: localhost:5556,localhost:5557 #ServerListRefreshInterval: 2000 OkToRetryOnAllOperations: true NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RoundRobinRulehystrix.command.BackendCall.execution.isolation.thread.timeoutInMilliseconds: 5000hystrix.threadpool.BackendCallThread.coreSize: 5 接下来启动类： 123456789101112131415161718192021222324252627282930313233package com.damon;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.boot.autoconfigure.security.oauth2.client.EnableOAuth2Sso;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;&#x2F;** * @author Damon * @date 2020年1月13日 下午3:23:06 * *&#x2F;@Configuration@EnableAutoConfiguration@ComponentScan(basePackages &#x3D; &#123;&quot;com.damon&quot;&#125;)@EnableDiscoveryClient@EnableOAuth2Ssopublic class ProviderApp &#123; public static void main(String[] args) &#123; SpringApplication.run(ProviderApp.class, args); &#125;&#125; 注意：注解 @EnableDiscoveryClient、@EnableOAuth2Sso 都需要。 这时，同样需要配置 ResourceServerConfig、SecurityConfig。 如果需要数据库，可以加上： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112package com.damon.config;import java.util.Properties;import javax.sql.DataSource;import org.apache.ibatis.plugin.Interceptor;import org.apache.ibatis.session.SqlSessionFactory;import org.mybatis.spring.SqlSessionFactoryBean;import org.mybatis.spring.SqlSessionTemplate;import org.mybatis.spring.annotation.MapperScan;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.core.env.Environment;import org.springframework.core.io.support.PathMatchingResourcePatternResolver;import org.springframework.jdbc.datasource.DataSourceTransactionManager;import org.springframework.stereotype.Component;import org.springframework.transaction.annotation.EnableTransactionManagement;import com.alibaba.druid.pool.DruidDataSourceFactory;import com.github.pagehelper.PageHelper;&#x2F;***** created by wangshoufa* 2018年5月23日 下午7:39:37**&#x2F;@Component@Configuration@EnableTransactionManagement@MapperScan(&quot;com.damon.*.dao&quot;)public class MybaitsConfig &#123; @Autowired private EnvConfig envConfig; @Autowired private Environment env; @Bean(name &#x3D; &quot;dataSource&quot;) public DataSource getDataSource() throws Exception &#123; Properties props &#x3D; new Properties(); props.put(&quot;driverClassName&quot;, envConfig.getJdbc_driverClassName()); props.put(&quot;url&quot;, envConfig.getJdbc_url()); props.put(&quot;username&quot;, envConfig.getJdbc_username()); props.put(&quot;password&quot;, envConfig.getJdbc_password()); return DruidDataSourceFactory.createDataSource(props); &#125; @Bean public SqlSessionFactory sqlSessionFactory(@Qualifier(&quot;dataSource&quot;) DataSource dataSource) throws Exception &#123; SqlSessionFactoryBean fb &#x3D; new SqlSessionFactoryBean(); &#x2F;&#x2F; 指定数据源(这个必须有，否则报错) fb.setDataSource(dataSource); &#x2F;&#x2F; 下边两句仅仅用于*.xml文件，如果整个持久层操作不需要使用到xml文件的话（只用注解就可以搞定），则不加 fb.setTypeAliasesPackage(env.getProperty(&quot;mybatis.typeAliasesPackage&quot;));&#x2F;&#x2F; 指定基包 fb.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(env.getProperty(&quot;mybatis.mapperLocations&quot;)));&#x2F;&#x2F; 指定xml文件位置 &#x2F;&#x2F; 分页插件 PageHelper pageHelper &#x3D; new PageHelper(); Properties props &#x3D; new Properties(); &#x2F;&#x2F; 启用合理化时，如果pageNum&lt;1会查询第一页，如果pageNum&gt;pages会查询最后一页 &#x2F;&#x2F;禁用合理化时，如果pageNum&lt;1或pageNum&gt;pages会返回空数据 props.setProperty(&quot;reasonable&quot;, &quot;true&quot;); &#x2F;&#x2F;指定数据库 props.setProperty(&quot;dialect&quot;, &quot;mysql&quot;); &#x2F;&#x2F;支持通过Mapper接口参数来传递分页参数 props.setProperty(&quot;supportMethodsArguments&quot;, &quot;true&quot;); &#x2F;&#x2F;总是返回PageInfo类型,check检查返回类型是否为PageInfo,none返回Page props.setProperty(&quot;returnPageInfo&quot;, &quot;check&quot;); props.setProperty(&quot;params&quot;, &quot;count&#x3D;countSql&quot;); pageHelper.setProperties(props); &#x2F;&#x2F; 添加插件 fb.setPlugins(new Interceptor[] &#123; pageHelper &#125;); try &#123; return fb.getObject(); &#125; catch (Exception e) &#123; throw e; &#125; &#125; &#x2F;** * 配置事务管理器 * @param dataSource * @return * @throws Exception *&#x2F; @Bean public DataSourceTransactionManager transactionManager(DataSource dataSource) throws Exception &#123; return new DataSourceTransactionManager(dataSource); &#125; @Bean public SqlSessionTemplate sqlSessionTemplate(SqlSessionFactory sqlSessionFactory) &#123; return new SqlSessionTemplate(sqlSessionFactory); &#125;&#125; 接下来新写一个 controller 类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.damon.user.controller;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.security.access.prepost.PreAuthorize;import org.springframework.security.core.Authentication;import org.springframework.security.oauth2.provider.authentication.OAuth2AuthenticationDetails;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import com.damon.commons.Response;import com.damon.user.service.UserService;&#x2F;** * * * @author Damon * @date 2020年1月13日 下午3:31:07 * *&#x2F;@RestController@RequestMapping(&quot;&#x2F;api&#x2F;user&quot;)public class UserController &#123; private static final Logger logger &#x3D; LoggerFactory.getLogger(UserController.class); @Autowired private UserService userService; @GetMapping(&quot;&#x2F;getCurrentUser&quot;) @PreAuthorize(&quot;hasAuthority(&#39;admin&#39;)&quot;) public Object getCurrentUser(Authentication authentication) &#123; logger.info(&quot;test password mode&quot;); return authentication; &#125; @PreAuthorize(&quot;hasAuthority(&#39;admin&#39;)&quot;) @GetMapping(&quot;&#x2F;auth&#x2F;admin&quot;) public Object adminAuth() &#123; logger.info(&quot;test password mode&quot;); return &quot;Has admin auth!&quot;; &#125; @GetMapping(value &#x3D; &quot;&#x2F;get&quot;) @PreAuthorize(&quot;hasAuthority(&#39;admin&#39;)&quot;) &#x2F;&#x2F;@PreAuthorize(&quot;hasRole(&#39;admin&#39;)&quot;)&#x2F;&#x2F;无效 public Object get(Authentication authentication)&#123; &#x2F;&#x2F;Authentication authentication &#x3D; SecurityContextHolder.getContext().getAuthentication(); authentication.getCredentials(); OAuth2AuthenticationDetails details &#x3D; (OAuth2AuthenticationDetails)authentication.getDetails(); String token &#x3D; details.getTokenValue(); return token; &#125; @GetMapping(&quot;&#x2F;getUserInfo&quot;) @PreAuthorize(&quot;hasAuthority(&#39;admin&#39;)&quot;) public Response&lt;Object&gt; getUserInfo(Authentication authentication) &#123; logger.info(&quot;test password mode&quot;); Object principal &#x3D; authentication.getPrincipal(); if(principal instanceof String) &#123; String username &#x3D; (String) principal; return userService.getUserByUsername(username); &#125; return null; &#125;&#125; 基本上一个代码就完成了。接下来测试一下： 认证： 1curl -i -X POST -d &quot;username&#x3D;admin&amp;password&#x3D;123456&amp;grant_type&#x3D;password&amp;client_id&#x3D;provider-service&amp;client_secret&#x3D;provider-service-123&quot; http:&#x2F;&#x2F;localhost:5555&#x2F;oauth-cas&#x2F;oauth&#x2F;token 拿到token后： 1curl -i -H &quot;Accept: application&#x2F;json&quot; -H &quot;Authorization:bearer f4a42baa-a24a-4342-a00b-32cb135afce9&quot; -X GET http:&#x2F;&#x2F;localhost:5555&#x2F;provider-service&#x2F;api&#x2F;user&#x2F;getCurrentUser 这里用到了 5555 端口，这是一个网关服务，好吧，既然提到这个，我们接下来看网关吧，引入依赖： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt; &lt;groupId&gt;com.damon&lt;&#x2F;groupId&gt; &lt;artifactId&gt;alibaba-gateway&lt;&#x2F;artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;&#x2F;version&gt; &lt;packaging&gt;jar&lt;&#x2F;packaging&gt; &lt;name&gt;alibaba-gateway&lt;&#x2F;name&gt; &lt;url&gt;http:&#x2F;&#x2F;maven.apache.org&lt;&#x2F;url&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;&#x2F;artifactId&gt; &lt;version&gt;2.1.8.RELEASE&lt;&#x2F;version&gt; &lt;relativePath&#x2F;&gt; &lt;&#x2F;parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;&#x2F;project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;&#x2F;java.version&gt; &lt;swagger.version&gt;2.6.1&lt;&#x2F;swagger.version&gt; &lt;xstream.version&gt;1.4.7&lt;&#x2F;xstream.version&gt; &lt;pageHelper.version&gt;4.1.6&lt;&#x2F;pageHelper.version&gt; &lt;fastjson.version&gt;1.2.51&lt;&#x2F;fastjson.version&gt; &lt;!-- &lt;springcloud.version&gt;2.1.8.RELEASE&lt;&#x2F;springcloud.version&gt; --&gt; &lt;springcloud.version&gt;Greenwich.SR3&lt;&#x2F;springcloud.version&gt; &lt;springcloud.kubernetes.version&gt;1.1.1.RELEASE&lt;&#x2F;springcloud.kubernetes.version&gt; &lt;mysql.version&gt;5.1.46&lt;&#x2F;mysql.version&gt; &lt;alibaba-cloud.version&gt;2.1.1.RELEASE&lt;&#x2F;alibaba-cloud.version&gt; &lt;springcloud.alibaba.version&gt;0.9.0.RELEASE&lt;&#x2F;springcloud.alibaba.version&gt; &lt;&#x2F;properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;alibaba-cloud.version&#125;&lt;&#x2F;version&gt; &lt;type&gt;pom&lt;&#x2F;type&gt; &lt;scope&gt;import&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;springcloud.alibaba.version&#125;&lt;&#x2F;version&gt; &lt;type&gt;pom&lt;&#x2F;type&gt; &lt;scope&gt;import&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;springcloud.version&#125;&lt;&#x2F;version&gt; &lt;type&gt;pom&lt;&#x2F;type&gt; &lt;scope&gt;import&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; &lt;&#x2F;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- 不要依赖spring-boot-starter-web，会和spring-cloud-starter-gateway冲突，启动时异常 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;!--基于 reactive stream 的redis --&gt; &lt;!-- &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis-reactive&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-commons&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;&#x2F;artifactId&gt; &lt;scope&gt;test&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt; &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;fastjson.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;&#x2F;groupId&gt; &lt;artifactId&gt;guava&lt;&#x2F;artifactId&gt; &lt;version&gt;19.0&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; &lt;build&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;&lt;&#x2F;finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;&#x2F;artifactId&gt; &lt;configuration&gt; &lt;jvmArguments&gt;-Dfile.encoding&#x3D;UTF-8&lt;&#x2F;jvmArguments&gt; &lt;fork&gt;true&lt;&#x2F;fork&gt; &lt;&#x2F;configuration&gt; &lt;&#x2F;plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.jacoco&lt;&#x2F;groupId&gt; &lt;artifactId&gt;jacoco-maven-plugin&lt;&#x2F;artifactId&gt; &lt;version&gt;0.7.8&lt;&#x2F;version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;prepare-agent&lt;&#x2F;goal&gt; &lt;goal&gt;report&lt;&#x2F;goal&gt; &lt;&#x2F;goals&gt; &lt;&#x2F;execution&gt; &lt;&#x2F;executions&gt; &lt;&#x2F;plugin&gt; &lt;!-- 自动生成代码 插件 begin --&gt; &lt;!-- &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;&#x2F;artifactId&gt; &lt;version&gt;1.3.2&lt;&#x2F;version&gt; &lt;configuration&gt; &lt;configurationFile&gt;src&#x2F;main&#x2F;resources&#x2F;generatorConfig.xml&lt;&#x2F;configurationFile&gt; &lt;verbose&gt;true&lt;&#x2F;verbose&gt; &lt;overwrite&gt;true&lt;&#x2F;overwrite&gt; &lt;&#x2F;configuration&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;&#x2F;artifactId&gt; &lt;version&gt;1.3.2&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; &lt;&#x2F;plugin&gt; --&gt; &lt;&#x2F;plugins&gt; &lt;&#x2F;build&gt;&lt;&#x2F;project&gt; 同样利用 Nacos 来发现服务。 相关配置在 Spring Cloud Kubernetes之实战三网关Gateway 一文中有讲过，这里的注册配置改为： 1234567891011121314spring: cloud: gateway: discovery: locator: enabled: true #并且我们并没有给每一个服务单独配置路由 而是使用了服务发现自动注册路由的方式 lowerCaseServiceId: true nacos: discovery: server-addr: 127.0.0.1:8848 config: server-addr: 127.0.0.1:8848 refreshable-dataids: actuator.properties,log.properties 前面用的是 kubernetes。 好了，网关配置好后，启动在 Nacos dashboard可以看到该服务，表示注册服务成功。接下来就可以利用其来调用其他服务了。具体 curl 命令： 1curl -i -H &quot;Accept: application&#x2F;json&quot; -H &quot;Authorization:bearer f4a42baa-a24a-4342-a00b-32cb135afce9&quot; -X GET http:&#x2F;&#x2F;localhost:5555&#x2F;consumer-service&#x2F;api&#x2F;order&#x2F;getUserInfo Ok，到此鉴权中心、服务提供者、服务消费者、服务的注册与发现、配置中心等功能已完成。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"微服务架构设计之解耦合","date":"2020-07-23T00:47:27.000Z","path":"2020/07/23/micro-service02/","text":"背景在各个 IT 行业的公司，我们会有大大小小的业务需求。当每个产品的业务功能越来越繁重时，也许用户的需求其实很简单，就想 One Click。但是，其实这一个按钮背后可能有很多的系统交互的操作在进行，这就涉及到业务数据操作的事务，涉及到每个系统的交互逻辑、先后顺序以及数据的一致性。这些都需要在设计的时候，需要考虑到的问题。 浅谈解耦合业务系统的设计有多重要在 今天被问微服务，这几点，让面试官刮目相看 一文中，我们讲过微服务设计时的方方面面。其中核心的六个字可能就是：“高内聚，低耦合”。高内聚，我们在那篇文章中已经讲的很清楚了。那么低耦合，这就涉及到我们业务系统的设计了。所谓低耦合，就是要每个业务模块之间的关系降低，减少冗余、重复、交叉的复杂度，模块功能划分也尽可能单一。这样，才能达到低耦合的目的。 在电商行业，主要的功能就是购物，至于其他的，都是为购物作铺垫、营销手段：直播、促销、发优惠券等。从用户的角度来说，其实网上 shopping 的逻辑很简单：选中想要买的，支付 money 就 OK 了。但对于网站，或者说运营服务平台来说，其逻辑远没有那么简单。下面是一个简单的购物流程图： 在这里，我们看到，就这个简单的购物流程，对于用户来说，可能操作很简单：打开网站，登录后选择商品和选中收货地址支付，坐等收货。对于平台，其实它也不简单，包括了很多系统：用户系统、商品系统、仓库系统、订单系统、支付系统、物流系统等等。 不能仅仅因为客户的需求，只是下了一个订单，买了一件商品，那系统就设计一个就认为能解决所有事情，这种认识，可能一开始就是错的。这样的业务设计后，不但导致业务系统的逻辑很笨重，也会导致代码的 code review 非常之复杂。我曾经就亲自目睹过：好几个事情都是一个代码块来处理，甚至都写到几千行，甚至上万行。这样的思路，虽然可以实现暂时的需求。但是从长远角度，这是一个很要命的事情：这样的设计不仅仅说 code review 很吃力，兼容新功能也是很麻烦的，让后来者无法下手。而且长期下去，会导致表的死锁，甚至进入系统瘫痪状态。 如何解耦合业务的复杂性，其实根本原因是没有把其给拆解化。如果把整个的大业务拆解成若干个小的需求，那对于实现，就显得即一目了然，又能完美兼容其他任何问题。咱们还是拿购物说事，为什么每个购物 app 的系统设计都是这样的套路：选中商品后必须先加入购物车，选好地址信息，然后再统一去提交订单，最后才去支付 money 呢？难道系统直接简单点，选中后就支付不就解决了吗？那么网站何必搞得这么的麻烦，浪费时间、金钱，是为了折腾人？统统都不是。其实这也是网站开发最初想的事情，并不是说一件事情一口气能解决，就鲁莽的直接一口气解决。也许到时候，时间久了，人的精力没那么旺盛，变得虚弱的时候，那一口气就无法完成了。网站也是，一个需求也许可以简单的设计，就能完成。但是如果仅仅想着，现在简单的就完成，那是对以后的不负责任。以后可能会出现一些难以想象的事情，并且难以解决。 上面扯远了，回归到解耦合，解耦合其实有很多办法。比如 Java 中就有很多解决低耦合的方法：监听、观察模式、异步回调、定时任务、消息中间件等等。 1.1 监听在Java 里，有很多设计模式：工厂模式、单例模式、建造者模式、代理模式、解释器模式、监听模式、观察者模式等等。其中，监听模式是低耦合解决的方案之一。 所谓监听模式：事件源经过事件的封装传给监听器，当事件源触发事件后，监听器接收到事件对象可以回调事件的方法。这其中涉及到三个信息：事件源、事件、监听器。 For example : 模拟某个服务启动后，发送通知信息。 事件源： 123456789101112131415161718192021package com.damon.event;import java.util.ArrayList;import java.util.List;public class Context &#123; private static List&lt;Listener&gt; list&#x3D;new ArrayList&lt;Listener&gt;(); public static void addListener(Listener listener)&#123; list.add(listener); &#125; public static void removeListener(Listener listener)&#123; list.remove(listener); &#125; public static void sendMsg(Event event)&#123; for(Listener listener:list)&#123; listener.onChange(event); &#125; &#125;&#125; 事件： 123456789101112131415161718192021222324package com.damon.event;public class Event &#123; public static final int INSTALLED &#x3D; 1; public static final int STARTED &#x3D; 2; public static final int RESOLVED &#x3D; 3; public static final int STOPPED &#x3D; 4; private int type; private Object source; public Event(int type, Object source) &#123; this.type &#x3D; type; this.source &#x3D; source; &#125; public int getType() &#123; return type; &#125; public Object getSource() &#123; return source; &#125;&#125; 监听器： 12345678910111213141516171819202122232425package com.damon.event;public class MyListener implements Listener &#123; @Override public void onChange(Event event) &#123; switch(event.getType())&#123; case Event.STARTED : System.out.println(&quot;started...&quot;); break; case Event.RESOLVED : System.out.println(&quot;resolved...&quot;); break; case Event.STOPPED : System.out.println(&quot;stopped...&quot;); break; default: throw new IllegalArgumentException(); &#125; &#125;&#125; 测试： 1234567891011121314package com.damon.event;public class EventTest &#123; public static void main(String[] args) &#123; Listener listener &#x3D; new MyListener(); &#x2F;&#x2F;加入监听者 Context.addListener(listener); &#x2F;&#x2F;启动完毕事件触发 Context.sendMsg(new Event(Event.STARTED, new MyBundle())); &#125;&#125; 在服务启动的操作中，我们不需要等待或者去处理，而是继续其他的逻辑，等到服务启动后，事件监听器监听后会进行相应的操作。这样，就不会在服务启动的过程中，需要等待其启动，因为其启动的时间是无法估量的。所以就很好的解决其耦合性的问题。避免用户在等待过程中，浪费了大量不应该由用户承担的时间成本。毕竟，对于用户来说，时间就是金钱。 1.2 观察者模式观察者模式，听着跟上面讲的监听模式有点像。但是，还是有区别的。所谓观察者模式：观察者相当于事件监听者，被观察者相当于事件源和事件，执行逻辑时通知观察者即可触发其 update，同时可传被观察者和其参数。看着是不是像简化了事件监听机制的实现。其又可以叫发布-订阅模式，只有两个角色。 For example : 微信群里发布了一条公告：下午三点开会，有些在群里的人接收到了消息去开会，但是有些人未在群里，未收到公告，被领导主动喊去开会。 观察者： 1234567891011121314public abstract class Observer &#123; protected String name; protected Subject subject; public Observer(String name, Subject subject) &#123; this.name &#x3D; name; this.subject &#x3D; subject; &#125; public abstract void update();&#125; 通知者： 1234567891011121314public interface Subject &#123; &#x2F;&#x2F;增加 public void attach(Observer observer); &#x2F;&#x2F;删除 public void detach(Observer observer); &#x2F;&#x2F;通知 public void notifyObservers(); &#x2F;&#x2F;状态 public void setAction(String action); public String getAction();&#125; 具体人：群管理员 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class WechatManager implements Subject &#123; &#x2F;&#x2F;同事好友列表 private List&lt;Observer&gt; observers &#x3D; new LinkedList&lt;&gt;(); private String action; &#x2F;&#x2F;添加 @Override public void attach(Observer observer) &#123; observers.add(observer); &#125; &#x2F;&#x2F;删除 @Override public void detach(Observer observer) &#123; observers.remove(observer); &#125; &#x2F;&#x2F;通知 @Override public void notifyObservers() &#123; for(Observer observer : observers) &#123; observer.update(); &#125; &#125; &#x2F;&#x2F;状态 @Override public String getAction() &#123; return action; &#125; @Override public void setAction(String action) &#123; this.action &#x3D; action; &#125;&#125; 具体观察者：群内人员与群外人员 1234567891011121314public class InWechatRoomObserver extends Observer &#123; public InWechatRoomObserver(String name, Subject subject) &#123; super(name, subject); &#125; @Override public void update() &#123; System.out.println(subject.getAction() + &quot;\\n&quot; + name + &quot;收到公告，去开会了&quot;); &#125;&#125; Test： 12345678910111213141516171819202122public class Test &#123; public static void main(String[] args) &#123; &#x2F;&#x2F;群管理员为通知者 WechatManager ma &#x3D; new WechatManager(); InWechatRoomObserver in &#x3D; new InWechatRoomObserver(&quot;tom&quot;, ma); OutWechatRoomObserver out &#x3D; new OutWechatRoomObserver(&quot;damon&quot;, ma); &#x2F;&#x2F;群管理员通知 ma.attach(out); ma.attach(in); &#x2F;&#x2F;damon没在群内，未被通知到，所以被领导发现 ma.detach(out); &#x2F;&#x2F;老板回来了 ma.setAction(&quot;下午三点，大家在大会议室开会&quot;); &#x2F;&#x2F;发通知 ma.notifyObservers(); &#125;&#125; 可以看到：当一个对象的改变需要同时改变其它对象，并且它不知道具体有多少对象有待改变的时候，可考虑使用观察者模式。 即使用观察者模式的动机在于：在保证相关业务数据的一致性，我们不希望为了维持一致性而使各个逻辑紧密耦合，这样会给维护、扩展和重用都带来不便，而观察者模式所做的工作就是在解除耦合。 1.3 异步异步，对于一个系统来说，异步操作可以很好的解耦合，因为每一步操作不需要等待结果即可继续往下进行，不论中间操作是否成功。在 Java 中，常见的异步注解：@Async，解决相应如果需要很多操作，或者操作时耗时很长，而异步进行处理来解决相关问题。有时需要注解 @EnableAsync 配合，然后弄一个异步线程池，来进行线程异步调度管理。 异步线程池初始化 bean ： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package com.damon.task;import java.util.concurrent.Executor;import java.util.concurrent.ThreadPoolExecutor;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.scheduling.annotation.EnableAsync;import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;&#x2F;** * 异步任务执行bean * * @author Damon * *&#x2F;@EnableAsync@Configurationpublic class TaskPoolConfig &#123; @Bean(&quot;taskExecutor&quot;) public Executor taskExecutor() &#123; ThreadPoolTaskExecutor executor &#x3D; new ThreadPoolTaskExecutor(); executor.setCorePoolSize(10); executor.setMaxPoolSize(20); executor.setQueueCapacity(200); executor.setKeepAliveSeconds(60); executor.setThreadNamePrefix(&quot;taskExecutor-&quot;); executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); return executor; &#125;&#125;异步调度方法类：package com.damon.task;import org.apache.commons.lang.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.http.*;import org.springframework.scheduling.annotation.Async;import org.springframework.stereotype.Component;import java.math.BigDecimal;import java.text.DecimalFormat;import java.util.ArrayList;import java.util.List;import java.util.Random;&#x2F;** * * 远程业务调用封装类 * * @author Damon * @date 2019年3月19日 下午3:29:45 * *&#x2F;@Componentpublic class TaskService &#123; private Logger logger &#x3D; LoggerFactory.getLogger(getClass()); public static Random random &#x3D; new Random(); &#x2F;** * @description 异步任务计算耗时 * @param start 开始时间 * @param userId 用户id * @throws Exception *&#x2F; @Async(&quot;taskExecutor&quot;) public void doTaskOne(long start, String userId) throws Exception &#123; logger.info(&quot; 开始做任务一 to &#123;&#125;&quot;, start); Thread.sleep(random.nextInt(10000)); long end &#x3D; System.currentTimeMillis(); logger.info(&quot;完成任务一，耗时：&quot; + (end - start) + &quot;毫秒&quot;); &#125;&#125; 异步可以常见于很多业务，比如异步发送短讯告诉用户，支付成功，异步发送日志到 ELK 系统等。 1.4 定时任务对于定时任务，就是指制定系统的某个时刻或每隔一段时间去触发一些逻辑执行，这样来保证业务数据的一致性，消息的一致性，或者数据的实时性。 我们常在 Java 里用 @EnableScheduling 来引入定时器，然后定义一个异步定时调度 bean： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.damon.task;import java.util.concurrent.Executor;import java.util.concurrent.ThreadPoolExecutor;import org.springframework.aop.interceptor.AsyncUncaughtExceptionHandler;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.scheduling.annotation.AsyncConfigurer;import org.springframework.scheduling.annotation.EnableAsync;import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;&#x2F;** * * 异步任务执行bean * @author Damon * @date 2019年7月17日 上午10:35:56 * *&#x2F;@EnableAsync@Configurationpublic class TaskPoolConfig implements AsyncConfigurer &#123; @Bean(&quot;asyncTask&quot;) public Executor taskExecutor() &#123; ThreadPoolTaskExecutor executor &#x3D; new ThreadPoolTaskExecutor(); &#x2F;&#x2F;线程池维护线程的最少数量 executor.setCorePoolSize(10); &#x2F;&#x2F;线程池维护线程的最大数量 executor.setMaxPoolSize(20); &#x2F;&#x2F; 缓存队列 executor.setQueueCapacity(200); &#x2F;&#x2F;允许的空闲时间 executor.setKeepAliveSeconds(60); executor.setThreadNamePrefix(&quot;asyncTask-&quot;); &#x2F;&#x2F;对拒绝task的处理策略 executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); return executor; &#125; @Override public Executor getAsyncExecutor() &#123; return null; &#125; @Override public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() &#123; return null; &#125;&#125; 同时，定义一个执行类： 12345678910111213141516171819202122232425262728293031&#x2F;** * * 执行调度 * @author Damon * @date 2019年3月19日 下午3:29:45 * *&#x2F;@Componentpublic class TaskSchedule &#123; private Logger logger &#x3D; LoggerFactory.getLogger(TaskSchedule.class); @Autowired private RestTemplate restTemplate; @Autowired private Environment env; &#x2F;&#x2F;@Scheduled(cron &#x3D; &quot;0 0&#x2F;1 * * * ?&quot;)&#x2F;&#x2F;每分钟 @Scheduled(cron &#x3D; &quot;0 * * * * ?&quot;)&#x2F;&#x2F;每分钟 public void dynamicResourceListener() &#123; logger.info(&quot;resourceLimitHandle timer start&quot;); String namespace &#x3D; env.getProperty(&quot;INFERENCE_JOB_NAMESPACE&quot;); resourceListenerCallBack(namespace); &#125; private void resourceListenerCallBack(String namespace) &#123; &#125;&#125; 其中，cron 从左到右（用空格隔开）： 1秒 分 小时 月份中的日期 月份 星期中的日期 年份 上面的逻辑是每分钟去执行某个逻辑，这样的业务我们也可能存在，For example：股票系统中，建模等数据一般都是用 Oracle 来存储的，有时候业务可能是用 Mysql，这时，需要一个定时任务来跑数据，常见的叫 ETL，所以 ETL 的由来，就是这样来的。这样的操作肯定不能在发生业务操作时来进行，否则会因为业务数据的海量读取，导致 IO 的性能，甚至内存、CPU 都会飙升。再如统计某个业务场景的数据，都可以通过这种解耦合的方式来处理。 1.5 消息中间件消息中间件的话，这个也是很多的，比如：redis、rocketmq、rabbitmq、zk等等。这些中间件技术都可以再一个复杂的业务流程起到至关重要得作用。 当我们需要做一个秒杀的功能时，可以用 redis 来作分布式锁，这个能起到缓冲系统压力的作用，同时可以做到秒杀锁。 当我们需要在处理一些业务逻辑时，需要告知其他方，这时候可以用 MQ 来作消息处理，防止处理流程的断续。 当我们需要发送一些消息给外部时，但又不希望耽误当前的业务处理，这时候，可以用 MQ 或 redis 来处理消息。 当我们。。。任何时候，都可以用消息中间件来作降低系统间的耦合。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"浅谈 Java 集合 | 底层源码解析","date":"2020-07-23T00:41:01.000Z","path":"2020/07/23/core-java02/","text":"在 Java 中，我们经常会使用到一些处理缓存数据的集合类，这些集合类都有自己的特点，今天主要分享下 Java 集合中几种经常用的 Map、List、Set。 目录 1、Map 一、背景 二、Map家族 三、HashMap、Hashtable等 四、HashMap 底层数据结构 2、List 一、List 包括的子类 二、ArrayList 三、ArrayList 源码分析 四、LinkedList 五、LinkedList 源码分析 3、Set 一、Set的实质 二、HashSet 三、TreeSet 集合 1：Map 背景 如果一个海量的数据中，需要查询某个指定的信息，这时候，可能会犹如大海捞针，这时候，可以使用 Map 来进行一个获取。因为 Map 是键值对集合。Map这种键值（key-value）映射表的数据结构，作用就是通过key能够高效、快速查找value。 举一个例子： import java.util.HashMap;import java.util.Map;import java.lang.Object; public class Test { public static void main(String[] args) { Object o = new Object(); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(“aaa”, o); //将”aaa”和 Object实例映射并关联 Student target = map.get(“aaa”); //通过key查找并返回映射的Obj实例 System.out.println(target == o); //true，同一个实例 Student another = map.get(“bbb”); //通过另一个key查找 System.out.println(another); //未找到则返回null }}Map&lt;K, V&gt;是一种键-值映射表，当我们调用put(K key, V value)方法时，就把key和value做了映射并放入Map。当我们调用V get(K key)时，就可以通过key获取到对应的value。如果key不存在，则返回null。和List类似，Map也是一个接口，最常用的实现类是HashMap。 在 Map&lt;K, V&gt; 中，如果遍历的时候，其 key 是无序的，如何理解： import java.util.HashMap;import java.util.Map;public class Test { public static void main(String[] args) { Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put(“dog”, “a”); map.put(“pig”, “b”); map.put(“cat”, “c”); for (Map.Entry&lt;String, Integer&gt; entry : map.entrySet()) { String key = entry.getKey(); Integer value = entry.getValue(); System.out.println(key + “ = “ + value); } }} //printcat = cdog = apig = b从上面的打印结果来看，其是无序的，有序的答案可以在下面找到。接下来我们分析下 Map ，首先我们先看看 Map 家族： 它的子孙下面有我们常用的 HashMap、LinkedHashMap，也有 TreeMap，另外还有继承 Dictionary、实现 Map 接口的 Hashtable。 下面针对各个实现类的特点来说明： （1）HashMap：它根据键的 hashCode 值存储数据，大多数情况下可以直接定位到它的值，因而具有高效的访问速度，但遍历顺序却是不确定的。HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap 非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections 的静态方法 synchronizedMap 方法使 HashMap 具有线程安全的能力，或者使用 ConcurrentHashMap(分段加锁)。 （2）LinkedHashMap：LinkedHashMap 是 HashMap 的一个子类，替 HashMap 完成了输入顺序的记录功能，所以要想实现像输出同输入顺序一致，应该使用 LinkedHashMap。 （3）TreeMap：TreeMap 实现 SortedMap 接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用 TreeMap 时，key 必须实现Comparable 接口或者在构造 TreeMap 传入自定义的 Comparator，否则会在运行时抛出 ClassCastException 类型的异常。 （4）Hashtable：Hashtable继承 Dictionary 类，实现 Map 接口，很多映射的常用功能与 HashMap 类似，Hashtable 采用”拉链法”实现哈希表，不同的是它来自 Dictionary 类，并且是线程安全的，任一时间只有一个线程能写 Hashtable，但并发性不如 ConcurrentHashMap，因为ConcurrentHashMap 引入了分段锁。Hashtable 使用 synchronized 来保证线程安全，在线程竞争激烈的情况下 HashTable 的效率非常低下。不建议在新代码中使用，不需要线程安全的场合可以用 HashMap 替换，需要线程安全的场合可以用 ConcurrentHashMap 替换。Hashtable 并不是像 ConcurrentHashMap 对数组的每个位置加锁，而是对操作加锁，性能较差。 上面讲到了 HashMap、Hashtable、ConcurrentHashMap，接下来先看看 HashMap 的源码实现： public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable { private static final long serialVersionUID = 362498820763181265L; /** * 默认大小 16 */ static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; /** * 最大容量是必须是2的幂30 */ static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; /** * 负载因子默认为0.75，hashmap每次扩容为原hashmap的2倍 */ static final float DEFAULT_LOAD_FACTOR = 0.75f; /** * 链表的最大长度为8，当超过8时会将链表装换为红黑树进行存储 */ static final int TREEIFY_THRESHOLD = 8; /** * The table, initialized on first use, and resized as * necessary. When allocated, length is always a power of two. * (We also tolerate length zero in some operations to allow * bootstrapping mechanics that are currently not needed.) */ transient Node&lt;K,V&gt;[] table; static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + &quot;=&quot; + value; } public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; } return false; } }从上面看到，HashMap 主要是数组 + 链表结构组成。HashMap 扩容是成倍的扩容。为什么是成倍，而不是1.5或其他的倍数呢？既然 HashMap 在进行 put 的时候针对 key 做了一些列的 hash 以及与运算就是为了减少碰撞的一个概率，如果扩容后的大小不是2的n次幂的话，之前做的不是白费了吗？ else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1;扩容后会重新把原来的所有的数据 key 的 hash 重新计算放入扩容后的数组里面去。为什么要这样做？因为不同的数组大小通过 key 的 hash 出来的下标是不一样的。还有，数组长度保持2的次幂，length-1的低位都为1，会使得获得的数组索引 index 更加均匀。 为何说 Hashmap 是非线程安全的呢？原因：当多线程并发时，检测到总数量超过门限值的时候就会同时调用 resize 操作，各自生成新的数组并rehash 后赋给底层数组，结果最终只有最后一个线程生成的新数组被赋给table 变量，其他线程均会丢失。而且当某些线程已经完成赋值而其他线程刚开始的时候，就会用已经被赋值的 table 作为原始数组，这样也是有问题滴。 疑问： HashMap 中某个 entry 链过长，查询时间达到最大限度，如何处理呢？这个在 Jdk1.8，当链表过长，把链表转成红黑树(TreeNode)实现了更高的时间复杂度的查找。 HashMap中哈希算法实现？我们使用put(key,value)方法往HashMap中添加元素时，先计算得到key的 Hash 值，然后通过Key高16位与低16位相异或（高16位不变），然后与数组大小-1相与，得到了该元素在数组中的位置，流程： 延伸：如果一个对象中，重写了equals()而不重写hashcode()会发生什么样的问题？尽管我们在进行 get 和 put 操作的时候，使用的key从逻辑上讲是等值的（通过equals比较是相等的），但由于没有重写hashCode()，所以put操作时，key(hashcode1)–&gt;hash–&gt;indexFor–&gt;index，而通过key取出value的时候 key(hashcode2)–&gt;hash–&gt;indexFor–&gt;index，由于hashcode1不等于hashcode2，导致没有定位到一个数组位置而返回逻辑上错误的值null。所以，在重写equals()的时候，必须注意重写hashCode()，同时还要保证通过equals()判断相等的两个对象，调用hashCode方法要返回同样的整数值。而如果equals判断不相等的两个对象，其hashCode也可以相同的（只不过会发生哈希冲突，应尽量避免）。（1. hash相同，但key不一定相同：key1、key2产生的hash很有可能是相同的，如果key真的相同，就不会存在散列链表了，散列链表是很多不同的键算出的hash值和index相同的 2. key相同，经过两次hash，其hash值一定相同） ConcurrentHashMap 采用了分段锁技术来将数据分成一段段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。 02 集合 2：List 集合 List 是接口 Collection 的子接口，也是大家经常用到的数据缓存。List 进行了元素排序，且允许存放相同的元素，即有序，可重复。我们先看看有哪些子类： 可以看到，其中包括比较多的子类，我们常用的是 ArrayList、LinkedList: ArrayList： 优点：操作读取操作效率高，基于数组实现的，可以为null值，可以允许重复元素，有序，异步。 缺点：由于它是由动态数组实现的，不适合频繁的对元素的插入和删除操作，因为每次插入和删除都需要移动数组中的元素。 LinkedList： 优点：LinkedList由双链表实现，增删由于不需要移动底层数组数据，其底层是链表实现的，只需要修改链表节点指针，对元素的插入和删除效率较高。 缺点：遍历效率较低。HashMap和双链表也有关系。 ArrayList 底层是一个变长的数组，基本上等同于Vector，但是ArrayList对writeObjec() 和 readObject()方法实现了同步。 transient Object[] elementData; /** Constructs an empty list with an initial capacity of ten. /public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;}If multiple threads access an ArrayList instanceconcurrently, and at least one of the threads modifiesthe list structurally, it must be synchronized externally.(A structural modification is any operation that adds or deletes one or more elements, or explicitly resizes the backing array; merely setting the value of an element is not a structural modification.)This is typically accomplished by synchronizingon some object that naturally encapsulates the list.从注释，我们知道 ArrayList 是线程不安全的，多线程环境下要通过外部的同步策略后使用，比如List list = Collections.synchronizedList(new ArrayList(…))。 源码实现： private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException{ // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. for (int i=0; i&lt;size; i++) { s.writeObject(elementData[i]); } if (modCount != expectedModCount) { throw new ConcurrentModificationException(); }} /** Reconstitute the ArrayList instance from a stream (that is, deserialize it). /private void readObject(java.io.ObjectInputStream s)throws java.io.IOException, ClassNotFoundException {elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuffs.defaultReadObject(); // Read in capacitys.readInt(); // ignored if (size &gt; 0) { // be like clone(), allocate array based upon size not capacity int capacity = calculateCapacity(elementData, size); SharedSecrets.getJavaOISAccess().checkArray(s, Object[].class, capacity); ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) { a[i] = s.readObject(); }}}当调用add函数时，会调用ensureCapacityInternal函数进行扩容，每次扩容为原来大小的1.5倍，但是当第一次添加元素或者列表中元素个数小于10的话，列表容量默认为10。 /** Default initial capacity. /private static final int DEFAULT_CAPACITY = 10; /** Shared empty array instance used for empty instances. /private static final Object[] EMPTY_ELEMENTDATA = {}; /** Shared empty array instance used for default sized empty instances. /private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; /** The array buffer into which the elements of the ArrayList are stored. /transient Object[] elementData; // non-private to simplify nested class access /** The size of the ArrayList (the number of elements it contains). /private int size;扩容原理：根据当前数组的大小，判断是否小于默认值10，如果大于，则需要扩容至当前数组大小的1.5倍，重新将新扩容的数组数据copy只当前elementData，最后将传入的元素赋值给size++位置。 private void ensureCapacityInternal(int minCapacity) { ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));} private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);} /** The maximum size of array to allocate. Some VMs reserve some header words in an array. Attempts to allocate larger arrays may result in OutOfMemoryError: Requested array size exceeds VM limit /private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** Increases the capacity to ensure that it can hold at least the number of elements specified by the minimum capacity argument. @param minCapacity the desired minimum capacity /private void grow(int minCapacity) {// overflow-conscious codeint oldCapacity = elementData.length;int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1);if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity;if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity);// minCapacity is usually close to size, so this is a win:elementData = Arrays.copyOf(elementData, newCapacity);} private static int hugeCapacity(int minCapacity) { if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;}接下来我们分析为什么 ArrayList 增删很慢，查询很快呢？ public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;}根据源码可知，当调用add函数时，首先要调用ensureCapacityInternal(size + 1)，该函数是进行自动扩容的，效率低的原因也就是在这个扩容上了，每次新增都要对现有的数组进行一次1.5倍的扩大，数组间值的copy等，最后等扩容完毕，有空间位置了，将数组size+1的位置放入元素e，实现新增。 删除时源码： /** Removes the element at the specified position in this list. Shifts any subsequent elements to the left (subtracts one from their indices). @param index the index of the element to be removed @return the element that was removed from the list @throws IndexOutOfBoundsException {@inheritDoc} /public E remove(int index) {rangeCheck(index); modCount++;E oldValue = elementData(index); int numMoved = size - index - 1;if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved);elementData[–size] = null; return oldValue;}在删除index位置的元素时，要先调用 rangeCheck(index) 进行 index 的check，index 要超过当前个数，则判定越界，抛出异常，throw new IndexOutOfBoundsException(outOfBoundsMsg(index))，其他函数也有用到如：get(int index)，set(int index, E element) 等后面删除重点在于计算删除的index是末尾还是中间位置，末尾直接–，然后置空完事，如果是中间位置，那就要进行一个数组间的copy，重新组合数组数据了，这一就比较耗性能了。 而查询： /** Returns the element at the specified position in this list. @param index index of the element to return @return the element at the specified position in this list @throws IndexOutOfBoundsException {@inheritDoc} /public E get(int index) {rangeCheck(index); return elementData(index);}获取指定index的元素，首先调用rangeCheck(index)进行index的check，通过后直接获取数组的下标index获取数据，没有任何多余操作，高效。 LinkedList 继承AbstractSequentialList和实现List接口，新增接口如下： addFirst(E e):将指定元素添加到刘表开头 addLast(E e):将指定元素添加到列表末尾 descendingIterator():以逆向顺序返回列表的迭代器 element():获取但不移除列表的第一个元素 getFirst():返回列表的第一个元素 getLast():返回列表的最后一个元素 offerFirst(E e):在列表开头插入指定元素 offerLast(E e):在列表尾部插入指定元素 peekFirst():获取但不移除列表的第一个元素 peekLast():获取但不移除列表的最后一个元素 pollFirst():获取并移除列表的最后一个元素 pollLast():获取并移除列表的最后一个元素 pop():从列表所表示的堆栈弹出一个元素 push(E e);将元素推入列表表示的堆栈 removeFirst():移除并返回列表的第一个元素 removeLast():移除并返回列表的最后一个元素 removeFirstOccurrence(E e):从列表中移除第一次出现的指定元素 removeLastOccurrence(E e):从列表中移除最后一次出现的指定元素LinkedList 的实现原理：LinkedList 的实现是一个双向链表。在 Jdk 1.6中是一个带空头的循环双向链表，而在 Jdk1.7+ 中则变为不带空头的双向链表，这从源码中可以看出： //jdk 1.6private transient Entry header = new Entry(null, null, null);private transient int size = 0; //jdk 1.7transient int size = 0; transient Node first; transient Node last;从源码注释看，LinkedList不是线程安全的，多线程环境下要通过外部的同步策略后使用，比如List list = Collections.synchronizedList(new LinkedList(…))： If multiple threads access a linked list concurrently, and at least one of the threads modifies the list structurally, it must be synchronized externally. (A structural modification is any operation that adds or deletes one or more elements; merely setting the value of an element is not a structural modification.) This is typically accomplished by synchronizing on some object that naturally encapsulates the list.为什么说 LinkedList 增删很快呢？ /** Appends the specified element to the end of this list. This method is equivalent to {@link #addLast}. @param e element to be appended to this list @return {@code true} (as specified by {@link Collection#add}) /public boolean add(E e) {linkLast(e);return true;}/** Links e as last element. /void linkLast(E e) {final Node l = last;final Node newNode = new Node&lt;&gt;(l, e, null);last = newNode;if (l == null) first = newNode;else l.next = newNode;size++;modCount++;}从注释看，add函数实则是将元素append至list的末尾，具体过程是：新建一个Node节点，其中将后面的那个节点last作为新节点的前置节点，后节点为null；将这个新Node节点作为整个list的后节点，如果之前的后节点l为null，将新建的Node作为list的前节点，否则，list的后节点指针指向新建Node，最后size+1，当前llist操作数modCount+1。 在add一个新元素时，LinkedList 所关心的重要数据，一共两个变量，一个first，一个last，这大大提升了插入时的效率，且默认是追加至末尾，保证了顺序。 再看删除一个元素： /** Removes the element at the specified position in this list. Shifts any subsequent elements to the left (subtracts one from their indices). Returns the element that was removed from the list. @param index the index of the element to be removed @return the element previously at the specified position @throws IndexOutOfBoundsException {@inheritDoc} /public E remove(int index) {checkElementIndex(index);return unlink(node(index));} /** Unlinks non-null node x. /E unlink(Node x) {// assert x != null;final E element = x.item;final Node next = x.next;final Node prev = x.prev; if (prev == null) { first = next;} else { prev.next = next; x.prev = null;} if (next == null) { last = prev;} else { next.prev = prev; x.next = null;} x.item = null;size–;modCount++;return element;}删除指定index的元素，删除之前要调用checkElementIndex(index)去check一下index是否存在元素，如果不存在抛出throw new IndexOutOfBoundsException(outOfBoundsMsg(index));越界错误，同样这个check方法也是很多方法用到的，如：get(int index)，set(int index, E element)等。 注释讲，删除的是非空的节点，这里的node节点也是通过node(index)获取的，分别根据当前Node得到链表上的关节要素：element、next、prev，分别对 prev 和 next 进行判断，以便对当前 list 的前后节点进行重新赋值，frist和last，最后将节点的element置为null，个数-1，操作数+1。根据以上分析，remove节点关键的变量，是Node实例本身的局部变量 next、prev、item 重新构建内部变量指针指向，以及list的局部变量first和last保证节点相连。这些变量的操作使得其删除动作也很高效。 而对于查询： /** Returns the element at the specified position in this list. @param index index of the element to return @return the element at the specified position in this list @throws IndexOutOfBoundsException {@inheritDoc} /public E get(int index) {checkElementIndex(index);return node(index).item;}获取指定index位置的node，获取之前还是调用checkElementIndex(index)进行检查元素，之后通过node(index)获取元素，上文有提到，node的获取是遍历得到的元素，所以相对性能效率会低一些。 03 集合 3：Set Set 集合在我们日常中，用到的也比较多。用于存储不重复的元素集合，它主要提供下面几种方法： 将元素添加进Set：add(E e) 将元素从Set删除：remove(Object e) 判断是否包含元素：contains(Object e) 这几种方法返回结果都是 boolean值，即返回是否正确或成功。Set 相当于只存储key、不存储value的Map。我们经常用 Set 用于去除重复元素，因为 重复add同一个 key 时，会返回 false。 public HashSet() { map = new HashMap&lt;&gt;();} public TreeSet() { this(new TreeMap&lt;E,Object&gt;());}Set 子孙中主要有：HashSet、SortedSet。HashSet是无序的，因为它实现了Set接口，并没有实现SortedSet接口，而TreeSet 实现了SortedSet接口，从而保证元素是有序的。 HashSet 添加后输出也是无序的： public class Test { public static void main(String[] args) { Set set = new HashSet&lt;&gt;(); set.add(“2”); set.add(“6”); set.add(“44”); set.add(“5”); for (String s : set) { System.out.println(s); } }} //print44256看到输出的顺序既不是添加的顺序，也不是String排序的顺序，在不同版本的JDK中，这个顺序也可能是不同的。 换成TreeSet： public static void main(String[] args) { Set set = new TreeSet&lt;&gt;(); set.add(“2”); set.add(“6”); set.add(“44”); set.add(“5”); for (String s : set) { System.out.println(s); } }//print24456 在遍历TreeSet时，输出就是有序的，不是添加时的顺序，而是元素的排序顺序。 注意：添加的元素必须实现Comparable接口，如果没有实现Comparable接口，那么创建TreeSet时必须传入一个Comparator对象。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"大佬整理的mysql规范，分享给大家","date":"2020-07-23T00:21:08.000Z","path":"2020/07/23/mysql-norm/","text":"最近涉及数据库相关操作较多，公司现有规范也不是太全面，就根据网上各路大神的相关规范，整理了一些自用的规范用法，万望指正。数据库环境dev: 开发环境开发可读写，可修改表结构。开发人员可以修改表结构，可以随意修改其中的数据但是需要保证不影响其他开发同事。 test: 测试环境开发可读写，开发人员可以通过工具修改表结构。 online: 线上环境开发人员不允许直接在线上环境进行数据库操作，如果需要操作必须找DBA进行操作并进行相应记录，禁止进行压力测试。重点的问题，各个环境的mysql服务器对应的用户权限，一定要做到权限划分明确，有辨识度，能具体区分业务场景等。 命名规范基本命名规则使用有意义的英文词汇，词汇中间以下划线分隔。（不要用拼音）只能使用英文字母，数字，下划线，并以英文字母开头。库、表、字段全部采用小写，不要使用驼峰式命名。避免用ORACLE、MySQL的保留字，如desc，关键字如index。命名禁止超过32个字符，须见名之意，建议使用名词不是动词数据库，数据表一律使用前缀临时库、表名必须以tmp为前缀，并以日期为后缀备份库、表必须以bak为前缀，并以日期为后缀为什么库、表、字段全部采用小写？在 MySQL 中，数据库和表对就于那些目录下的目录和文件。因而，操作系统的敏感性决定数据库和表命名的大小写敏感。Windows下是不区分大小写的。Linux下大小写规则数据库名与表名是严格区分大小写的；表的别名是严格区分大小写的；列名与列的别名在所有的情况下均是忽略大小写的；变量名也是严格区分大小写的；如果已经设置了驼峰式的命名如何解决？需要在MySQL的配置文件my.ini中增加 lower_case_table_names = 1即可。 表命名同一个模块的表尽可能使用相同的前缀，表名称尽可能表达含义。所有日志表均以 log_ 开头 字段命名表达其实际含义的英文单词或简写。布尔意义的字段以is_作为前缀，后接动词过去分词。各表之间相同意义的字段应同名。各表之间相同意义的字段，以去掉模块前缀的表名_字段名命名。外键字段用表名_字段名表示其关联关系。表的主键一般都约定成为id，自增类型，是别的表的外键均使用xxx_id的方式来表明。 索引命名非唯一索引必须按照“idx_字段名称_字段名称[_字段名]”进行命名唯一索引必须按照“uniq_字段名称_字段名称[_字段名]”进行命名 约束命名主键约束：pk_表名称。唯一约束：uk_表名称_字段名。（应用中需要同时有唯一性检查逻辑。） 表设计规范表引擎取决于实际应用场景；日志及报表类表建议用myisam，与交易，审核，金额相关的表建议用innodb引擎。如无说明，建表时一律采用innodb引擎默认使用utf8mb4字符集，数据库排序规则使用utf8mb4_general_ci，（由于数据库定义使用了默认，数据表可以不再定义，但为保险起见，建议都写上为什么字符集不选择utf8，排序规则不使用utf8_general_ci采用utf8编码的MySQL无法保存占位是4个字节的Emoji表情。为了使后端的项目，全面支持客户端输入的Emoji表情，升级编码为utf8mb4是最佳解决方案。对于JDBC连接串设置了characterEncoding为utf8或者做了上述配置仍旧无法正常插入emoji数据的情况，需要在代码中指定连接的字符集为utf8mb4。所有表、字段均应用 comment 列属性来描述此表、字段所代表的真正含义，如枚举值则建议将该字段中使用的内容都定义出来。如无说明，表中的第一个id字段一定是主键且为自动增长，禁止在非事务内作为上下文作为条件进行数据传递。禁止使用varchar类型作为主键语句设计。如无说明，表必须包含create_time和modify_time字段，即表必须包含记录创建时间和修改时间的字段如无说明，表必须包含is_del，用来标示数据是否被删除，原则上数据库数据不允许物理删除。用尽量少的存储空间来存数一个字段的数据能用int的就不用char或者varchar能用tinyint的就不用int使用UNSIGNED存储非负数值。不建议使用ENUM、SET类型，使用TINYINT来代替使用短数据类型，比如取值范围为0-80时，使用TINYINT UNSIGNED存储精确浮点数必须使用DECIMAL替代FLOAT和DOUBLE时间字段，除特殊情况一律采用int来记录unix_timestamp存储年使用YEAR类型。存储日期使用DATE类型。存储时间（精确到秒）建议使用TIMESTAMP类型，因为TIMESTAMP使用4字节，DATETIME使用8个字节。建议使用INT UNSIGNED存储IPV4。尽可能不使用TEXT、BLOB类型禁止在数据库中使用VARBINARY、BLOB存储图片、文件等。建议使用其他方式存储（TFS/SFS），MySQL只保存指针信息。单条记录大小禁止超过8k（列长度(中文)_3(UTF8)+列长度(英文)_1） datetime与timestamp有什么不同？相同点：TIMESTAMP列的显示格式与DATETIME列相同。显示宽度固定在19字符，并且格式为YYYY-MM-DD HH:MM:SS。 不同点：TIMESTAMP4个字节储存，时间范围：1970-01-01 08:00:01 ~ 2038-01-19 11:14:07 值以UTC格式保存，涉及时区转化 ，存储时对当前的时区进行转换，检索时再转换回当前的时区。datetime 8个字节储存，时间范围：1000-01-01 00:00:00 ~ 9999-12-31 23:59:59实际格式储存，与时区无关 如何使用TIMESTAMP的自动赋值属性？将当前时间作为ts的默认值：ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP。当行更新时，更新ts的值：ts TIMESTAMP DEFAULT 0 ON UPDATE CURRENT_TIMESTAMP。可以将1和2结合起来：ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP。如何使用INT UNSIGNED存储ip？使用INT UNSIGNED而不是char(15)来存储ipv4地址，通过MySQL函数inet_ntoa和inet_aton来进行转化。Ipv6地址目前没有转化函数，需要使用DECIMAL或者两个bigINT来存储。如无备注，所有字段都设置NOT NULL，并设置默认值；禁止在数据库中存储明文密码如无备注，所有的布尔值字段，如is_hot、is_deleted，都必须设置一个默认值，并设为0；如无备注，排序字段order_id在程序中默认使用降序排列；整形定义中不添加长度，比如使用INT，而不是INT[4]INT[M]，M值代表什么含义？注意数值类型括号后面的数字只是表示宽度而跟存储范围没有关系。很多人他们认为INT(4)和INT(10)其取值范围分别是 (-9999到9999)和(-9999999999到9999999999)，这种理解是错误的。其实对整型中的 M值与 ZEROFILL 属性结合使用时可以实现列值等宽。不管INT[M]中M值是多少，其取值范围还是 (-2147483648到2147483647 有符号时)，(0到4294967295无符号时)。显示宽度并不限制可以在列内保存的值的范围，也不限制超过列的指定宽度的值的显示。当结合可选扩展属性ZEROFILL使用时默认补充的空格用零代替。例如：对于声明为INT(5) ZEROFILL的列，值4检索为00004。请注意如果在整数列保存超过显示宽度的一个值，当MySQL为复杂联接生成临时表时会遇到问题，因为在这些情况下MySQL相信数据适合原列宽度，如果为一个数值列指定ZEROFILL, MySQL自动为该列添加UNSIGNED属性。使用VARBINARY存储大小写敏感的变长字符串什么时候用CHAR，什么时候用VARCHAR？CHAR和VARCHAR类型类似，但它们保存和检索的方式不同。它们的最大长度和是否尾部空格被保留等方面也不同。CHAR和VARCHAR类型声明的长度表示你想要保存的最大字符数。例如，CHAR(30)可以占用30个字符。CHAR列的长度固定为创建表时声明的长度。长度可以为从0到255的任何值。当保存CHAR值时，在它们的右边填充空格以达到指定的长度。当检索到CHAR值时，尾部的空格被删除掉。在存储或检索过程中不进行大小写转换。VARCHAR列中的值为可变长字符串。长度可以指定为0到65,535之间的值。(VARCHAR的最大有效长度由最大行大小和使用的字符集确定。整体最大长度是65,532字节）。同CHAR对比，VARCHAR值保存时只保存需要的字符数，另加一个字节来记录长度(如果列声明的长度超过255，则使用两个字节)。VARCHAR值保存时不进行填充。当值保存和检索时尾部的空格仍保留，符合标准SQL。char适合存储用户密码的MD5哈希值，它的长度总是一样的。对于经常改变的值，char也好于varchar,因为固定长度的行不容易产生碎片，对于很短的列，char的效率也高于varchar。char(1)字符串对于单字节字符集只会占用一个字节，但是varchar(1)则会占用2个字节，因为1个字节用来存储长度信息。 索引设计规范MySQL的查询速度依赖良好的索引设计，因此索引对于高性能至关重要。合理的索引会加快查询速度（包括UPDATE和DELETE的速度，MySQL会将包含该行的page加载到内存中，然后进行UPDATE或者DELETE操作），不合理的索引会降低速度。MySQL索引查找类似于新华字典的拼音和部首查找，当拼音和部首索引不存在时，只能通过一页一页的翻页来查找。当MySQL查询不能使用索引时，MySQL会进行全表扫描，会消耗大量的IO。索引的用途：去重、加速定位、避免排序、覆盖索引。 什么是覆盖索引InnoDB存储引擎中，secondary index（非主键索引）中没有直接存储行地址，存储主键值。如果用户需要查询secondary index中所不包含的数据列时，需要先通过secondary index查找到主键值，然后再通过主键查询到其他数据列，因此需要查询两次。覆盖索引的概念就是查询可以通过在一个索引中完成，覆盖索引效率会比较高，主键查询是天然的覆盖索引。合理的创建索引以及合理的使用查询语句，当使用到覆盖索引时可以获得性能提升。比如SELECT email,uid FROM user_email WHERE uid=xx，如果uid不是主键，适当时候可以将索引添加为index(uid,email)，以获得性能提升。 索引的基本规范索引数量控制，单张表中索引数量不超过5个，单个索引中的字段数不超过5个。综合评估数据密度和分布 考虑查询和更新比例为什么一张表中不能存在过多的索引？InnoDB的secondary index使用b+tree来存储，因此在UPDATE、DELETE、INSERT的时候需要对b+tree进行调整，过多的索引会减慢更新的速度。对字符串使用前缀索引，前缀索引长度不超过8个字符，建议优先考虑前缀索引，必要时可添加伪列并建立索引。不要索引blob/text等字段,不要索引大型字段,这样做会让索引占用太多的存储空间 什么是前缀索引？前缀索引说白了就是对文本的前几个字符（具体是几个字符在建立索引时指定）建立索引，这样建立起来的索引更小，所以查询更快。前缀索引能有效减小索引文件的大小，提高索引的速度。但是前缀索引也有它的坏处：MySQL 不能在 ORDER BY 或 GROUP BY 中使用前缀索引，也不能把它们用作覆盖索引(Covering Index)。建立前缀索引的语法： 1ALTER TABLE table_name ADD KEY(column_name(prefix_length)); 主键准则表必须有主键不使用更新频繁的列尽量不选择字符串列不使用UUID MD5 HASH默认使用非空的唯一键建议选择自增或发号器重要的SQL必须被索引，核心SQL优先考虑覆盖索索引UPDATE、DELETE语句的WHERE条件列ORDER BY、GROUP BY、DISTINCT的字段多表JOIN的字段区分度最大的字段放在前面选择筛选性更优的字段放在最前面，比如单号、userid等，type，status等筛选性一般不建议放在最前面索引根据左前缀原则，当建立一个联合索引(a,b,c)，则查询条件里面只有包含(a)或(a,b)或(a,b,c)的时候才能走索引,(a,c)作为条件的时候只能使用到a列索引,所以这个时候要确定a的返回列一定不能太多，不然语句设计就不合理,(b,c)则不能走索引合理创建联合索引（避免冗余），(a,b,c) 相当于 (a) 、(a,b) 、(a,b,c) 索引禁忌不在低基数列上建立索引，例如“性别”不在索引列进行数学运算和函数运算不要索引常用的小型表尽量不使用外键外键用来保护参照完整性，可在业务端实现对父表和子表的操作会相互影响，降低可用性INNODB本身对online DDL的限制MYSQL 中索引的限制MYISAM 存储引擎索引长度的总和不能超过 1000 字节BLOB 和 TEXT 类型的列只能创建前缀索引MYSQL 目前不支持函数索引使用不等于 (!= 或者 &lt;&gt;) 的时候, MYSQL 无法使用索引。过滤字段使用函数运算 (如 abs (column)) 后, MYSQL无法使用索引。join语句中join条件字段类型不一致的时候MYSQL无法使用索引使用 LIKE 操作的时候如果条件以通配符开始 (如 ‘%abc…’)时, MYSQL无法使用索引。使用非等值查询的时候, MYSQL 无法使用 Hash 索引。 语句设计规范使用预编译语句只传参数，比传递SQL语句更高效一次解析，多次使用降低SQL注入概率避免隐式转换会导致索引失效充分利用前缀索引必须是最左前缀不可能同时用到两个范围条件不使用%前导的查询，如like “%ab”不使用负向查询，如not in/like无法使用索引，导致全表扫描全表扫描导致buffer pool利用率降低避免使用存储过程、触发器、UDF、events等让数据库做最擅长的事降低业务耦合度，为sacle out、sharding留有余地避开BUG避免使用大表的JOINMySQL最擅长的是单表的主键/二级索引查询JOIN消耗较多内存，产生临时表避免在数据库中进行数学运算MySQL不擅长数学运算和逻辑判断无法使用索引减少与数据库的交互次数 123INSERT … ON DUPLICATE KEY UPDATEREPLACE INTO、INSERT IGNORE 、INSERT INTO VALUES(),(),()UPDATE … WHERE ID IN(10,20,50,…) 合理的使用分页限制分页展示的页数 只能点击上一页、下一页 采用延迟关联 如何正确的使用分页？假如有类似下面分页语句：SELECT * FROM table ORDER BY id LIMIT 10000, 10 由于MySQL里对LIMIT OFFSET的处理方式是取出OFFSET+LIMIT的所有数据，然后去掉OFFSET，返回底部的LIMIT。所以，在OFFSET数值较大时，MySQL的查询性能会非常低。可以使用id &gt; n 的方式进行解决：使用id &gt; n 的方式有局限性，对于id不连续的问题，可以通过翻页的时候同时传入最后一个id方式来解决。 123456http:&#x2F;&#x2F;example.com&#x2F;page.php?last&#x3D;100select * from table where id&lt;100 order by id desc limit 10&#x2F;&#x2F;上一页http:&#x2F;&#x2F;example.com&#x2F;page.php?first&#x3D;110select * from table where id&gt;110 order by id desc limit 10 这种方式比较大的缺点是，如果在浏览中有插入/删除操作，翻页不会更新，而总页数可能仍然是根据新的count(*) 来计算，最终可能会产生某些记录访问不到。为了修补这个问题，可以继续引入当前页码以及在上次翻页以后是否有插入/删除等影响总记录数的操作并进行缓存。 1select * from table where id &gt;&#x3D; (select id from table order by id limit #offset#, 1) 拒绝大SQL，拆分成小SQL充分利用QUERY CACHE充分利用多核CPU使用in代替or，in的值不超过1000个禁止使用order by rand()使用EXPLAIN诊断，避免生成临时表 EXPLAIN语句（在MySQL客户端中执行）可以获得MySQL如何执行SELECT语句的信息。通过对SELECT语句执行EXPLAIN，可以知晓MySQL执行该SELECT语句时是否使用了索引、全表扫描、临时表、排序等信息。尽量避免MySQL进行全表扫描、使用临时表、排序等。详见官方文档。用union all而不是unionunion all与 union有什么区别？union和union all关键字都是将两个结果集合并为一个，但这两者从使用和效率上来说都有所不同。union在进行表链接后会筛选掉重复的记录，所以在表链接后会对所产生的结果集进行排序运算，删除重复的记录再返回结果。如： 12select * from test_union1union select * from test_union2 这个SQL在运行时先取出两个表的结果，再用排序空间进行排序删除重复的记录，最后返回结果集，如果表数据量大的话可能会导致用磁盘进行排序。而union all只是简单的将两个结果合并后就返回。这样，如果返回的两个结果集中有重复的数据，那么返回的结果集就会包含重复的数据了。从效率上说，union all要比union快很多，所以，如果可以确认合并的两个结果集中不包含重复的数据的话，那么就使用union all，如下： 1select * from test_union1 union all select * from test_union2 程序应有捕获SQL异常的处理机制禁止单条SQL语句同时更新多个表不使用select * ，SELECT语句只获取需要的字段消耗CPU和IO、消耗网络带宽无法使用覆盖索引减少表结构变更带来的影响因为大，select/join 可能生成临时表UPDATE、DELETE语句不使用LIMITINSERT语句必须显式的指明字段名称，不使用INSERT INTO table()INSERT语句使用batch提交（INSERT INTO table VALUES(),(),()……），values的个数不超过500统计表中记录数时使用COUNT(*)，而不是COUNT(primary_key)和COUNT(1) 备注：仅针对Myisam数据更新建议使用二级索引先查询出主键，再根据主键进行数据更新禁止使用跨库查询禁止使用子查询，建议将子查询转换成关联查询针对varchar类型字段的程序处理，请验证用户输入，不要超出其预设的长度； 分表规范单表一到两年内数据量超过500w或数据容量超过10G考虑分表，需提前考虑历史数据迁移或应用自行删除历史数据，采用等量均衡分表或根据业务规则分表均可。要分表的数据表必须与DBA商量分表策略用HASH进行散表，表名后缀使用十进制数，下标从0开始按日期时间分表需符合YYYY[MM][dd][HH]格式采用合适的分库分表策略。例如千库十表、十库百表等禁止使用分区表，分区表对分区键有严格要，分区表在表变大后执行DDL、SHARDING、单表恢复等都变得更加困难。拆分大字段和访问频率低的字段，分离冷热数据 行为规范批量导入、导出数据必须提前通知DBA协助观察禁止在线上从库执行后台管理和统计类查询禁止有super权限的应用程序账号存在产品出现非数据库导致的故障时及时通知DBA协助排查推广活动或上线新功能必须提前通知DBA进行流量评估数据库数据丢失，及时联系DBA进行恢复对单表的多次alter操作必须合并为一次操作不在MySQL数据库中存放业务逻辑重大项目的数据库方案选型和设计必须提前通知DBA参与对特别重要的库表，提前与DBA沟通确定维护和备份优先级不在业务高峰期批量更新、查询数据库其他规范提交线上建表改表需求，必须详细注明所有相关SQL语句 其他规范日志类数据不建议存储在MySQL上，优先考虑Hbase或OceanBase，如需要存储请找DBA评估使用压缩表存储。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 特别声明 原文作者：白程序员的自习室 本文原链：https://www.studytime.xin/article/mysql-internal-specifications.html 本文转载如有侵权，请联系站长删除，谢谢 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"极客时间 | 买课奖励 | 返现 | 学习资料 | 学习路径 | 白嫖","date":"2020-07-22T00:03:01.000Z","path":"2020/07/22/popularize-01/","text":"极客时间 | 买课奖励 | 返现 | 学习资料 | 学习路径 | 白嫖 |最近几年，知识付费越来越火爆，花几十块钱订阅一个专栏，跟着讲师的课程安排一节节学习，学习结束后，就可以对一项技术或者一系列知识点有很好的掌握，渐渐地，越来越多的人接受了这种学习模式。相应的，知识付费平台也像雨后春笋般成长了起来。目前做的比较大的像知乎、得到、喜马拉雅 FM 等，平台上每门课程的订阅人数少则几千，多则几万，甚至几十万，像得到的课程《薛兆丰的经济学课》累计订阅人数已经超过 40 万。所以未来一段时间内，知识付费仍将是一种很受欢迎的学习模式。 极客时间是极客邦科技（InfoQ）出品的一个知识付费平台，他们的 slogan 是“提升你的技术认知”，相应地，他们的课程也大多数都是面向技术方向。现在平台上总共有七十多门课程，每个月也会有五、六门新课上线，合作讲师都是各个领域的专家和高级工程师，有着丰富的实践经验，同时，他们的课程价格平均只有几十块，特别实惠。 而【秃头和尚】是知识付费领域的导学导购平台，在平台上面，提供了极客时间所有课程的入口，通过我的平台前去极客时间购买课程，在课程购买成功后，极课助手会向你进行返现，返现额度普遍超过 30%，有些超过 40%，最高可返 36 元。而在新课上线时，极客时间会进行限时优惠活动，价格本身就已经远远低于原价，再加上和尚的返现，最终购买课程的花费基本低于课程原价的 50%，所以特别划算。如果你想提升自己，学习更多技能，同时享受低价优惠，可以关注【秃头和尚】公众号了解更多信息。愿你我一起努力，变成更好的我们。 通过我的链接买极客时间课程，极客时间官方会给我一些返现，有 ¥36、¥24、¥18等等，当然我会把这些返现给你们，不从中间赚取一分钱，只为回报支持我的粉丝！ 同样道理你购买我分享的课程后，您分享后也可以得到 ¥36、¥24、¥18等等对应返现! 课程目录如下： 技术与商业案例解读AI技术内参左耳听风朱赟的技术管理课邱岳的产品手记人工智能基础课赵成的运维体系管理课推荐系统三十六式深入浅出区块链技术领导力实战笔记硅谷产品实战36讲从0开始学架构Java核心技术面试精讲微服务架构实战160讲趣谈网络协议从0开始学游戏开发机器学习40讲零基础学PythonReact实战进阶45讲软件测试52讲持续交付36讲快速上手Kotlin开发深入拆解Java虚拟机邱岳的产品实战程序员进阶攻略Go语言核心36讲技术管理实战36讲从0开始学微服务深入剖析Kubernetes数据结构与算法之美代码精进之路算法面试通关40讲白话法律42讲从0开始学大数据Nginx核心知识100讲MySQL实战45讲Linux性能优化实战Android开发高手课程序员的数学基础课玩转Git三剑客数据分析实战45讲10x程序员工作法TensorFlow快速入门与实战重学前端面试现场玩转Spring全家桶软件工程之美Java并发编程实战Go语言从入门到实战iOS开发高手课Vue开发实战趣谈Linux操作系统从0开始做增长许式伟的架构课大规模数据处理实战从0开发一款iOS App深入浅出计算机组成原理Web协议详解与抓包实战Python核心技术与实战深入拆解Tomcat &amp; Jetty 零基础学JavaJava性能调优实战OpenResty从入门到实战透视HTTP协议玩转webpackKafka核心技术与实战SQL必知必会Linux实战技能100讲Elasticsearch核心技术与实战黄勇的OKR实战笔记Flutter核心技术与实战Spring Boot与Kubernetes云原生微服务实践 从0打造音视频直播系统TypeScript开发实战消息队列高手课网络编程实战浏览器工作原理与实践Swift核心技术与实战编译原理之美ZooKeeper实战与源码剖析研发效率破局之道即时消息技术剖析与实战全栈工程师修炼指南高并发系统设计40问Node.js开发实战分布式技术原理与算法解析说透中台DevOps实战笔记Netty源码剖析与实战DDD实战课苏杰的产品创新课移动端自动化测试实战项目管理实战20讲设计模式之美JavaScript核心原理解析MongoDB高手课后端技术面试38讲现代C++实战30讲性能工程高手课安全攻防技能30讲性能测试实战30讲小马哥讲Spring核心编程思想摄影入门课人人都能学会的编程入门课Electron开发实战说透敏捷.NET Core开发实战接口测试入门课分布式协议与算法实战RPC实战与核心原理架构实战案例解析NLP实战高手课后端存储实战课深入浅出云计算Java业务开发常见错误100例图解 Google V8SRE实战手册检索技术核心20讲数据中台实战课Service Mesh实战Kafka核心源码解读Serverless入门课视觉笔记入门课分布式缓存高手课系统性能调优必知必会罗剑锋的C++实战笔记互联网人的英语私教课职场求生攻略微信小程序全栈开发实战软件设计之美编译原理实战课TensorFlow 2项目进阶实战正则表达式入门课分布式系统案例课跟月影学可视化OAuth 2.0实战课Web安全攻防实战Selenium自动化测试实战Vim 实用技巧必知必会如何看懂一幅画 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"ArrayList、LinkedList 你真的了解吗？","date":"2020-07-18T06:34:34.000Z","path":"2020/07/18/hello-world/","text":"1、 前言 经常在面试时，被问到集合的概念，集合 List、Map、Set 等底层设计以及其使用场景与注意细节。但大部分人的回答都是千篇一律，跟网上的答案一模一样，这是致命滴。其实，大家都错了，尤其是网上，更是误导大家，详细原因，且听我来分析。 2、集合 List 2.1 大家心中的 List在广大的网友心中，List 是一个缓存数据的容器，是 JDK 为开发者提供的一种集合类型。面试时，被问到最常见的就是 ArrayList 和 LinkedList 的区别。 相信大部分网友都能回答上：ArrayList 是基于数组实现，LinkedList 是基于链表实现。而在使用场景时，我发现大部分网友的答案都是：在新增、删除操作时，LinkedList 的效率要高于 ArrayList，而在查询、遍历操作的时候，ArrayList 的效率要高于 LinkedList。这个答案是否准确呢？今天就带大家验证一哈。 2.2 性能比较首先，大家都知道 ArrayList、LinkedList 都继承了 AbstractList 抽象类，而 AbstractList 实现了 List 接口。ArrayList 使用数组实现，而 LinkedList 使用了双向链表实现。接下来，我们就详细地分析下 ArrayList 和 LinkedList 的性能。 123public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable 在源码中，我们知道 ArrayList 除了实现克隆和序列化，还实现了 RandomAccess 接口。大家可能会对这个接口比较陌生，通过代码我们可以发现，这个接口其实是一个空接口，没有实现逻辑，那么 ArrayList 为什么要实现它呢？原来 RandomAccess 接口是一个标志接口，它标志着只要实现该接口，就能实现快速随机访问。 至于 ArrayList、LinkedList 的各种操作方法这里不再说了，大家可以看 这一篇。 接下来，我们看看一些测试数据，以测试 50000 次为例： ArrayList、LinkedList 新增测试 123头部：ArrayList.Time 大于 LinkedList.Time中间：ArrayList.Time 小于 LinkedList.Time末尾：ArrayList.Time 小于 LinkedList.Time 通过这测试，我们可以看到 LinkedList 新增元素的未必要快于 ArrayList。 由于 ArrayList 是数组实现的，而数组是一块连续的内存空间，在新增元素到数组头部的时候，需要对头部以后的数据进行重排，所以效率很低。而 LinkedList 是基于链表实现，在新增元素的时候，首先会通过循环查找到新增元素的位置，如果要新增的位置处于前半段，就从前往后找；若其位置处于后半段，就从后往前找。故 LinkedList 新增元素到头部是非常高效的。 在中间位置插入时，ArrayList 同样有部分数据需要重排，效率也不是很高，而 LinkedList 将元素新增到中间，耗时最久的，因为靠近中间位置，在新增元素之前的循环查找是遍历元素最多的操作。 而在尾部操作时，发现在没有扩容的前提下，ArrayList 的效率要高于 LinkedList。这是因为 ArrayList 在新增元素到尾部的时候，不需要复制、重排，效率非常高。而 LinkedList 虽然也不用循环查找元素，但 LinkedList 中多了 new 对象以及变换指针指向对象的逻辑，所以要耗时多于 ArrayList 的操作。 12345678910111213141516 public boolean add(E e) &#123; linkLast(e); return true;&#125;void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; ArrayList、LinkedList 删除测试 123头部：ArrayList.Time 大于 LinkedList.Time中间：ArrayList.Time 小于 LinkedList.Time末尾：ArrayList.Time 小于 LinkedList.Time 大家会发现 ArrayList 和 LinkedList 删除操作的测试结果和新增的结果很接近，这是一样的道理，我就不赘述了。 ArrayList、LinkedList 遍历测试 12for循环：ArrayList.Time 小于 LinkedList.Time迭代器：ArrayList.Time 几乎等于 LinkedList.Time 我们可以看到，LinkedList 的 for 循环遍历比不上 ArrayList 的 for 循环。这是因为 LinkedList 基于链表实现的，在使用 for 循环的时候，每一次 for 循环都会去遍历大半个 List，所以严重影响了遍历的效率。而 ArrayList 是基于数组实现的，并且实现了 RandomAccess 接口标志，意味着 ArrayList 可以实现快速随机访问，所以 for 循环非常快。LinkedList 的迭代遍历和 ArrayList 的迭代性能差不多，也不会太差，所以在遍历 LinkedList 时，我们要使用迭代循环遍历。 3、常错点 思考在一次 ArrayList 删除操作的过程中，有下面两种写法： 12345678public static void removeA(ArrayList&lt;String&gt; l) &#123; for (String s : l)&#123; if (s.equals(&quot;aaa&quot;)) &#123; l.remove(s); &#125; &#125;&#125; 1234567891011public static void removeB(ArrayList&lt;String&gt; l) &#123; Iterator&lt;String&gt; it &#x3D; l.iterator(); while (it.hasNext()) &#123; String str &#x3D; it.next(); if (str.equals(&quot;aaa&quot;)) &#123; it.remove(); &#125; &#125;&#125; 第一种写法错误，第二种是正确的，原因是上面的两种写法都有用到 list 内部迭代器Iterator，即遍历时，ArrayList 内部创建了一个内部迭代器 iterator，在使用 next 方法来取下一个元素时，会使用 ArrayList 里保存的一个用来记录 list 修改次数的变量 modCount，与 iterator 保存了一个叫 expectedModCount 的表示期望的修改次数进行比较，如果不相等则会抛出一个叫 ConcurrentModificationException 的异常。且在 for 循环中调用 list 中的 remove 方法，会走到一个 fastRemove 方法，该方法不是 iterator 中的方法，而是 ArrayList 中的方法，在该方法只做了 modCount++，而没有同步到 expectedModCount。所以不一致就抛出了 ConcurrentModificationException 异常了。 下面是 ArrayList 自己的remove 方法： 12345678910111213141516public boolean remove(Object o) &#123; if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false;&#125; 12345678private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work&#125; 如果有看过阿里 Java 编程规范就知道，在集合中进行 remove 操作时，不要在 foreach 循环里进行元素的 remove/add 操作。remove 元素使用 Iterator 方式，如果并发操作，需要对 Iterator 对象加锁。 结束福利 开源实战利用 k8s 作微服务的架构设计代码: https://gitee.com/damon_one/spring-cloud-k8s 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"基于OpenPAI细化部署 Hadoop 集群","date":"2020-07-18T06:34:34.000Z","path":"2020/07/18/hadoop-deploy/","text":"前提 https://github.com/microsoft/pai/tree/v0.14.0 Hadoop 2.9.0 k8s 1.9.4 (高版本未测) 本次讲解的主要是基于 Microsoft 开源的 OpenPAI，向大家通俗易懂的讲解 OpenPAI 是如何快速部署 Hadoop 集群的。便于大家快速部署Hadoop集群。 环境：1234ubuntu 16.04docker 18.06k8s 1.9.4Hadoop 2.9.0 1. 准备分析以上系统环境准备好，首先克隆 Microsoft 开源的 OpenPAI 的代码: https://github.com/microsoft/pai，切换到分支 v0.14.0。 由于我的目录在 /home/damon 下，所以直接: 12cd &#x2F;home&#x2F;damon&#x2F;paill 可以看到有如下目录: 其中，src 目录下都是一些代码目录以及脚本： 我们再看看与 src 同一级的 deployment 目录: 看着很多，其实我们只要看 quick-start 下的几个文件: 1234sudo vi deployment&#x2F;quick-start&#x2F;quick-start-example.yaml #配置master节点信息sudo vi deployment&#x2F;quick-start&#x2F;kubernetes-configuration.yaml.template #不作大改sudo vi deployment&#x2F;quick-start&#x2F;layout.yaml.template #增加机器相关信息sudo vi deployment&#x2F;quick-start&#x2F;services-configuration.yaml.template #配置docker相关信息 第一个配置文件主要是关于 master 节点。第二个配置主要是配置 k8s 的基本信息，因为 OpenPAI 不仅可以部署 Hadoop，还可以基于 Docker、python 来部署 k8s。第三个配置主要是增加机器的信息，我们需要修改的是配置 master 节点的信息，至于 node 节点，我们可以通过打标签的方式来。第四个配置主要是配置 docker 信息，存储 image 的各种 tag 形式。 根据配置模板生成配置文件1sudo python paictl.py config generate -i deployment&#x2F;quick-start&#x2F;quick-start-example.yaml -o ~&#x2F;damon&#x2F;pai-config -f 把生成的本地配置文件推送到远程 k8s 集群1sudo python paictl.py config push -p ~&#x2F;damon&#x2F;pai-config&#x2F; 执行上面的命令时，会出现输入命令，意思是让你输入一个 cluster-id，这是 OpenPAI 为集群设置的一个 id。输入后回车即可把配置推送到远程了。 获取 cluster-id如果生成过，执行 12damon@master:~&#x2F;damon&#x2F;pai$ sudo python paictl.py config get-id2020-07-16 19:56:48,066 [INFO] - deployment.confStorage.get_cluster_id : Cluster-id is: ustc 即可获取。 重点以上配置都结束后，上面说过了，配置中只有 master 节点信息，需要手动给 node 节点打标签: 1kubectl label node nodeName hdfsrole&#x3D;master 同样的标签还有类似: 123master labels:hdfsrole&#x3D;master,jobhistory&#x3D;true,launcher&#x3D;true,node-exporter&#x3D;true,pai-master&#x3D;true,yarnrole&#x3D;master,zookeeper&#x3D;true 12node labels:gpu-check&#x3D;true,hdfsrole&#x3D;worker,node-exporter&#x3D;true,pai-worker&#x3D;true,yarnrole&#x3D;worker 打完标签后，即可开始部署 Hadoop 集群了。 部署 Hadoop部署 Hadoop 的命令: 1sudo python paictl.py service start [-c &#x2F;path&#x2F;to&#x2F;kubeconfig] [ -n service-name ] 解释：-c 参数中带的是 k8s 授权的 kube-config 路径，-n 参数是服务名，如果没带 -n，则会默认启动 src 下的所有的服务。 Hadoop 中主要有这些服务: 1234567zookeeperhadoop-name-nodehadoop-data-nodehadoop-resource-managerhadoop-node-managerhadoop-batch-jobhadoop-jobhistory 那就手动一个个执行吧。执行一个后看看 pod 有没有启动，相关的 configmap 有没有创建，默认都是官方的。 注意项如果发现 namenode 启用了安全模式，而不想启用的话，执行: 12kubectl exec -it hadoop-name-node-e3bw9 bashhadoop dfsadmin -safemode leave 即：进入 name-node 容器中执行关闭。 模块功能说明 resource-manager 是调度中心，负责资源管理。 node-manager 是容器启动的的执行者。通常异常情况需要重启 node-manager。 zookeeper 为数据的存储中心。 namenode 和 datanode 为 hadoop 服务（HDFS）的基础层。 模块运维方法说明 resource-manager 重启：大量任务 waiting 和 stopping 和数据不一致等情况。 node-manager 重启：更新节点的资源信息或者节点故障等。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"微服务自动化部署CI/CD","date":"2020-07-14T12:03:14.000Z","path":"2020/07/14/ci-cd/","text":"一直有人说想了解微服务的自动化部署，今天它来了。 在微服务化的时代，自动化部署越来越成为企业的重中之重了，因为这样减少了人员的成本，开发人员将代码提交后，触发相关事件即可部署测试环境，甚至得到许可后部署到线上。这样，原先开发人员、运维人员等要做的事，通通不必再重复劳作了。这对于一个企业来说，leader比较在乎的一件事。今天讲解通过jenkins、gitlab、harbor、k8s来作简单的CI/CD平台，暂时未涉及到代码检测等。 环境：123ubuntu16.04docker18.04k8s1.13.x + 1. 准备以上系统环境准备好，本文讲述的是用 k8s 来进行部署 jenkins 2. 部署 jenkins新建部署脚本 jenkins.yaml: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263apiVersion: v1kind: Servicemetadata: name: jenkins-service namespace: default labels: app: jenkinsspec: type: NodePort ports: - port: 8080 targetPort: 8080 protocol: TCP nodePort: 30600 name: jenkins - port: 30000 targetPort: 30000 nodePort: 30000 protocol: TCP name: agent selector: app: jenkins tier: jenkins---apiVersion: apps&#x2F;v1kind: Deploymentmetadata: name: jenkins-deployment labels: app: jenkinsspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: jenkins tier: jenkins spec: containers: - name: jenkins image: jenkinsci&#x2F;blueocean:latest imagePullPolicy: Always ports: - containerPort: 8080 name: jenkins - containerPort: 30000 name: agent volumeMounts: - mountPath: &#x2F;var&#x2F;jenkins_home name: jenkins-data - mountPath: &#x2F;data&#x2F;jenkins name: jenkins-log-path volumes: - name: jenkins-data hostPath: path: &#x2F;home&#x2F;demo&#x2F;jenkins - name: jenkins-log-path hostPath: path: &#x2F;data&#x2F;jenkins 执行脚本： 1kubectl apply -f jenkins.yaml 查看部署成功的 pod 1kubectl get pod 同时会看到一个 service 生成，映射端口到外部。 接下来通过页面访问该服务，首次打开后，会让输入一个 admin 的密钥，这一串字符可以在日志中找到，执行 kubectl logs -f test-jenkins-c6bd58bf9-tgmsa 即可，输入一串字符后，进入下一步，需要安装一些插件，在安装插件的时候，需要注意的是，选择 jenkins 的 image 不同，其所需安装的插件也是不一样的，有的可能 image 已经存在了。具体请看：https://jenkins.io/zh/doc/book/installing/ 安装完插件后，进入下一步，创建第一个账户，管理员账户，一般建议创建，方便后面使用，注：邮箱也需要填写。 完成以上后，我们进入正式界面： 第一步：点击系统管理—&gt;插件管理，添加一些插件，这里有用到kubernetes的插件，故安装了kubernetes plugins，其他的可根据自行项目确定下载、安装。 第二步：点击系统管理—&gt;系统设置 添加jenkins的地址以及邮件地址 第三步：拉动滚动框到最下面，新增一个云 这里我加了kubernetes的配置，为什么后面会讲。 第四步：新建任务 在触发器中新增规则，最下面要生成token。 第五步： 接下来就是选择与gitlab互动，gitlab的地址以及凭据，凭据可通过首页加上可访问gitlab的用户信息，脚本路径需要根据自身的Jenkinsfile路径情况填写。 第六步： 如果启用全局安全，这个端口本身是50000，但由于k8s的默认nodePort范围是30000-32767，故可以修改在这区间内，比如：30000，这也是为什么上面创建时service的nodePort是30000了。 至此，jenkins环境配置完成 第七步：配置gitlab 这里的url就是在新建任务时生成的Gitlab webhook，token就是上面截图生成的token，最后add webhook。 第八步： 关于Harbor，自己可以简单搭建一个harbor服务，找度娘问一下很多，此处略。 第九步： 新建Jenkinsfile文件，如果刚才的路径是在项目根目录，则直接在项目根目录下创建Jenkinsfile文件: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def label &#x3D; &quot;mypod-$&#123;UUID.randomUUID().toString()&#125;&quot;def k8sPodYaml &#x3D; &quot;&quot;&quot; apiVersion: &quot;v1&quot; kind: &quot;Pod&quot; metadata: labels: jenkins: &quot;slave&quot; name: &quot;$&#123;label&#125;&quot; spec: containers: - env: - name: &quot;CI_ENV&quot; value: &quot;YES&quot; - name: &quot;JENKINS_AGENT_WORKDIR&quot; value: &quot;&#x2F;home&#x2F;jenkins&#x2F;agent&quot; image: &quot;jenkins&#x2F;jnlp-slave&quot; imagePullPolicy: &quot;Always&quot; name: &quot;jnlp&quot; resources: limits: &#123;&#125; requests: &#123;&#125; securityContext: privileged: false tty: false volumeMounts: - mountPath: &quot;&#x2F;home&#x2F;jenkins&#x2F;.kube&quot; name: &quot;volume-2&quot; readOnly: false - mountPath: &quot;&#x2F;var&#x2F;run&#x2F;docker.sock&quot; name: &quot;volume-0&quot; readOnly: false - mountPath: &quot;&#x2F;var&#x2F;inference&#x2F;config&quot; name: &quot;volume-1&quot; readOnly: false - mountPath: &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;kubectl&quot; name: &quot;volume-3&quot; readOnly: false - mountPath: &quot;&#x2F;home&#x2F;jenkins&#x2F;agent&quot; name: &quot;workspace-volume&quot; readOnly: false workingDir: &quot;&#x2F;home&#x2F;jenkins&#x2F;agent&quot; nodeSelector: &#123;&#125; restartPolicy: &quot;Never&quot; volumes: - hostPath: path: &quot;&#x2F;var&#x2F;run&#x2F;docker.sock&quot; name: &quot;volume-0&quot; - hostPath: path: &quot;&#x2F;home&#x2F;leinao&#x2F;.kube&quot; name: &quot;volume-2&quot; - hostPath: path: &quot;&#x2F;home&#x2F;leinao&#x2F;inference-deploy&#x2F;output_config&quot; name: &quot;volume-1&quot; - emptyDir: &#123;&#125; name: &quot;workspace-volume&quot; - hostPath: path: &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;kubectl&quot; name: &quot;volume-3&quot; 下面是自己的任务构建项目时的逻辑，由于我们这块用了自己的框架写的编译逻辑，故比较简单的，这里不合适公开，但是逻辑都差不多，大家可自行编写。 这块解释上面的k8s的yaml，这个就是为了在k8s中启动一个pod，通过该pod来执行构建逻辑的过程。 到此，关于Jenkins结合harbor、gitlab、k8s来实现CI/CD结束了，有几点注意的地方： 1. 如果jenkins是在容器中启动的一定要记得将这个端口(30000)暴露到外部，不然jenkins-master会不知道slave是否已经启动，会反复去创建pod只到超过重试次数。 2. 如果提示Jenkins doesn’t have label jenkins-jnlp-slave，可能原因： 1). 因为slave节点无法链接到jenkins节点开放端口50000导致 2). 因为slave镜像中slave启动失败导致的 3). 因为jenkins和k8s通信有延时导致超时jenkins会反复创建pod 4). 因为slave pod启动失败 5). 因为pipeline中指定的label与配置中的不一致导致 3. 也可以通过Ingress暴露的方式来进行暴露。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"Nginx 日常操作","date":"2020-07-14T07:05:15.000Z","path":"2020/07/14/nginx/","text":"Ubuntu 下 Nginx 安装 先下载相关文件: cd /usr/local/src sudo 模式下 wget http://nginx.org/download/nginx-1.10.2.tar.gz wget http://www.openssl.org/source/openssl-fips-2.0.10.tar.gz wget http://zlib.net/zlib-1.2.11.tar.gz wget https://netix.dl.sourceforge.net/project/pcre/pcre/8.40/pcre-8.40.tar.gz 安装nginx相关组件: 安装openssl sudo tar zxvf openssl-fips-2.0.10.tar.gz cd openssl-fips-2.0.10 sudo ./config && make && make install 安装pcre: sudo tar zxvf pcre-8.40.tar.gz cd pcre-8.40 sudo ./configure && make && make install 安装zlib: sudo tar zxvf zlib-1.2.11.tar.gz cd zlib-1.2.11 sudo ./configure && make && make install 安装nginx: sudo tar zxvf nginx-1.10.2.tar.gz cd nginx-1.10.2 sudo ./configure && make && make install 启动nginx: /usr/local/nginx/sbin/nginx 查看nginx是否启动成功: netstat -lnp 查看 nginx 的测试，以及相关配置: nginx -t nginx 默认的日志位置: tail -f /var/log/nginx/access.log 加配置: sudo /usr/local/nginx/sbin/nginx sudo /usr/local/nginx/sbin/nginx -s stop(quit、reload) sudo /usr/local/nginx/sbin/nginx -h sudo vi /usr/local/nginx/conf/nginx.conf Ubuntu 下 Nginx 完全卸载 先暂停 nginx 服务 1sudo service nginx stop 删除 nginx，–purge 包括配置文件: 1sudo apt-get --purge remove nginx 移除全部不使用的软件包: 1sudo apt-get autoremove 列出与 nginx 相关的软件 并删除显示的软件: 12345dpkg --get-selections|grep nginxsudo apt-get --purge remove nginxsudo apt-get --purge remove nginx-commonsudo apt-get --purge remove nginx-core 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"Spring cloud 之熔断机制（实战）","date":"2020-07-10T06:16:14.000Z","path":"2020/07/10/springcloud-04/","text":"前面讲过 Spring cloud 之多种方式限流（实战）来处理请求频繁的压力。大家都知道，多个微服务之间调用的时候，假设微服务 A 调用微服务 B 和微服务 C，微服务 B 和微服务 C 有调用其他的微服务，这就是所谓的 扇出，若扇出的链路上某个微服务的请求时间过长或者不可用，对微服务 A 的调用就会占用越来越多的时间以及更多资源，进而引起系统雪崩，即”雪崩效应”。 这个时候，需要一个机制来保证当某个微服务出现异常时（请求反应慢或宕机），其整个流程还是阔以友好滴进行下去。即向调用方返回一个符合预期的、可处理的备选响应(FallBack)，而不是长时间的等待或者抛出调用方无法处理的异常，这样就可以保证调用方的线程不会被长时间、无厘头滴占用，从而避免了故障在分布式系统中的蔓延，乃至雪崩。我们把这个机制，或者这种处理方式叫作“熔断器”。 熔断机制是应对雪崩效应的一种微服务链路保护机制，当整个链路的某个微服务异常时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回“合理”的响应信息。当检测到该节点微服务正常后恢复调用链路，在Spring cloud 框架机制通过 Hystrix 实现，Hystrix 会监控微服务见调用的状况，当失败的调用到一个阈值，默认是5秒内20次调用失败就会启动熔断机制，熔断机制的注解是 @HystrixCommand。 最近研究了一下 Spring cloud 的熔断机制，特分享一些代码，以及实战中的坑。 在Spring cloud 中，假设有几个微服务：用户管理服务、订单服务、鉴权中心、物流服务等。这时，订单服务中，某个接口请求用户管理服务，这个时候如果需要熔断机制，该怎么处理呢？ 首先，订单服务引入依赖： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; 这个时候，订单服务启动类中需要引用熔断注解 @EnableCircuitBreaker，使其生效： 12345678910111213141516171819&#x2F;** * @author Damon * @date 2020年1月13日 下午3:23:06 * *&#x2F;@EnableOAuth2Sso@Configuration@EnableAutoConfiguration@ComponentScan(basePackages &#x3D; &#123;&quot;com.damon&quot;&#125;)@EnableDiscoveryClient@EnableCircuitBreakerpublic class OrderApp &#123; public static void main(String[] args) &#123; SpringApplication.run(OrderApp.class, args); &#125;&#125; 这里，不要忘记注解 @EnableDiscoveryClient 来相互暴露服务。 最后需要在调用用户管理服务的函数中，加入注解 @HystrixCommand： 123456789101112131415161718192021222324252627282930313233343536373839@HystrixCommand(fallbackMethod &#x3D; &quot;admin_service_fallBack&quot;, commandProperties &#x3D; &#123; @HystrixProperty(name &#x3D; &quot;execution.isolation.thread.timeoutInMilliseconds&quot;, value &#x3D; &quot;5000&quot;) &#125;)&#x2F;&#x2F;隔离策略:execution.isolation.strategy &#x3D;SEMAPHORE or THREAD(不配置默认) @Override public Response&lt;Object&gt; getUserInfo(HttpServletRequest req, HttpServletResponse res) &#123; ResponseEntity&lt;String&gt; forEntity &#x3D; restTemplate.getForEntity(envConfig.getAdmin_web_url() + &quot;&#x2F;api&#x2F;user&#x2F;getUserInfo&quot;, String.class); HttpHeaders headers &#x3D; new HttpHeaders(); MediaType type &#x3D; MediaType.parseMediaType(&quot;application&#x2F;json; charset&#x3D;UTF-8&quot;); headers.setContentType(type); headers.add(&quot;Accept&quot;, MediaType.APPLICATION_JSON.toString()); headers.add(&quot;Authorization&quot;, &quot;bearer &quot; + StrUtil.subAfter(req.getHeader(&quot;Authorization&quot;), &quot;bearer &quot;, false)); HttpEntity&lt;String&gt; formEntity &#x3D; new HttpEntity&lt;String&gt;(null, headers); String body &#x3D; &quot;&quot;; try &#123; ResponseEntity&lt;String&gt; responseEntity &#x3D; restTemplate.exchange(&quot;http:&#x2F;&#x2F;admin-web-service&#x2F;api&#x2F;user&#x2F;getUserInfo&quot;, HttpMethod.GET, formEntity, String.class); if (responseEntity.getStatusCodeValue() &#x3D;&#x3D; 200) &#123; logger.debug(String.format(&quot;request getUserInfo return: &#123;&#125;&quot;, JSON.toJSON(responseEntity.getBody()))); return Response.ok(responseEntity.getStatusCodeValue(), 0, &quot;success&quot;, JSON.toJSON(responseEntity.getBody())); &#125; &#125; catch (Exception e) &#123; logger.error(&quot;loadJobDetail error&quot;); logger.error(e.getMessage(), e); &#125; return null; &#125; &#x2F;** * 熔断时调用的方法 * * 参数要与被请求的方法的参数一致 * * @return *&#x2F; private Response&lt;Object&gt; admin_service_fallBack(HttpServletRequest req, HttpServletResponse res) &#123; String token &#x3D; StrUtil.subAfter(req.getHeader(&quot;Authorization&quot;), &quot;bearer &quot;, false); logger.info(&quot;admin_service_fallBack token: &#123;&#125;&quot;, token); return Response.ok(200, -2, &quot;用戶服務掛啦!&quot;, null); &#125; 其中上面代码需要注意的是：注解中 fallbackMethod 的值指定了熔断后的处理函数，这个函数的参数与当前的调用方法的参数需要保持一致，否则报错： 1com.netflix.hystrix.contrib.javanica.exception.FallbackDefinitionException:fallback method wasn&#39;t found. 最后，配置 Hystrix 相关的参数配置yaml： 12hystrix.command.BackendCall.execution.isolation.thread.timeoutInMilliseconds: 5000hystrix.threadpool.BackendCallThread.coreSize: 5 其中第一个配置在调用函数中其实也可以配置： 12@HystrixCommand(fallbackMethod &#x3D; &quot;admin_service_fallBack&quot;, commandProperties &#x3D; &#123; @HystrixProperty(name &#x3D; &quot;execution.isolation.thread.timeoutInMilliseconds&quot;, value &#x3D; &quot;3000&quot;) &#125;) 这里配置的3000毫秒生效后，如果配置文件中也配置了，则会被覆盖。 如果不加@HystrixCommand中的commandProperties=@HystrixProperty注解配置，下面的FallBack函数admin_service_fallBack()是一个线程；@HystrixCommand()是一个隔离线程。若加上commandProperties=@HystrixProperty注解配置后，将2个线程合并到一个线程里。 这样到此为止，调用方就结束配置了，至于被调用方，相关配置与源码在Spring Cloud Kubernetes之实战二服务注册与发现&nbsp;一文中，讲过被调用服务的相关，这里的http://admin-web-service 为被调用服务，则在其服务启动类中需要注解 @EnableDiscoveryClient： 12345678910111213141516171819202122232425262728293031package com.damon;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.boot.autoconfigure.security.oauth2.client.EnableOAuth2Sso;import org.springframework.boot.context.properties.EnableConfigurationProperties;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;import com.damon.config.EnvConfig;&#x2F;** * @author Damon * @date 2020年1月13日 下午3:23:06 * *&#x2F;@EnableOAuth2Sso@Configuration@EnableAutoConfiguration@ComponentScan(basePackages &#x3D; &#123;&quot;com.damon&quot;&#125;)@EnableConfigurationProperties(EnvConfig.class)@EnableDiscoveryClientpublic class AdminApp &#123; public static void main(String[] args) &#123; SpringApplication.run(AdminApp.class, args); &#125;&#125; 另外，配置 RestTemplate 的 Bean 中加上注解 @LoadBalanced 需要作 LB，这样利用服务名来根据 LB 规则找到对应的其中一个服务，这样比较明显看出 LB 的效果： 12345678910111213141516171819202122232425262728293031package com.damon.config;import javax.annotation.Resource;import org.springframework.cloud.client.loadbalancer.LoadBalanced;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.core.env.Environment;import org.springframework.http.client.SimpleClientHttpRequestFactory;import org.springframework.web.client.RestTemplate;&#x2F;** * @author Damon * @date 2018年2月2日 下午7:15:53 *&#x2F;@Configurationpublic class BeansConfig &#123; @Resource private Environment env; @LoadBalanced&#x2F;&#x2F;就不能用ip等形式来请求其他服务 @Bean public RestTemplate restTemplate() &#123; SimpleClientHttpRequestFactory requestFactory &#x3D; new SimpleClientHttpRequestFactory(); requestFactory.setReadTimeout(env.getProperty(&quot;client.http.request.readTimeout&quot;, Integer.class, 15000)); requestFactory.setConnectTimeout(env.getProperty(&quot;client.http.request.connectTimeout&quot;, Integer.class, 3000)); RestTemplate rt &#x3D; new RestTemplate(requestFactory); return rt; &#125;&#125; 最后如果没问题了，可以先暂停用户管理服务，然后运行订单服务时，返回熔断结果： 1&#123;&quot;message&quot;:&#123;&quot;code&quot;:-2,&quot;message&quot;:&quot;用戶服務掛啦!&quot;,&quot;status&quot;:200&#125;&#125; OK，Spring cloud 熔断实战就结束了！ 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"Spring cloud 之多种方式限流（实战）","date":"2020-07-10T06:15:01.000Z","path":"2020/07/10/springcloud-03/","text":"在频繁的网络请求时，服务有时候也会受到很大的压力，尤其是那种网络攻击，非法的。这样的情形有时候需要作一些限制。例如：限制对方的请求，这种限制可以有几个依据：请求IP、用户唯一标识、请求的接口地址等等。 当前限流的方式也很多：Spring cloud 中在网关本身自带限流的一些功能，基于 redis 来做的。同时，阿里也开源了一款：限流神器 Sentinel。今天我们主要围绕这两块来实战微服务的限流机制。 首先讲 Spring cloud 原生的限流功能，因为限流可以是对每个服务进行限流，也可以对于网关统一作限流处理。 一、实战基于 Spring cloud Gateway 的限流 pom.xml引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis-reactive&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; 其基础是基于redis，所以： 1234567891011121314spring: application: name: gateway-service redis: #redis相关配置 database: 8 host: 10.12.15.5 port: 6379 password: 123456 #有密码时设置 jedis: pool: max-active: 8 max-idle: 8 min-idle: 0 timeout: 10000ms 接下来需要注入限流策略的 bean： 1234567891011121314151617181920212223242526272829303132@Primary @Bean(value &#x3D; &quot;ipKeyResolver&quot;) KeyResolver ipKeyResolver() &#123; return exchange -&gt; Mono.just(exchange.getRequest().getRemoteAddress().getHostName()); &#x2F;&#x2F;return exchange -&gt; Mono.just(exchange.getRequest().getRemoteAddress().getAddress().getHostAddress()); &#x2F;&#x2F;return exchange -&gt; Mono.just(exchange.getRequest().getRemoteAddress().getAddress().getHostAddress()); &#125; &#x2F;** * API限流 * @return * @author Damon * @date 2020年3月18日 * *&#x2F; @Bean(value &#x3D; &quot;apiKeyResolver&quot;) KeyResolver apiKeyResolver() &#123; return exchange -&gt; Mono.just(exchange.getRequest().getPath().value()); &#125; &#x2F;** * 请求路径中必须携带userId参数 * 用户限流 * @return * @author Damon * @date 2020年3月18日 * *&#x2F; @Bean(value &#x3D; &quot;userKeyResolver&quot;) KeyResolver userKeyResolver() &#123; return exchange -&gt; Mono.just(exchange.getRequest().getQueryParams().getFirst(&quot;userId&quot;)); &#125; 这里引入ipKeyResolver、apiKeyResolver、userKeyResolver三种策略，可以利用注解 @Primary 来决定其中一个被使用。 注入bean后，需要在配置中备用： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576spring: application: name: gateway-service redis: #redis相关配置 database: 8 host: 10.12.15.5 port: 6379 password: 123456 #有密码时设置 jedis: pool: max-active: 8 max-idle: 8 min-idle: 0 timeout: 10000ms cloud: kubernetes: discovery: all-namespaces: true gateway: discovery: locator: enabled: true lowerCaseServiceId: true routes: #路由配置:参数为一个List - id: cas-server #唯一标识 uri: lb:&#x2F;&#x2F;cas-server-service #转发的地址,写服务名称 order: -1 predicates: - Path&#x3D;&#x2F;cas-server&#x2F;** #判断匹配条件,即地址带有&#x2F;ribbon&#x2F;**的请求,会转发至lb:cas-server-service filters: - StripPrefix&#x3D;1 #去掉Path前缀,参数为1代表去掉&#x2F;ribbon - name: RequestRateLimiter #基于redis的Gateway的自身限流 args: redis-rate-limiter.replenishRate: 1 # 允许用户每秒处理多少个请求 redis-rate-limiter.burstCapacity: 3 # 令牌桶的容量，允许在一秒钟内完成的最大请求数 key-resolver: &quot;#&#123;@ipKeyResolver&#125;&quot; #SPEL表达式取的对应的bean - id: admin-web uri: lb:&#x2F;&#x2F;admin-web-service order: -1 predicates: - Path&#x3D;&#x2F;admin-web&#x2F;** filters: - StripPrefix&#x3D;1 - name: RequestRateLimiter args: redis-rate-limiter.replenishRate: 1 # 允许用户每秒处理多少个请求 redis-rate-limiter.burstCapacity: 3 # 令牌桶的容量，允许在一秒钟内完成的最大请求数 key-resolver: &quot;#&#123;@ipKeyResolver&#125;&quot; #SPEL表达式取的对应的bean - id: order-service uri: lb:&#x2F;&#x2F;order-service-service order: -1 predicates: - Path&#x3D;&#x2F;order-service&#x2F;** filters: - StripPrefix&#x3D;1 - name: RequestRateLimiter args: redis-rate-limiter.replenishRate: 1 # 允许用户每秒处理多少个请求 redis-rate-limiter.burstCapacity: 3 # 令牌桶的容量，允许在一秒钟内完成的最大请求数 key-resolver: &quot;#&#123;@ipKeyResolver&#125;&quot; #SPEL表达式取的对应的bean http: encoding: charset: UTF-8 enabled: true force: true mvc: throw-exception-if-no-handler-found: true main: allow-bean-definition-overriding: true # 当遇到同样名称时，是否允许覆盖注册 这里是在原有的路由基础上加入 RequestRateLimiter限流过滤器，包括三个参数： 12345- name: RequestRateLimiter #基于redis的Gateway的自身限流 args: redis-rate-limiter.replenishRate: 3 #允许用户每秒处理多少个请求 redis-rate-limiter.burstCapacity: 5 #令牌桶的容量，允许在一秒钟内完成的最大请求数 key-resolver: &quot;#&#123;@ipKeyResolver&#125;&quot; #SPEL表达式取的对应的bean 其中 replenishRate，其含义表示允许每秒处理请求数； burstCapacity 表示允许在一秒内处理的最大请求数； key-resolver 这里采用请求 IP 限流，利用SPEL 表达式取对应的 bean 写一个小脚本来压测一下： 123for i in $(seq 1 30000); do echo $(expr $i \\\\* 3 + 1);curl -i -H &quot;Accept: application&#x2F;json&quot; -H &quot;Authorization:bearer b064d95b-af3f-4053-a980-377c63ab3413&quot; -X GET http:&#x2F;&#x2F;10.10.15.5:5556&#x2F;order-service&#x2F;api&#x2F;order&#x2F;getUserInfo;donefor i in $(seq 1 30000); do echo $(expr $i \\\\* 3 + 1);curl -i -H &quot;Accept: application&#x2F;json&quot; -H &quot;Authorization:bearer b064d95b-af3f-4053-a980-377c63ab3413&quot; -X GET http:&#x2F;&#x2F;10.10.15.5:5556&#x2F;admin-web&#x2F;api&#x2F;user&#x2F;getCurrentUser;done 上面两个脚本分别对2个服务进行压测，打印结果： 12345678910111213141516171819202122232425262728293031323334HTTP&#x2F;1.1 200 OKtransfer-encoding: chunkedX-RateLimit-Remaining: 2X-RateLimit-Burst-Capacity: 3X-RateLimit-Replenish-Rate: 1Expires: 0Cache-Control: no-cache, no-store, max-age&#x3D;0, must-revalidateSet-Cookie: ORDER-SERVICE-SESSIONID&#x3D;R99Ljit9XvfCapyUJDWL8I0rZqxReoY6HwcQV2n2; path&#x3D;&#x2F;X-XSS-Protection: 1; mode&#x3D;blockPragma: no-cacheX-Frame-Options: DENYDate: Thu, 19 Mar 2020 06:32:27 GMTX-Content-Type-Options: nosniffContent-Type: application&#x2F;json;charset&#x3D;UTF-8&#123;&quot;message&quot;:&#123;&quot;status&quot;:200,&quot;code&quot;:0,&quot;message&quot;:&quot;success&quot;&#125;,&quot;data&quot;:&quot;&#123;\\&quot;message\\&quot;:&#123;\\&quot;status\\&quot;:200,\\&quot;code\\&quot;:0,\\&quot;message\\&quot;:\\&quot;get user success\\&quot;&#125;,\\&quot;data\\&quot;:&#123;\\&quot;id\\&quot;:23,\\&quot;isAdmin\\&quot;:1,\\&quot;userId\\&quot;:\\&quot;fbb18810-e980-428c-932f-848f3b9e7c84\\&quot;,\\&quot;userType\\&quot;:\\&quot;super_admin\\&quot;,\\&quot;username\\&quot;:\\&quot;admin\\&quot;,\\&quot;realName\\&quot;:\\&quot;super_admin\\&quot;,\\&quot;password\\&quot;:\\&quot;$2a$10$89AqlYKlnsTpNmWcCMvgluRFQ&#x2F;6MLK1k&#x2F;nkBpz.Lw6Exh.WMQFH6W\\&quot;,\\&quot;phone\\&quot;:null,\\&quot;email\\&quot;:null,\\&quot;createBy\\&quot;:\\&quot;admin\\&quot;,\\&quot;createTime\\&quot;:1573119753172,\\&quot;updateBy\\&quot;:\\&quot;admin\\&quot;,\\&quot;updateTime\\&quot;:1573119753172,\\&quot;loginTime\\&quot;:null,\\&quot;expireTime\\&quot;:null,\\&quot;remarks\\&quot;:\\&quot;super_admin\\&quot;,\\&quot;delFlag\\&quot;:0,\\&quot;loginType\\&quot;:null&#125;&#125;&quot;&#125;ex同一秒内多次后：HTTP&#x2F;1.1 429 Too Many RequestsX-RateLimit-Remaining: 0X-RateLimit-Burst-Capacity: 3X-RateLimit-Replenish-Rate: 1content-length: 0expr: syntax errorHTTP&#x2F;1.1 429 Too Many RequestsX-RateLimit-Remaining: 0X-RateLimit-Burst-Capacity: 3X-RateLimit-Replenish-Rate: 1content-length: 0expr: syntax error 从上面可以看到，执行后，会出现调用失败的情况，状态变为429 (Too Many Requests) 。 二、基于阿里开源限流神器：Sentinel 首先引入依赖： 12345&lt;!--基于 阿里的sentinel作限流 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; 在配置文件 application.yaml 文件中配置，需要新增2个配置： 123456789101112131415161718spring: application: name: admin-web cloud: kubernetes: discovery: all-namespaces: true sentinel: eager: true #取消Sentinel控制台的懒加载 transport: dashboard: 10.12.15.2:8080 #sentinel的Dashboard地址 port: 8719 #是sentinel应用端和控制台通信端口 heartbeat-interval-ms: 500 #心跳时间 scg: fallback: #scg.fallback为sentinel限流后的响应配置 mode: response response-status: 455 response-body: 已被限流 其中，这里面配置了一个服务：spring.cloud.sentinel.transport.dashboard，配置的是 sentinel 的 Dashboard 地址。同时 spring.cloud.sentinel.transport.port 这个端口配置会在应用对应的机器上启动一个Http Server，该 Server 会与 Sentinel 控制台做交互。 Sentinel 默认为所有的 HTTP 服务提供限流埋点，上面配置完成后自动完成所有埋点，只需要控制配置限流规则即可。 这里我们讲下通过注解来给指定接口函数加上限流埋点，写一个RestController，在接口函数上加上注解 @SentinelResource： 123456789@GetMapping(value &#x3D; &quot;&#x2F;getToken&quot;)@SentinelResource(&quot;getToken&quot;)public Response&lt;Object&gt; getToken(Authentication authentication)&#123; &#x2F;&#x2F;Authentication authentication &#x3D; SecurityContextHolder.getContext().getAuthentication(); authentication.getCredentials(); OAuth2AuthenticationDetails details &#x3D; (OAuth2AuthenticationDetails)authentication.getDetails(); String token &#x3D; details.getTokenValue(); return Response.ok(200, 0, &quot;get token success&quot;, token);&#125; 以上代码部分完成了，接下来先安装SentinelDashBoard，Sentinel DashBoard下载地址：https://github.com/alibaba/Sentinel/releases 。 下载完成后，命令启动： 1java -jar sentinel-dashboard-1.6.2.jar 默认启动端口为8080，访问 IP:8080，就可以显示 Sentinel 的登录界面，用户名与密码均为sentinel。登录 Dashboard 成功后，多次访问接口”/getToken”，可以在 Dashboard 看到相应数据，这里不展示了。接下来可以设置接口的限流功能，在 “+流控” 按钮点击打开设置界面，设置阈值类型为 qps，单机阈值为5。 浏览器重复请求 http://10.10.15.5:5556/admin-web/api/user/getToken 如果超过阀值就会出现如下界面信息： 1Blocked by Sentinel (flow limiting) 此时，就看到Sentinel 限流起作用了，可以加上 spring.cloud.sentinel.scg.fallback 为sentinel 限流后的响应配置，亦可自定义限流异常信息： 1234567891011121314151617@GetMapping(value &#x3D; &quot;&#x2F;getToken&quot;)@SentinelResource(value &#x3D; &quot;getToken&quot;, blockHandler &#x3D; &quot;handleSentinelException&quot;, blockHandlerClass &#x3D; &#123;MySentinelException.class&#125;))public Response&lt;Object&gt; getToken(Authentication authentication)&#123; &#x2F;&#x2F;Authentication authentication &#x3D; SecurityContextHolder.getContext().getAuthentication(); authentication.getCredentials(); OAuth2AuthenticationDetails details &#x3D; (OAuth2AuthenticationDetails)authentication.getDetails(); String token &#x3D; details.getTokenValue(); return Response.ok(200, 0, &quot;get token success&quot;, token);&#125;public class MySentinelException &#123; public static Response&lt;Object&gt; handleSentinelException(BlockException e) &#123; Map&lt;String,Object&gt; map&#x3D;new HashMap&lt;&gt;(); logger.info(&quot;Oops: &quot; + ex.getClass().getCanonicalName()); return Response.ok(200, -8, &quot;通过注解 @SentinelResource 配置限流埋点并自定义限流后的处理逻辑&quot;, null); &#125;&#125; 这里讲下注解 @SentinelResource 包含以下属性： value：资源名称，必需项； entryType：入口类型，可选项（默认为 EntryType.OUT）； blockHandler：blockHandlerClass中对应的异常处理方法名，参数类型和返回值必须和原方法一致； blockHandlerClass：自定义限流逻辑处理类 Sentinel 限流逻辑处理完毕了，但每次服务重启后，之前配置的限流规则就会被清空。因为是内存形式的规则对象。所以下面就讲下用 Sentinel 的一个特性 ReadableDataSource 获取文件、数据库或者配置中心设置限流规则，目前支持 Apollo、Nacos、ZK 配置来管理。 首先回忆一下，一条限流规则主要由下面几个因素组成： resource：资源名，即限流规则的作用对象，即为注解 @SentinelResource 的value； count：限流阈值；grade：限流阈值类型（QPS 或并发线程数）； limitApp：流控针对的调用来源，若为 default 则不区分调用来源； strategy：基于调用关系的限流策略； controlBehavior：流量控制效果（直接拒绝、排队等待、匀速器模式） 理解了意思，接下来通过文件来配置： 1234#通过文件读取限流规则spring.cloud.sentinel.datasource.ds1.file.file&#x3D;classpath:flowrule.jsonspring.cloud.sentinel.datasource.ds1.file.data-type&#x3D;jsonspring.cloud.sentinel.datasource.ds1.file.rule-type&#x3D;flow 在resources新建一个文件，比如 flowrule.json 添加限流规则： 123456789101112131415161718[ &#123; &quot;resource&quot;: &quot;getToken&quot;, &quot;count&quot;: 1, &quot;controlBehavior&quot;: 0, &quot;grade&quot;: 1, &quot;limitApp&quot;: &quot;default&quot;, &quot;strategy&quot;: 0 &#125;, &#123; &quot;resource&quot;: &quot;resource&quot;, &quot;count&quot;: 1, &quot;controlBehavior&quot;: 0, &quot;grade&quot;: 1, &quot;limitApp&quot;: &quot;default&quot;, &quot;strategy&quot;: 0 &#125;] 重新启动项目，出现如下日志说明成功： 12DataSource ds1-sentinel-file-datasource start to loadConfigDataSource ds1-sentinel-file-datasource load 2 FlowRule 如果采用 Nacos 作为配置获取限流规则，可在文件中加如下配置： 1234567891011121314151617181920spring: application: name: order-service cloud: nacos: config: server-addr: 10.10.15.5:8848 discovery: server-addr: 10.10.15.5:8848 sentinel: eager: true transport: dashboard: 10.10.15.5:8080 datasource: ds1: nacos: server-addr: 10.10.15.5:8848 dataId: $&#123;spring.application.name&#125;-flow-rules data-type: json rule-type: flow 以上即为限流的两种方式。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"面试被问finally 和 return，到底谁先执行？","date":"2020-07-10T06:13:40.000Z","path":"2020/07/10/core-java01/","text":"经常有人面试被问到，finally 和 return，到底谁先执行呢？ 为了解决这个问题，其实我们可以先想想 finally 是被用来干嘛的呢？它是被用来结束一些正常的收尾动作或结束标识。也就是说无论怎么样，finally 都会被最后执行。例如：一般在操作数据库时，用Jdbc连接池连接数据库后释放资源，需要 finally 来处理。再如 redis 连接，在获取连接池处理完数据的增删改查后，需要释放其连接池。 但是，如果 return 是在 finally 前面呢？或者在 finally 后面呢？我们先来看看 return 在 finally 前面时，如： 1234567891011121314151617181920212223242526272829303132333435363738package com.test;&#x2F;** * * * @author Damon * @date 2020年3月18日 上午11:02:08 * *&#x2F;public class App &#123; public static void main(String[] args) &#123; System.out.println(&quot;return result: &quot; + test()); &#125; public static int test() &#123; try &#123; Thread.sleep(1); System.out.println(&quot;執行 return 1&quot;); return 1;&#x2F;&#x2F; return 在try里，則先執行，再執行finally后才有可能执行该return &#125; catch (InterruptedException e) &#123; e.printStackTrace(); return -1; &#125; finally &#123; System.out.println(&quot;执行 finally&quot;); &#x2F;&#x2F;return 3; &#125; &#x2F;&#x2F;System.out.println(&quot;執行 return 2&quot;); &#x2F;&#x2F;return 1; &#125;&#125;结果：執行 return 1执行 finallyreturn result: 1 也就是说，在执行 return 之前，先执行了 finally。 我们在看，如果 finally 前面有 return，在其内部也有 return： 1234567891011121314151617181920212223242526272829303132333435363738package com.test;&#x2F;** * * * @author Damon * @date 2020年3月18日 上午11:02:08 * *&#x2F;public class App &#123; public static void main(String[] args) &#123; System.out.println(&quot;return result: &quot; + test()); &#125; public static int test() &#123; try &#123; Thread.sleep(1); System.out.println(&quot;執行 return 1&quot;); return 1;&#x2F;&#x2F; return 在try里，則先執行，再執行finally后才有可能执行该return &#125; catch (InterruptedException e) &#123; e.printStackTrace(); return -1; &#125; finally &#123; System.out.println(&quot;执行 finally&quot;); return 3; &#125; &#x2F;&#x2F;System.out.println(&quot;執行 return 2&quot;); &#x2F;&#x2F;return 1; &#125;&#125;结果：執行 return 1执行 finallyreturn result: 3 其内部被 return 后，就不再执行前面那个 return 了。 我们再来看 return 在 finally 之后，如： 1234567891011121314151617181920212223242526272829303132333435363738package com.test;&#x2F;** * * * @author Damon * @date 2020年3月18日 上午11:02:08 * *&#x2F;public class App &#123; public static void main(String[] args) &#123; System.out.println(&quot;return result: &quot; + test()); &#125; public static int test() &#123; try &#123; Thread.sleep(1); &#x2F;&#x2F;System.out.println(&quot;執行 return 1&quot;); &#x2F;&#x2F;return 1;&#x2F;&#x2F; return 在try里，則先執行，再執行finally后才有可能执行该return &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#x2F;&#x2F;return -1; &#125; finally &#123; System.out.println(&quot;执行 finally&quot;); &#x2F;&#x2F;return 3; &#125; System.out.println(&quot;執行 return 2&quot;); return 1; &#125;&#125;结果：执行 finally執行 return 2return result: 1 总结：finally 在 return 之后时，先执行 finally 后，再执行该 return；finally 内含有 return 时，直接执行其 return 后结束；finally 在 return 前，执行完 finally 后再执行 return。 接下来还有常被问到的是：Java 中 final、finally、finalize 的区别与用法： final 用于声明属性，方法和类，分别表示属性不可变，方法不可覆盖，类不可继承。即如果一个类被声明为 final，意味着它不能作为父类被继承，因此一个类不能同时被声明为 abstract 的，又被声明为 final 的。变量或方法被声明为 final，可以保证它们在使用中不被修改。被声明为 final 的变量必须在声明时给赋予初值，而在以后的引用中只能读取，不可修改。被声明为 final 的方法也同样只能使用，不能重载。 finally 是异常处理语句结构的一部分，总是执行，常见的场景：释放一些资源，例如前面所说的 redis、db 等。在异常处理时提供 finally 块来执行任何清除操作，即在执行 catch 后会执行 finally 代码块。 finalize 是 Object 类的一个方法，在垃圾收集器执行的时候会调用被回收对象的此方法，可以覆盖此方法提供垃圾收集时的其他资源回收，例如关闭文件等。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"Spring Cloud Kubernetes之实战一配置管理","date":"2020-07-10T06:10:45.000Z","path":"2020/07/10/springcloud-02/","text":"一直以来，玩springcloud的，基本都是在玩Springboot1.x，Springcloud(Dalston版)的众多相关组件来做配置中心、服务注册与发现，网关用的是Netflix公司对springboot做的LB，等等，但是这些东西太过沉重，复杂了。在一个以k8s为基础的iaas服务系统，如果不用k8s的特性来做这些事，那是说不过去的。理由这就不重复述说了。一句话：减少系统服务的复杂性。 本文主要介绍springcloud结合k8s，做配置管理，避免更多服务组件的冗余，完美填坑版！ 环境： ubuntu16.04 docker18.04 k8s1.13.x + maven3.5.3 java1.8 + springboot 2.1.1 spring-cloud-kubernetes：1.0.1.RELEAS 前提 Ubuntu下安装docker18.04 or 其它较高版本，k8s1.13.x及以上，jvm环境等。 创建项目基础依赖： 12345678910111213141516171819202122232425262728293031323334&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;&#x2F;artifactId&gt; &lt;version&gt;2.1.8.RELEASE&lt;&#x2F;version&gt; &lt;relativePath&#x2F;&gt; &lt;&#x2F;parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;&#x2F;project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;&#x2F;java.version&gt; &lt;swagger.version&gt;2.6.1&lt;&#x2F;swagger.version&gt; &lt;xstream.version&gt;1.4.7&lt;&#x2F;xstream.version&gt; &lt;pageHelper.version&gt;4.1.6&lt;&#x2F;pageHelper.version&gt; &lt;fastjson.version&gt;1.2.51&lt;&#x2F;fastjson.version&gt; &lt;shiro.version&gt;1.3.0&lt;&#x2F;shiro.version&gt; &lt;!-- &lt;kubernetes-client-version&gt;6.0.1&lt;&#x2F;kubernetes-client-version&gt; --&gt; &lt;kubernetes-client-version&gt;5.0.0&lt;&#x2F;kubernetes-client-version&gt; &lt;fabric8-kubernetes-client.version&gt;4.6.1&lt;&#x2F;fabric8-kubernetes-client.version&gt;&lt;!-- 对应k8s v1.15.3 --&gt; &lt;springcloud.version&gt;Greenwich.SR4&lt;&#x2F;springcloud.version&gt; &lt;springcloud.kubernetes.version&gt;1.1.1.RELEASE&lt;&#x2F;springcloud.kubernetes.version&gt; &lt;mysql.version&gt;5.1.46&lt;&#x2F;mysql.version&gt; &lt;&#x2F;properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;springcloud.version&#125;&lt;&#x2F;version&gt; &lt;type&gt;pom&lt;&#x2F;type&gt; &lt;scope&gt;import&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; &lt;&#x2F;dependencyManagement&gt; 核心依赖： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;&#x2F;artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;&#x2F;artifactId&gt; &lt;&#x2F;exclusion&gt; &lt;&#x2F;exclusions&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-undertow&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes-config&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;&#x2F;artifactId&gt; &lt;scope&gt;test&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt;&lt;!-- springcloud-k8s-discovery --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-commons&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-kubernetes-core&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-kubernetes-discovery&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes-ribbon&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; **本次依赖引入配置管理、服务的发现(即消费者)。**如果有操作redis和db的话，引入相应的依赖： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;!-- mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;&#x2F;artifactId&gt; &lt;version&gt;1.1.1&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!--分页插件--&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;&#x2F;groupId&gt; &lt;artifactId&gt;pagehelper&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;pageHelper.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!-- datasource pool--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt; &lt;artifactId&gt;druid&lt;&#x2F;artifactId&gt; &lt;version&gt;1.1.3&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;&#x2F;groupId&gt; &lt;artifactId&gt;commons-lang3&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-collections&lt;&#x2F;groupId&gt; &lt;artifactId&gt;commons-collections&lt;&#x2F;artifactId&gt; &lt;version&gt;3.2.2&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!-- 对redis支持,引入的话项目缓存就支持redis了,所以必须加上redis的相关配置,否则操作相关缓存会报异常 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-redis&lt;&#x2F;artifactId&gt; &lt;version&gt;1.4.7.RELEASE&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;&#x2F;groupId&gt; &lt;artifactId&gt;guava&lt;&#x2F;artifactId&gt; &lt;version&gt;19.0&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; 剩下的就是构建镜像时的插件： 1234567891011121314151617181920212223242526&lt;build&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;&lt;&#x2F;finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;&#x2F;artifactId&gt; &lt;configuration&gt; &lt;jvmArguments&gt;-Dfile.encoding&#x3D;UTF-8&lt;&#x2F;jvmArguments&gt; &lt;fork&gt;true&lt;&#x2F;fork&gt; &lt;&#x2F;configuration&gt; &lt;&#x2F;plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.jacoco&lt;&#x2F;groupId&gt; &lt;artifactId&gt;jacoco-maven-plugin&lt;&#x2F;artifactId&gt; &lt;version&gt;0.7.8&lt;&#x2F;version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;prepare-agent&lt;&#x2F;goal&gt; &lt;goal&gt;report&lt;&#x2F;goal&gt; &lt;&#x2F;goals&gt; &lt;&#x2F;execution&gt; &lt;&#x2F;executions&gt; &lt;&#x2F;plugin&gt; &lt;&#x2F;plugins&gt; &lt;&#x2F;build&gt; 接下来，我们创建主类： 12345678910@SpringBootApplication(scanBasePackages &#x3D; &#123; &quot;com.leinao&quot; &#125;)@EnableConfigurationProperties(EnvConfig.class)@EnableDiscoveryClient@EnableHystrix@EnableSchedulingpublic class AdminApp &#123; public static void main(String[] args) &#123; SpringApplication.run(AdminApp.class, args); &#125;&#125; 注意这里创建启动类时，对springboot的项目进行了优化，避免启动时加载很多，启动繁重，具体深度优化，可参考：https://mp.weixin.qq.com/s?__biz=MzU2NjIzNDk5NQ==&amp;mid=2247487954&amp;idx=1&amp;sn=2426451f3bd83161cfe1237f82d6b448&amp;key=f8fb043b3d2681a794e51a46e142af77355722dff712776af12b1f3c831218df6dfc329df63c8e5e550b3d88d58f0f178c4c3c16b141733e0e3344fa595e2bc25241d864d45132753fd99279b832de85&amp;ascene=1&amp;uin=MzQzMzI2NjAxMQ%3D%3D&amp;devicetype=Windows+10&amp;version=62070158&amp;lang=zh_CN&amp;pass_ticket=pnSSI9jAq0M11V5hYMmkoVm5qO%2FWk9l3UUUJMglbdtdDOzLHa7iHsDmwSzs486sD。 然后我们在进行配置，注意：据官方说，项目的src\\main\\resources路径下不要创建application.yml文件，只创建名为bootstrap.yml的文件： 123456789101112131415161718192021222324252627282930313233343536management: endpoint: restart: enabled: true health: enabled: true info: enabled: truespring: application: name: edge-admin cloud: kubernetes: config: sources: - name: $&#123;spring.application.name&#125; namespace: default discovery: all-namespaces: true reload: #自动更新配置的开关设置为打开 enabled: true #更新配置信息的模式：polling是主动拉取，event是事件通知 mode: polling #主动拉取的间隔时间是500毫秒 period: 500 http: encoding: charset: UTF-8 enabled: true force: true mvc: throw-exception-if-no-handler-found: true main: allow-bean-definition-overriding: true # 当遇到同样名称时，是否允许覆盖注册 这里，我创建了bootstrap文件，同时也加了application文件，启动时会先加载bootstrap，验证有效。 在application.yaml中，我们加入如下内容： 12345678910111213141516171819202122232425262728293031323334353637383940server: port: 9999 undertow: accesslog: enabled: false pattern: combined servlet: session: timeout: PT120Mlogging: path: &#x2F;data&#x2F;$&#123;spring.application.name&#125;&#x2F;logsmanagement: endpoint: restart: enabled: true health: enabled: true info: enabled: trueclient: http: request: connectTimeout: 8000 readTimeout: 30000mybatis: mapperLocations: classpath:mapper&#x2F;*.xml typeAliasesPackage: com.demo.*.modelbackend: ribbon: eureka: enabled: false client: enabled: true ServerListRefreshInterval: 5000hystrix.command.BackendCall.execution.isolation.thread.timeoutInMilliseconds: 5000hystrix.threadpool.BackendCallThread.coreSize: 5 注意：这里的server设置session的超时时间，对于springboot2.0与1.0版本完全不一样了，具体看内容。 其他的application-test.yaml等配置文件，配置的是日志的级别： 1234567logging: level: com: leinao: INFO org: springframework: web: INFO 接下来配置环境配置： EnvConfig.java类作为环境变量配置，注解ConfigurationProperties的prefix=”spring_cloud”， 表示该类用到的配置项都是名为”spring_cloud”的配置项的子内容 ： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363package com.demo.config;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.context.annotation.Configuration;&#x2F;** * 配置信息 * @author Damon * @date 2019年10月25日 下午1:54:01 * *&#x2F;@Configuration@ConfigurationProperties(prefix &#x3D; &quot;greeting&quot;)public class EnvConfig &#123; private String message &#x3D; &quot;This is a dummy message&quot;; private String container_command; private String model_dir_path; private String so_path; private String config_path; private String task_role_name; private String container_name; private String container_workdir; private String init_containers_image; private String service_account_name; private String spring_mq_host; private String spring_mq_port; private String spring_mq_user; private String spring_mq_pwd; private String jdbc_driverClassName; private String jdbc_url; private String jdbc_username; private String jdbc_password; private String spring_redis_host; private String spring_redis_port; private String spring_redis_pwd; private String kube_apiserver_address; private String image_path; private String volume_image_path; private String inference_job_namespace; private String api_version; private String remote_deployment_url; private String remote_pods_url; private String remote_deployment_pod_log_url; private String base_path; private String chunk_size; private String cas_url; private String create_job_url; private String abnormal_data_dir; private Long expire_time&#x3D; 600000L; public String getMessage() &#123; return this.message; &#125; public void setMessage(String message) &#123; this.message &#x3D; message; &#125; public String getContainer_command() &#123; return container_command; &#125; public void setContainer_command(String container_command) &#123; this.container_command &#x3D; container_command; &#125; public String getModel_dir_path() &#123; return model_dir_path; &#125; public void setModel_dir_path(String model_dir_path) &#123; this.model_dir_path &#x3D; model_dir_path; &#125; public String getSo_path() &#123; return so_path; &#125; public void setSo_path(String so_path) &#123; this.so_path &#x3D; so_path; &#125; public String getConfig_path() &#123; return config_path; &#125; public void setConfig_path(String config_path) &#123; this.config_path &#x3D; config_path; &#125; public String getTask_role_name() &#123; return task_role_name; &#125; public void setTask_role_name(String task_role_name) &#123; this.task_role_name &#x3D; task_role_name; &#125; public String getContainer_name() &#123; return container_name; &#125; public void setContainer_name(String container_name) &#123; this.container_name &#x3D; container_name; &#125; public String getContainer_workdir() &#123; return container_workdir; &#125; public void setContainer_workdir(String container_workdir) &#123; this.container_workdir &#x3D; container_workdir; &#125; public String getInit_containers_image() &#123; return init_containers_image; &#125; public void setInit_containers_image(String init_containers_image) &#123; this.init_containers_image &#x3D; init_containers_image; &#125; public String getService_account_name() &#123; return service_account_name; &#125; public void setService_account_name(String service_account_name) &#123; this.service_account_name &#x3D; service_account_name; &#125; public String getSpring_mq_host() &#123; return spring_mq_host; &#125; public void setSpring_mq_host(String spring_mq_host) &#123; this.spring_mq_host &#x3D; spring_mq_host; &#125; public String getSpring_mq_port() &#123; return spring_mq_port; &#125; public void setSpring_mq_port(String spring_mq_port) &#123; this.spring_mq_port &#x3D; spring_mq_port; &#125; public String getSpring_mq_user() &#123; return spring_mq_user; &#125; public void setSpring_mq_user(String spring_mq_user) &#123; this.spring_mq_user &#x3D; spring_mq_user; &#125; public String getSpring_mq_pwd() &#123; return spring_mq_pwd; &#125; public void setSpring_mq_pwd(String spring_mq_pwd) &#123; this.spring_mq_pwd &#x3D; spring_mq_pwd; &#125; public String getJdbc_driverClassName() &#123; return jdbc_driverClassName; &#125; public void setJdbc_driverClassName(String jdbc_driverClassName) &#123; this.jdbc_driverClassName &#x3D; jdbc_driverClassName; &#125; public String getJdbc_url() &#123; return jdbc_url; &#125; public void setJdbc_url(String jdbc_url) &#123; this.jdbc_url &#x3D; jdbc_url; &#125; public String getJdbc_username() &#123; return jdbc_username; &#125; public void setJdbc_username(String jdbc_username) &#123; this.jdbc_username &#x3D; jdbc_username; &#125; public String getJdbc_password() &#123; return jdbc_password; &#125; public void setJdbc_password(String jdbc_password) &#123; this.jdbc_password &#x3D; jdbc_password; &#125; public String getSpring_redis_host() &#123; return spring_redis_host; &#125; public void setSpring_redis_host(String spring_redis_host) &#123; this.spring_redis_host &#x3D; spring_redis_host; &#125; public String getSpring_redis_port() &#123; return spring_redis_port; &#125; public void setSpring_redis_port(String spring_redis_port) &#123; this.spring_redis_port &#x3D; spring_redis_port; &#125; public String getSpring_redis_pwd() &#123; return spring_redis_pwd; &#125; public void setSpring_redis_pwd(String spring_redis_pwd) &#123; this.spring_redis_pwd &#x3D; spring_redis_pwd; &#125; public String getKube_apiserver_address() &#123; return kube_apiserver_address; &#125; public void setKube_apiserver_address(String kube_apiserver_address) &#123; this.kube_apiserver_address &#x3D; kube_apiserver_address; &#125; public String getImage_path() &#123; return image_path; &#125; public void setImage_path(String image_path) &#123; this.image_path &#x3D; image_path; &#125; public String getVolume_image_path() &#123; return volume_image_path; &#125; public void setVolume_image_path(String volume_image_path) &#123; this.volume_image_path &#x3D; volume_image_path; &#125; public String getInference_job_namespace() &#123; return inference_job_namespace; &#125; public void setInference_job_namespace(String inference_job_namespace) &#123; this.inference_job_namespace &#x3D; inference_job_namespace; &#125; public String getApi_version() &#123; return api_version; &#125; public void setApi_version(String api_version) &#123; this.api_version &#x3D; api_version; &#125; public String getRemote_deployment_url() &#123; return remote_deployment_url; &#125; public void setRemote_deployment_url(String remote_deployment_url) &#123; this.remote_deployment_url &#x3D; remote_deployment_url; &#125; public String getRemote_pods_url() &#123; return remote_pods_url; &#125; public void setRemote_pods_url(String remote_pods_url) &#123; this.remote_pods_url &#x3D; remote_pods_url; &#125; public String getRemote_deployment_pod_log_url() &#123; return remote_deployment_pod_log_url; &#125; public void setRemote_deployment_pod_log_url(String remote_deployment_pod_log_url) &#123; this.remote_deployment_pod_log_url &#x3D; remote_deployment_pod_log_url; &#125; public String getBase_path() &#123; return base_path; &#125; public void setBase_path(String base_path) &#123; this.base_path &#x3D; base_path; &#125; public String getChunk_size() &#123; return chunk_size; &#125; public void setChunk_size(String chunk_size) &#123; this.chunk_size &#x3D; chunk_size; &#125; public Long getExpire_time() &#123; return expire_time; &#125; public void setExpire_time(Long expire_time) &#123; this.expire_time &#x3D; expire_time; &#125; public String getCas_url() &#123; return cas_url; &#125; public void setCas_url(String cas_url) &#123; this.cas_url &#x3D; cas_url; &#125; public String getCreate_job_url() &#123; return create_job_url; &#125; public void setCreate_job_url(String create_job_url) &#123; this.create_job_url &#x3D; create_job_url; &#125; public String getAbnormal_data_dir() &#123; return abnormal_data_dir; &#125; public void setAbnormal_data_dir(String abnormal_data_dir) &#123; this.abnormal_data_dir &#x3D; abnormal_data_dir; &#125;&#125;测试demo类：&#x2F;** * @author Damon * @date 2019年12月27日 上午9:16:41 * *&#x2F;@RestControllerpublic class DemoController &#123; @Autowired private EnvConfig envConfig; &#x2F;** * * @author Damon * @date 2019年12月26日 * *&#x2F; @GetMapping(value &#x3D; &quot;&#x2F;getTest&quot;) public String getTest() &#123; return envConfig.getBase_path(); &#125;&#125; 重点：默认的svc是没有权限访问k8s的API Server的资源的，执行如下脚本，可以提升权限，允许其访问configmap的可读权限： 123456789101112131415#使用这个代表集群最高权限，deployment中无需引入serviceAccount: config-readerapiVersion: rbac.authorization.k8s.io&#x2F;v1kind: ClusterRoleBindingmetadata: name: fabric8-rbacsubjects: - kind: ServiceAccount # Reference to upper&#39;s &#96;metadata.name&#96; name: default # Reference to upper&#39;s &#96;metadata.namespace&#96; namespace: defaultroleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io 配置configmap： 1234567891011121314151617181920212223kind: ConfigMapapiVersion: v1metadata: name: edge-admindata: application.yaml: |- greeting: message: Say Hello to the World --- spring: profiles: dev greeting: message: Say Hello to the Developers --- spring: profiles: test greeting: message: Say Hello to the Test --- spring: profiles: prod greeting: message: Say Hello to the Prod 接下来就是执行deployment启动项目了： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162apiVersion: apps&#x2F;v1kind: Deploymentmetadata: name: edge-admin-deployment labels: app: edge-adminspec: replicas: 1 selector: matchLabels: app: edge-admin template: metadata: labels: app: edge-admin spec: nodeSelector: edge-admin: &quot;true&quot; containers: - name: edge-admin image: 10.11.2.20:8000&#x2F;harbor&#x2F;edge-admin imagePullPolicy: IfNotPresent ports: - name: admin01 containerPort: 1002 volumeMounts: - mountPath: &#x2F;home&#x2F;edge-admin name: edge-admin-path - mountPath: &#x2F;data&#x2F;edge-admin name: edge-admin-log-path - mountPath: &#x2F;etc&#x2F;kubernetes name: kube-config-path - mountPath: &#x2F;abnormal_data_dir name: abnormal-data-dir args: [&quot;sh&quot;, &quot;-c&quot;, &quot;nohup java $JAVA_OPTS -jar -XX:MetaspaceSize&#x3D;128m -XX:MaxMetaspaceSize&#x3D;128m -Xms1024m -Xmx1024m -Xmn256m -Xss256k -XX:SurvivorRatio&#x3D;8 -XX:+UseConcMarkSweepGC edge-admin.jar --spring.profiles.active&#x3D;dev&quot;, &quot;&amp;&quot;] hostAliases: - ip: &quot;10.10.1.5&quot; hostnames: - &quot;k8s.api.server&quot; - &quot;foo.remote&quot; - ip: &quot;127.0.0.1&quot; hostnames: - &quot;foo.localhost&quot; - ip: &quot;0.0.0.0&quot; hostnames: - &quot;foo.all&quot; #利用admin-rbac.yaml来获取权限 #serviceAccount: config-reader #serviceAccountName: config-reader volumes: - name: edge-admin-path hostPath: path: &#x2F;var&#x2F;pai&#x2F;edge-admin - name: edge-admin-log-path hostPath: path: &#x2F;data&#x2F;edge-admin - name: kube-config-path hostPath: path: &#x2F;etc&#x2F;kubernetes - name: abnormal-data-dir hostPath: path: &#x2F;data&#x2F;images&#x2F;detect_result&#x2F;defect 其中，前面说的，项目启动参数对其性能优化，是对jvm的参数设置。分别执行kubectl apply -f deployment.yaml和configmap.yaml，创建demo时所用的configmap的资源以及利用k8s部署启动项目。 最后打开浏览器：执行ip:port/hello，即可看到configmap中对应的属性值，这里就不展示了，有兴趣的可以试试。 以上即是对springcloud和k8s首次结合后利用其configmap特性，来做配置管理，摒弃springcloud-config、spring-boot-starter-actuator的组件，减少系统的复杂性，毕竟k8s是肯定会被用到的，所以可以直接用其特性来做系统服务的环境配置管理。 结束福利 开源实战利用 k8s 作微服务的架构设计代码： https://gitee.com/damon_one/spring-cloud-k8s https://gitee.com/damon_one/spring-cloud-oauth2 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]},{"title":"浅谈负载均衡","date":"2020-07-10T05:47:15.000Z","path":"2020/07/10/springcloud-01/","text":"1、 前言 负载均衡，英文：Load Balance，其含义是请求分发到多个粒度单元上进行执行操作，例如各种服务器、应用服务、中台服务、数据服务等，从而达到共同完成某项任务的目的。为了拓宽网络设备和服务器的带宽、增加吞吐量、加强网络请求处理能力、提高网络的灵活性和高可用性，负载均衡是一种廉价、有效、透明的方法，它为服务的高并发做了一次缓冲，让单个服务的压力瞬间减少，实现了服务的高可用，避免服务因为压力而面临宕机的危险。 2、负载均衡 2.1 基于网络的负载均衡大家都知道，OSI 模型有 7 层结构，每层都可以有几个子层。OSI 的 7 层从上到下分别是物理层、数据链路层、网络层、传输层、会话层、表示层、应用层： 在这七层结构中，高层次都是依赖于低层次的。层次越高，使用起来越方便。 根据负载均衡技术实现在 OSI 七层模型的不同层次，是可以给负载均衡分类的。 常见的实现方式中，主要可以在应用层、传输层、网络层和数据传输层做文章。所以，工作在应用层的负载均衡，我们通常称之为七层负载均衡、工作在传输层的我们称之为四层负载均衡。我们一个个来看看： 七层负载均衡 七层负载均衡工作在 OSI 模型的应用层，应用层协议较多，常用 http、dns、ftp 等。七层负载就可以基于这些协议来负载。这些应用层协议中会包含很多有意义的内容。比如同一个 Web 服务器的负载均衡，除了根据 IP 加 port 进行负载外，还可根据 URL 来决定是否要进行负载均衡。 四层负载均衡 四层负载均衡工作在 OSI 模型的传输层，由于在传输层，只有 TCP/UDP 协议，这两种协议中除了包含源 IP、目标 IP 以外，还包含源端口及目的端口。四层负载均衡服务器在接受到客户端请求后，以后通过修改数据包的地址信息（IP+端口号）将流量转发到应用服务器。 2.2 负载均衡工具负载均衡的工具，常见的有 Nginx、k8s、Ribbon、Feign、HAProxy 等。 Nginx Nginx 主要用来作七层负载均衡，反向代理 http、https 的协议链接，同时也提供了 IMAP/POP3/SMTP 的服务。 upstream proxy_demo_aaa { server weight=5; server weight=6;} location ~ ^/demo-aaa/api(.*)$ { proxy_pass http://proxy_demo_aaa/api$1$is_args$args;}k8s k8s 的负载均衡是基于 kube-proxy，其服务发现基于 kube-dns，最后由于每个 Service 对应的 pod 可以是多个，所以可以基于 kube-proxy 实现负载均衡，kube-proxy 进程其实就是一个智能的软件负载均衡器，他负责把 service 的请求转发到后端的某个 pod 实例。 Ribbon Ribbon 是一个为客户端提供负载均衡功能的服务，它内部提供了一个叫做 ILoadBalance 的接口代表负载均衡器的操作，比如有添加服务器、选择服务器、获取所有的服务器列表、获取可用的服务器列表等等。 常见的，使用 RestTemplate 进行服务提供者、服务消费者之间的通信，只需为 RestTemplate 配置类添加@LoadBalanced 注解即可。 @Bean@LoadBalanced public RestTemplate restTemplate() { return new RestTemplate();}Feign Feign 是一个声明式负载均衡客户端使用 Feign 能让编写 WebService 的客户端更加简单，它的使用方法是定义一个接口，然后在上面添加注解，同时也支持 JAX-RS 标准的注解。Feign 也支持可拔插式的编码器和解码器。 @FeignClient(name = “provider-service”, configuration = {Feign4HttpConfiguration.class, FeignLogConfiguration.class}, fallback = CustomerClientImpl.class)public interface CustomerClient { @PostMapping(&quot;/save&quot;) String save(); @GetMapping(&quot;/api/user/getUserInfo&quot;) Response&lt;Object&gt; getUserInfo();}HAProxy HAProxy 是一个使用 C 语言编写的自由、开放源代码软件，其提供高可用性、负载均衡，以及基于 TCP 和 HTTP 的应用程序代理的功能。 2.3 负载均衡算法 常见的几种负载均衡的算法有：随机、轮询、最少链接、Hash、加权、重试等。 随机：即请求随机分配到各台服务器上，这是默认的策略机制。 轮询：将所有请求，依次分发到每台服务器上，适合服务器硬件相同的场景，服务请求数相同。 最少链接：将本次请求分配到请求数最少的服务上，这种可以根据服务器当前的请求处理情况，动态分配。 Hash：根据 IP 地址进行 Hash 计算，得到 IP 地址，这种可以将来自同一 IP 地址的请求，同一会话期内，转发到同一服务器；实现会话粘滞。但目标服务器宕机后，会话也会随之丢失。 加权：在上面几种算法基础上，进行一定的加权比例分配。 重试：这种策略一般都会有，就是在调用失败后，进行二次重试机制。 当然，还有其他的动态的算法规则：最快模式、观察模式、动态性能分配等。 结束福利 开源实战利用 k8s 作微服务的架构设计代码: https://gitee.com/damon_one/spring-cloud-k8s 欢迎大家 star，多多指教。 关于作者 笔名：Damon，技术爱好者，长期从事 Java 开发、Spring Cloud 的微服务架构设计，以及结合 Docker、K8s 做微服务容器化，自动化部署等一站式项目部署、落地。目前主要从事基于 K8s 云原生架构研发的工作。Golang 语言开发，长期研究边缘计算框架 KubeEdge、调度框架 Volcano 等。公众号 程序猿Damon 发起人。个人微信 MrNull008，个人网站：Damon | Micro-Service | Containerization | DevOps，欢迎來撩。 欢迎关注：InfoQ 欢迎关注：腾讯自媒体专栏 欢迎关注 欢迎关注 (adsbygoogle = window.adsbygoogle || []).push({});","tags":[]}]